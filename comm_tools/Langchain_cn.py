import os.path
from comm_tools import toolbox
from crazy_functions import crazy_utils
import gradio as gr
from comm_tools import func_box
import time

def knowledge_base_writing(files, links: str, select, name, ipaddr: gr.Request):
    try:
        from zh_langchain import construct_vector_store
        from langchain.embeddings.huggingface import HuggingFaceEmbeddings
        from crazy_functions.crazy_utils import knowledge_archive_interface
    except Exception as e:
        yield 'å¯¼å…¥ä¾èµ–å¤±è´¥ã€‚æ­£åœ¨å°è¯•è‡ªåŠ¨å®‰è£…', gr.Dropdown.update(), '' # åˆ·æ–°ç•Œé¢
        from crazy_functions.crazy_utils import try_install_deps
        try_install_deps(['zh_langchain==0.2.1'])
    # < --------------------è¯»å–å‚æ•°--------------- >
    vector_path = os.path.join(func_box.knowledge_path, ipaddr.client.host)
    if name and select != 'æ–°å»ºçŸ¥è¯†åº“':
        os.rename(os.path.join(vector_path, select), os.path.join(vector_path, name))
        kai_id = name
    elif name and select == 'æ–°å»ºçŸ¥è¯†åº“': kai_id = name
    elif select and select != 'æ–°å»ºçŸ¥è¯†åº“': kai_id = select
    else: kai_id = func_box.created_atime()
    yield 'å¼€å§‹å’¯å¼€å§‹å’¯ï½', gr.Dropdown.update(), ''
    # < --------------------è¯»å–æ–‡ä»¶--------------- >
    file_manifest = []
    network_files = links.splitlines()
    spl,  = toolbox.get_conf('spl')
    # æœ¬åœ°æ–‡ä»¶
    for sp in spl:
        _, file_manifest_tmp, _ = crazy_utils.get_files_from_everything(files, type=f'.{sp}')
        file_manifest += file_manifest_tmp
    for net_file in network_files:
        _, file_manifest_tmp, _ = crazy_utils.get_files_from_everything(net_file, type=f'.md')
        file_manifest += file_manifest_tmp
    if len(file_manifest) == 0:
        types = "\t".join(f"`{s}`" for s in spl)
        yield (toolbox.markdown_convertion(f'æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯è¯»å–æ–‡ä»¶ï¼Œ å½“å‰æ”¯æŒè§£æçš„æ–‡ä»¶æ ¼å¼åŒ…æ‹¬: \n\n{types}'),
               gr.Dropdown.update(), '')
        return
    # < -------------------é¢„çƒ­æ–‡æœ¬å‘é‡åŒ–æ¨¡ç»„--------------- >
    yield ('æ­£åœ¨åŠ è½½å‘é‡åŒ–æ¨¡å‹...', gr.Dropdown.update(), '')
    from langchain.embeddings.huggingface import HuggingFaceEmbeddings
    with toolbox.ProxyNetworkActivate():    # ä¸´æ—¶åœ°æ¿€æ´»ä»£ç†ç½‘ç»œ
        HuggingFaceEmbeddings(model_name="GanymedeNil/text2vec-large-chinese")
    # < -------------------æ„å»ºçŸ¥è¯†åº“--------------- >
    preprocessing_files = func_box.to_markdown_tabs(head=['æ–‡ä»¶'], tabs=[file_manifest])
    yield (toolbox.markdown_convertion(f'æ­£åœ¨å‡†å¤‡å°†ä»¥ä¸‹æ–‡ä»¶å‘é‡åŒ–ï¼Œç”ŸæˆçŸ¥è¯†åº“æ–‡ä»¶ï¼š\n\n{preprocessing_files}'),
           gr.Dropdown.update(), '')
    with toolbox.ProxyNetworkActivate():    # ä¸´æ—¶åœ°æ¿€æ´»ä»£ç†ç½‘ç»œ
        kai = knowledge_archive_interface(vs_path=vector_path)
        kai.feed_archive(file_manifest=file_manifest, id=kai_id)
    kai_files = kai.get_loaded_file()
    kai_files = func_box.to_markdown_tabs(head=['æ–‡ä»¶'], tabs=[kai_files])
    yield (toolbox.markdown_convertion(f'æ„å»ºå®Œæˆ, å½“å‰çŸ¥è¯†åº“å†…æœ‰æ•ˆçš„æ–‡ä»¶å¦‚ä¸‹, å·²è‡ªåŠ¨å¸®æ‚¨é€‰ä¸­çŸ¥è¯†åº“ï¼Œç°åœ¨ä½ å¯ä»¥ç•…å¿«çš„å¼€å§‹æé—®å•¦ï½\n\n{kai_files}'),
           gr.Dropdown.update(value='æ–°å»ºçŸ¥è¯†åº“', choices=obtain_a_list_of_knowledge_bases(ipaddr)), kai_id)


def knowledge_base_query(txt, kai_id, chatbot, history, llm_kwargs, ipaddr: gr.Request):
    # resolve deps
    try:
        from zh_langchain import construct_vector_store
        from langchain.embeddings.huggingface import HuggingFaceEmbeddings
        from crazy_functions.crazy_utils import knowledge_archive_interface
    except Exception as e:
        chatbot.append(["ä¾èµ–ä¸è¶³", "å¯¼å…¥ä¾èµ–å¤±è´¥ã€‚æ­£åœ¨å°è¯•è‡ªåŠ¨å®‰è£…ï¼Œè¯·æŸ¥çœ‹ç»ˆç«¯çš„è¾“å‡ºæˆ–è€å¿ƒç­‰å¾…..."])
        yield from toolbox.update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
        from crazy_functions.crazy_utils import try_install_deps
        try_install_deps(['zh_langchain==0.2.1'])

    # < -------------------ä¸ºç©ºæ—¶ï¼Œä¸å»æŸ¥è¯¢å‘é‡æ•°æ®åº“--------------- >
    if not txt: return txt
    # < -------------------æ£€ç´¢Prompt--------------- >
    kai = knowledge_archive_interface(vs_path=os.path.join(func_box.knowledge_path, ipaddr.client.host))
    new_txt = f'{txt}'
    for id in kai_id:
        # < -------------------æŸ¥è¯¢å‘é‡æ•°æ®åº“--------------- >
        chatbot.append([txt, f'æ­£åœ¨å°†é—®é¢˜å‘é‡åŒ–ï¼Œç„¶åå¯¹{func_box.html_tag_color(id)}çŸ¥è¯†åº“è¿›è¡ŒåŒ¹é…'])
        yield from toolbox.update_ui(chatbot=chatbot, history=history)  # åˆ·æ–°ç•Œé¢
        resp, prompt, _ok = kai.answer_with_archive_by_id(txt, id)
        if resp:
            referenced_documents = "\n".join([f"{k}: " + doc.page_content for k, doc in enumerate(resp['source_documents'])])
            new_txt += f'\nä»¥ä¸‹ä¸‰ä¸ªå¼•å·å†…çš„æ˜¯çŸ¥è¯†åº“æä¾›çš„å‚è€ƒæ–‡æ¡£ï¼š\n"""\n{referenced_documents}\n"""'
    return new_txt


def obtain_a_list_of_knowledge_bases(ipaddr):
    def get_directory_list(folder_path):
        directory_list = []
        for root, dirs, files in os.walk(folder_path):
            for dir_name in dirs:
                directory_list.append(dir_name)
        return directory_list
    user_path = os.path.join(func_box.knowledge_path, ipaddr.client.host)
    return get_directory_list(user_path) + get_directory_list(func_box.knowledge_path_sys_path)

def obtaining_knowledge_base_files(vs_id, chatbot, show,ipaddr: gr.Request):
    from crazy_functions.crazy_utils import knowledge_archive_interface
    if vs_id and 'çŸ¥è¯†åº“å±•ç¤º' in show:
        kai = knowledge_archive_interface(vs_path=os.path.join(func_box.knowledge_path, ipaddr.client.host))
        if isinstance(chatbot, toolbox.ChatBotWithCookies):
            pass
        else:
            chatbot = toolbox.ChatBotWithCookies(chatbot)
            chatbot.write_list(chatbot)
        chatbot.append([None, f'æ­£åœ¨æ£€æŸ¥çŸ¥è¯†åº“å†…æ–‡ä»¶{"  ".join([func_box.html_tag_color(i)for i in vs_id])}'])
        yield chatbot, gr.Column.update(visible=False), 'ğŸƒğŸ»â€ æ­£åœ¨åŠªåŠ›è½®è¯¢ä¸­....è¯·ç¨ç­‰ï¼Œ tipsï¼šçŸ¥è¯†åº“å¯ä»¥å¤šé€‰ï¼Œä½†ä¸è¦è´ªæ¯å“¦ï½ï¸'
        kai_files = {}
        for id in vs_id:
            kai_files.update(kai.get_init_file(vs_id=id))
        tabs = [[_id, func_box.html_view_blank(file), kai_files[file][_id]] for file in kai_files for _id in kai_files[file]]
        chatbot.append([None, f'æ£€æŸ¥å®Œæˆï¼Œå½“å‰é€‰æ‹©çš„çŸ¥è¯†åº“å†…å¯ç”¨æ–‡ä»¶å¦‚ä¸‹ï¼š'
                              f'\n\n {func_box.to_markdown_tabs(head=["æ‰€å±çŸ¥è¯†åº“", "æ–‡ä»¶", "æ–‡ä»¶ç±»å‹"], tabs=tabs)}\n\n'
                              f'ğŸ¤© å¿«æ¥å‘æˆ‘æé—®å§ï½'])
        yield chatbot, gr.Column.update(visible=False), 'âœ… æ£€æŸ¥å®Œæˆ'
    else:
        yield chatbot, gr.update(), 'Done'

