import json
import time
import gradio as gr
import logging
import traceback
import requests
import importlib
import random

# config_private.pyæ”¾è‡ªå·±çš„ç§˜å¯†å¦‚APIå’Œä»£ç†ç½‘å€
# è¯»å–æ—¶é¦–å…ˆçœ‹æ˜¯å¦å­˜åœ¨ç§å¯†çš„config_privateé…ç½®æ–‡ä»¶ï¼ˆä¸å—gitç®¡æ§ï¼‰ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¦†ç›–åŸconfigæ–‡ä»¶
from toolbox import (
    get_conf,
    update_ui,
    trimmed_format_exc,
    is_the_upload_folder,
    read_one_api_model_name,
)

proxies, TIMEOUT_SECONDS, MAX_RETRY = get_conf(
    "proxies", "TIMEOUT_SECONDS", "MAX_RETRY"
)

timeout_bot_msg = (
    "[Local Message] Request timeout. Network error. Please check proxy settings in config.py."
    + "ç½‘ç»œé”™è¯¯ï¼Œæ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦å¯ç”¨ï¼Œä»¥åŠä»£ç†è®¾ç½®çš„æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæ ¼å¼é¡»æ˜¯[åè®®]://[åœ°å€]:[ç«¯å£]ï¼Œç¼ºä¸€ä¸å¯ã€‚"
)


def get_full_error(chunk, stream_response):
    """
    å°è¯•è·å–å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
    """
    while True:
        try:
            chunk += next(stream_response)
        except:
            break
    return chunk


def decode_chunk(chunk):
    """
    ç”¨äºè§£è¯»"content"å’Œ"finish_reason"çš„å†…å®¹
    """
    chunk = chunk.decode()
    respose = ""
    finish_reason = "False"
    try:
        chunk = json.loads(chunk)
    except:
        pass
    try:
        respose = chunk["choices"][0]["delta"]["content"]
    except:
        pass
    try:
        finish_reason = chunk["choices"][0]["finish_reason"]
    except:
        pass
    return respose, finish_reason


def generate_message(input, model, key, history, token, system_prompt, temperature):
    """
    æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œé€‰æ‹©LLMæ¨¡å‹ï¼Œç”Ÿæˆhttpè¯·æ±‚ï¼Œä¸ºå‘é€è¯·æ±‚åšå‡†å¤‡
    """
    api_key = f"Bearer {key}"

    headers = {"Content-Type": "application/json", "Authorization": api_key}

    conversation_cnt = len(history) // 2

    messages = [{"role": "system", "content": system_prompt}]
    if conversation_cnt:
        for index in range(0, 2 * conversation_cnt, 2):
            what_i_have_asked = {}
            what_i_have_asked["role"] = "user"
            what_i_have_asked["content"] = history[index]
            what_gpt_answer = {}
            what_gpt_answer["role"] = "assistant"
            what_gpt_answer["content"] = history[index + 1]
            if what_i_have_asked["content"] != "":
                if what_gpt_answer["content"] == "":
                    continue
                if what_gpt_answer["content"] == timeout_bot_msg:
                    continue
                messages.append(what_i_have_asked)
                messages.append(what_gpt_answer)
            else:
                messages[-1]["content"] = what_gpt_answer["content"]
    what_i_ask_now = {}
    what_i_ask_now["role"] = "user"
    what_i_ask_now["content"] = input
    messages.append(what_i_ask_now)
    playload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "stream": True,
        "max_tokens": token,
    }
    try:
        print(f" {model} : {conversation_cnt} : {input[:100]} ..........")
    except:
        print("è¾“å…¥ä¸­å¯èƒ½å­˜åœ¨ä¹±ç ã€‚")
    return headers, playload


def predict_no_ui_long_connection(
    inputs,
    llm_kwargs,
    history=[],
    sys_prompt="",
    observe_window=None,
    console_slience=False,
):
    """
    å‘é€è‡³chatGPTï¼Œç­‰å¾…å›å¤ï¼Œä¸€æ¬¡æ€§å®Œæˆï¼Œä¸æ˜¾ç¤ºä¸­é—´è¿‡ç¨‹ã€‚ä½†å†…éƒ¨ç”¨streamçš„æ–¹æ³•é¿å…ä¸­é€”ç½‘çº¿è¢«æã€‚
    inputsï¼š
        æ˜¯æœ¬æ¬¡é—®è¯¢çš„è¾“å…¥
    sys_prompt:
        ç³»ç»Ÿé™é»˜prompt
    llm_kwargsï¼š
        chatGPTçš„å†…éƒ¨è°ƒä¼˜å‚æ•°
    historyï¼š
        æ˜¯ä¹‹å‰çš„å¯¹è¯åˆ—è¡¨
    observe_window = Noneï¼š
        ç”¨äºè´Ÿè´£è·¨è¶Šçº¿ç¨‹ä¼ é€’å·²ç»è¾“å‡ºçš„éƒ¨åˆ†ï¼Œå¤§éƒ¨åˆ†æ—¶å€™ä»…ä»…ä¸ºäº†fancyçš„è§†è§‰æ•ˆæœï¼Œç•™ç©ºå³å¯ã€‚observe_window[0]ï¼šè§‚æµ‹çª—ã€‚observe_window[1]ï¼šçœ‹é—¨ç‹—
    """
    watch_dog_patience = 5  # çœ‹é—¨ç‹—çš„è€å¿ƒï¼Œè®¾ç½®5ç§’ä¸å‡†å’¬äºº(å’¬çš„ä¹Ÿä¸æ˜¯äºº
    if inputs == "":
        inputs = "ä½ å¥½ğŸ‘‹"
    headers, playload = generate_message(
        input=inputs,
        model=llm_kwargs["model"],
        key=get_conf(llm_kwargs["APIKEY"]),
        history=history,
        token=llm_kwargs["token"],
        system_prompt=sys_prompt,
        temperature=llm_kwargs["temperature"],
    )
    retry = 0
    while True:
        try:
            from .bridge_all import model_info

            endpoint = model_info[llm_kwargs["llm_model"]]["endpoint"]
            if not llm_kwargs["not_use_proxy"]:
                response = requests.post(
                    endpoint,
                    headers=headers,
                    proxies=proxies,
                    json=playload,
                    stream=True,
                    timeout=TIMEOUT_SECONDS,
                )
            else:
                response = requests.post(
                    endpoint,
                    headers=headers,
                    json=playload,
                    stream=True,
                    timeout=TIMEOUT_SECONDS,
                )
            break
        except requests.exceptions.ReadTimeout as e:
            retry += 1
            traceback.print_exc()
            if retry > MAX_RETRY:
                raise TimeoutError
            if MAX_RETRY != 0:
                print(f"è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry}/{MAX_RETRY}) â€¦â€¦")

    stream_response = response.iter_lines()
    result = ""
    while True:
        try:
            chunk = next(stream_response)
        except StopIteration:
            break
        except requests.exceptions.ConnectionError:
            chunk = next(stream_response)  # å¤±è´¥äº†ï¼Œé‡è¯•ä¸€æ¬¡ï¼Ÿå†å¤±è´¥å°±æ²¡åŠæ³•äº†ã€‚
        response_text, finish_reason = decode_chunk(chunk)
        # è¿”å›çš„æ•°æ®æµç¬¬ä¸€æ¬¡ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
        if response_text == "" and finish_reason != "False":
            continue
        if chunk:
            try:
                if finish_reason == "stop":
                    logging.info(f"[response] {result}")
                    break
                result += response_text
                if not console_slience:
                    print(response_text, end="")
                if observe_window is not None:
                    # è§‚æµ‹çª—ï¼ŒæŠŠå·²ç»è·å–çš„æ•°æ®æ˜¾ç¤ºå‡ºå»
                    if len(observe_window) >= 1:
                        observe_window[0] += response_text
                    # çœ‹é—¨ç‹—ï¼Œå¦‚æœè¶…è¿‡æœŸé™æ²¡æœ‰å–‚ç‹—ï¼Œåˆ™ç»ˆæ­¢
                    if len(observe_window) >= 2:
                        if (time.time()-observe_window[1]) > watch_dog_patience:
                            raise RuntimeError("ç”¨æˆ·å–æ¶ˆäº†ç¨‹åºã€‚")
            except Exception as e:
                chunk = get_full_error(chunk, stream_response)
                chunk_decoded = chunk.decode()
                error_msg = chunk_decoded
                print(error_msg)
                raise RuntimeError("Jsonè§£æä¸åˆå¸¸è§„")
    return result

