# encoding: utf-8
# @Time   : 2024/3/24
# @Author : Spike
# @Descr   :
import gradio

from webui_elem.func_signals.__import__ import *


# TODO < -------------------------------- çŸ¥è¯†åº“å‡½æ•°æ³¨å†ŒåŒº -------------------------------------->
def kb_select_show(select: gr.Dropdown):
    """ é€‰æ‹©çŸ¥è¯†åº“é€‰é¡¹åå±•ç¤ºå¯¹åº”çš„é¡µé¢
    Args:
        select:
    Returns:
    """
    if select == 'æ–°å»ºçŸ¥è¯†åº“':
        return gr.update(visible=True), gr.update(visible=False)
    else:
        return gr.update(visible=False), gr.update(visible=True)


def kb_name_select_then(kb_name):
    """  é€‰æ‹©çŸ¥è¯†åº“åæ›´æ–°ç®€ä»‹ã€åˆ—è¡¨ã€è¯¦æƒ…ã€ç‰‡æ®µ
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
    Returns:
        [çŸ¥è¯†åº“åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“ç®€ä»‹ï¼ŒçŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“è¯¦æƒ…ï¼ŒçŸ¥è¯†åº“æ–‡ä»¶ç‰‡æ®µ]
    """
    kb_dict = base.kb_list_to_dict([kb_name])
    kb_name = list(kb_dict.keys())[0]
    file_details = pd.DataFrame(base.get_kb_file_details(kb_name))
    if 'file_name' in file_details:
        db_file_list = file_details['file_name'].to_list()
        file_name = db_file_list[0]
        last_details = __get_kb_details_df(file_details, file_details["No"] == 1)
        last_fragment = __get_kb_fragment_df(kb_name, file_name)
        kb_file_list = gr.update(choices=db_file_list, value=file_name)
        kb_file_details = gr.DataFrame.update(value=last_details, label=f'{file_name}-æ–‡ä»¶è¯¦æƒ…')
        kb_file_fragment = gr.DataFrame.update(value=last_fragment, label=f'{file_name}-æ–‡ä»¶ç‰‡æ®µç¼–è¾‘')
    else:
        kb_file_list = gr.update(choices=[], value='')
        kb_file_details = gr.DataFrame.update(value=pd.DataFrame(data=copy.deepcopy(kb_config.file_details_template)))
        kb_file_fragment = gr.DataFrame.update(value=pd.DataFrame(data=copy.deepcopy(kb_config.file_fragment_template)))

    kb_name_tm = base.kb_details_to_dict()
    kb_info = base.get_kb_details_by_name(kb_name).get('kb_info', '')
    update_output = {
        'kb_name_list': gr.update(choices=base.kb_dict_to_list(kb_name_tm) + ['æ–°å»ºçŸ¥è¯†åº“']),
        'kb_info_txt': kb_info,
        'kb_file_list': kb_file_list,
        'kb_file_details': kb_file_details,
        'kb_file_fragment': kb_file_fragment
    }
    return list(update_output.values())


def kb_name_change_btn(name):
    """ æ ¹æ®çŸ¥è¯†åº“åç§°çŠ¶æ€åˆ‡æ¢ä¸åŒé¢œè‰²çš„æŒ‰é’®
    Args:
        name: çŸ¥è¯†åº“åç§°
    Returns:
        [æ–°å»ºçŸ¥è¯†åº“æŒ‰é’®é¢œè‰²]
    """
    if name:
        return gr.Button.update(variant='primary')
    else:
        return gr.Button.update(variant='secondary')


def kb_upload_btn(upload: gr.Files, cloud: str):
    """ æ ¹æ®ä¸Šä¼ ã€äº‘æ–‡æ¡£é“¾æ¥çŠ¶æ€åˆ‡æ¢ä¸åŒé¢œè‰²çš„æŒ‰é’®
    Args:
        upload: ä¸Šä¼ æ–‡ä»¶
        cloud: æ–‡æ¡£é“¾æ¥
    Returns:
        [æ„å»ºçŸ¥è¯†åº“æŒ‰é’®é¢œè‰²]
    """
    if upload or URL(cloud).host:
        return gr.Button.update(variant='primary')
    else:
        return gr.Button.update(variant='secondary')


def kb_introduce_change_btn(kb_name, kb_info):
    """ æ›´æ–°çŸ¥è¯†åº“ç®€ä»‹
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        kb_info: çŸ¥è¯†åº“ç®€ä»‹
    Returns:
        [Spike-Toast]
    """
    kb_dict = base.kb_list_to_dict([kb_name])
    kb_name = list(kb_dict.keys())[0]
    resp = kb_doc_api.update_info(kb_name, kb_info)

    if not resp.data != 200:
        raise gr.Error(json.dumps(resp.__dict__, indent=4, ensure_ascii=False))
    else:
        return gr.update()
        # yield gr.update(value=spike_toast('æ›´æ–°çŸ¥è¯†åº“ç®€ä»‹æˆåŠŸ'), visible=True)
        # time.sleep(1)
        # yield gr.update(visible=False)


def kb_date_add_row(source_data: pd.DataFrame):
    """ é’ˆå¯¹å‘é‡æ–‡ä»¶æ–°å¢ä¸€ä¸ªç©ºè¡Œ
    Args:
        source_data: åŸæ•°æ®
    Returns:
        [çŸ¥è¯†åº“æ–‡ä»¶ç‰‡æ®µ]
    """
    # æ·»åŠ ä¸€è¡Œæ•°æ®
    last_index = source_data.iloc[-1].iloc[1]
    # æ£€æŸ¥æœ€åä¸€è¡Œçš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦ä¸ºæ•´æ•°ç±»å‹
    if last_index.dtype == 'int64':
        new_index = last_index + 1
    else:
        new_index = 'ErrorType'
    new_row_data = [uuid.uuid4(), new_index, '', '']
    source_data.loc[len(source_data)] = new_row_data

    return gr.update(value=source_data)


def kb_new_confirm(kb_name: gr.Textbox, kb_type: gradio.Dropdown, kb_model: gr.Dropdown, kb_info: gr.DataFrame):
    """ æ ¹æ®é€‰æ‹©çš„æ¨¡å‹é¢„åˆ›å»ºä¸€ä¸ªç©ºçš„çŸ¥è¯†åº“
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        kb_type: å‘é‡åº“ç±»å‹
        kb_model: Embeddingæ¨¡å‹
        kb_info: çŸ¥è¯†åº“ç®€ä»‹
    Returns:
        [æ–°å¢åˆ—ï¼Œç¼–è¾‘åˆ—ï¼Œ çŸ¥è¯†åº“åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“ç®€ä»‹ï¼Œ çŸ¥è¯†åº“å†…æ–‡ä»¶åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“æ–‡ä»¶è¯¦æƒ…ï¼ŒçŸ¥è¯†åº“æ–‡ä»¶ç‰‡æ®µ]
    """
    kb_name_tm = base.kb_details_to_dict()

    if not kb_name or not kb_type or not kb_model:
        keywords = {"çŸ¥è¯†åº“åç§°": kb_name, "å‘é‡ç±»å‹": kb_type, "Embedding æ¨¡å‹": kb_model}
        error_keyword = " - ".join([f"ã€{i}ã€‘" for i in keywords if not keywords[i]])
        raise gr.Error(f'ç¼ºå¤± {error_keyword} å­—æ®µ,æ— æ³•åˆ›å»º, ')

    kb_server_list = base.KBServiceFactory.get_service_by_name(kb_name)
    if kb_name in kb_name_tm or kb_server_list is not None:
        if kb_name_tm.get(kb_name, {}).get('model') == kb_model:
            raise gr.Error(f'{kb_name} @ {kb_model} å·²å­˜åœ¨åŒåçŸ¥è¯†åº“ï¼Œè¯·é‡æ–°å‘½å')

    kb = base.KBServiceFactory.get_service(kb_name, kb_type, kb_model)
    kb.kb_info = kb_info
    try:
        kb.create_kb()
    except Exception as e:
        msg = f"åˆ›å»ºçŸ¥è¯†åº“å‡ºé”™ï¼š {e}"
        logger.error(f'{e.__class__.__name__}: {msg}')
        if not utils.validate_kb_name(kb_name):
            raise gr.Error("Don't attack me")
        shutil.rmtree(os.path.join(init_path.private_knowledge_path, kb_name))
        raise gr.Error(msg)
    select_name = base.kb_name_tm_merge(kb_name, kb_type, kb_model)
    new_output = {
        'new_clo': gr.update(visible=False),
        'edit_clo': gr.update(visible=True),
    }

    edit_output = {
        'kb_name_list': gr.update(choices=[select_name] + base.kb_dict_to_list(kb_name_tm) + ['æ–°å»ºçŸ¥è¯†åº“'],
                                  value=select_name),
        'kb_info_txt': kb_info,
        'kb_file_list': gr.update(choices=[], value=''),
        'kb_file_details': gr.DataFrame.update(value=pd.DataFrame(data=copy.deepcopy(kb_config.file_details_template))),
        'kb_file_fragment': gr.DataFrame.update(
            value=pd.DataFrame(data=copy.deepcopy(kb_config.file_fragment_template)))
    }

    return list(new_output.values()) + list(edit_output.values()) + [gr.update(choices=list(kb_name_tm.keys()) + [kb_name])]


def kb_download_embedding_model(model_name):
    """ ä¸‹è½½embeddingæ¨¡å‹
    Args:
        model_name: æ¨¡å‹è·¯å¾„
    Returns:
        [ä¸‹è½½çŠ¶æ€]
    """
    if not model_name:
        raise gr.Error('å¿…é¡»è¦é€‰ä¸€ä¸ª')
    from common.embeddings_api import embed_download
    obj, stream = embed_download(model_name)
    download_result = "```\n"
    for tag, chuck in stream:
        download_result += chuck
        yield gr.update(value=download_result)
    yield gr.update(value=download_result + '\n```\n`Done`')


def __get_kb_details_df(file_details: pd.DataFrame, condition: pd.Series):
    select_document = file_details[condition]

    select_kb_details = copy.deepcopy(kb_config.file_details_template)

    select_kb_details.update({
        'æ–‡æ¡£åŠ è½½å™¨': select_document['document_loader'].to_list(),
        'åˆ†è¯å™¨': select_document['text_splitter'].to_list(),
        'æ–‡æ¡£ç‰‡æ®µæ•°é‡': select_document['docs_count'].to_list(),
        'å‘é‡åº“': select_document['in_db'].to_list(),
        'æºæ–‡ä»¶': select_document['in_folder'].to_list()
    })
    return pd.DataFrame(data=select_kb_details)


def __get_kb_fragment_df(kb_name, file_name):
    """
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        file_name: æ–‡ä»¶åç§°
    Returns:
        [æŒ‡å®šæ–‡ä»¶ç‰‡æ®µ]
    """
    kb_fragment = copy.deepcopy(kb_config.file_fragment_template)

    info_fragment = kb_doc_api.search_docs(query='', knowledge_base_name=kb_name,
                                           top_k=1, score_threshold=1,
                                           file_name=file_name, metadata={})

    for i, v in enumerate(info_fragment):
        kb_fragment['id'].append(info_fragment[i].id)
        kb_fragment['N'].append(i + 1)
        kb_fragment['å†…å®¹'].append(info_fragment[i].page_content)
        kb_fragment['åˆ é™¤'].append('')
    return pd.DataFrame(data=kb_fragment)


def kb_file_update_confirm(upload_files: gr.Files, kb_name: gr.Textbox, kb_info: gr.Textbox, kb_max: gr.Number | int,
                           kb_similarity: gr.Number | int, kb_tokenizer: gradio.Dropdown | str,
                           kb_loader: gr.Dropdown | str, cloud_link: gr.Textbox, ipaddr: gr.Request):
    """ æ ¹æ®æäº¤æ–‡ä»¶ï¼Œå°†æ–‡ä»¶æ·»åŠ åˆ°çŸ¥è¯†åº“
    Args:
        upload_files: ä¸Šä¼ çš„æ–‡ä»¶
        kb_name: çŸ¥è¯†åº“åç§°
        kb_info: çŸ¥è¯†åº“ç®€ä»‹
        kb_max: æœ€é•¿æ–‡æœ¬æ£€æµ‹
        kb_similarity: ç›¸é‚»æ–‡æœ¬æ£€æµ‹
        kb_tokenizer: åˆ†è¯å™¨
        kb_loader: è¯»å–å™¨
        cloud_link: ç½‘ç»œé“¾æ¥
        ipaddr: gradio å¸¦è¿‡æ¥çš„wssä¿¡æ¯
    Returns:
        [çŸ¥è¯†åº“åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“ç®€ä»‹ï¼ŒçŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“è¯¦æƒ…ï¼ŒçŸ¥è¯†åº“æ–‡ä»¶ç‰‡æ®µ]
    """
    kb_name = list(base.kb_list_to_dict([kb_name]).keys())[0]
    user = user_client_mark(ipaddr)
    cloud_map, status = detach_cloud_links(cloud_link, {'ipaddr': user}, ['*'])
    if status.get('status'):
        raise gr.Error(f'æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œ{cloud_map}')
    files = []
    if upload_files:
        files.extend([f.name for f in upload_files])
    files += list(cloud_map.keys())
    response = kb_doc_api.upload_docs(files=files, knowledge_base_name=kb_name, override=True,
                                      to_vector_store=True, chunk_size=kb_max, chunk_overlap=kb_similarity,
                                      loader_enhance=kb_loader, docs={}, not_refresh_vs_cache=False,
                                      text_splitter_name=kb_tokenizer)
    if response.code != 200:
        raise gr.Error(json.dumps(response.__dict__, indent=4, ensure_ascii=False))
    if response.data.get('failed_files'):
        raise gr.Error(json.dumps(response.data, indent=4, ensure_ascii=False))
    return kb_name_select_then(kb_name)


def kb_select_file(kb_name, kb_file: str):
    """ é€‰æ‹©çŸ¥è¯†åº“å†…æ–‡ä»¶ï¼Œæ›´æ–°è¯¦æƒ…å’Œç‰‡æ®µ
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        kb_file: çŸ¥è¯†åº“æ–‡æ¡£
    Returns:
        [æ–‡ä»¶è¯¦æƒ…ï¼Œæ–‡ä»¶ç‰‡æ®µ]
    """
    kb_name = list(base.kb_list_to_dict([kb_name]).keys())[0]
    file_details = pd.DataFrame(base.get_kb_file_details(kb_name))

    last_details = __get_kb_details_df(file_details, file_details["file_name"] == kb_file)
    last_fragment = __get_kb_fragment_df(kb_name, kb_file)

    return gr.update(value=last_details, label=f'{kb_file}-æ–‡ä»¶è¯¦æƒ…'), gr.update(value=last_fragment,
                                                                                label=f'{kb_file}-æ–‡æ¡£ç‰‡æ®µç¼–è¾‘')


def kb_base_del(kb_name, del_confirm, _):
    """ åˆ é™¤çŸ¥è¯†åº“
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        del_confirm: çŸ¥è¯†åº“æ–‡ä»¶ï¼Œç”±jsä¼ é€’è¿‡æ¥çš„thiså€¼
        _: å ä½è€Œå·²
    Returns:
        [æ–°å¢åˆ—ï¼Œç¼–è¾‘åˆ—ï¼ŒçŸ¥è¯†åº“åˆ—è¡¨ï¼Œå¯ç”¨çŸ¥è¯†åº“åˆ—è¡¨]
    """
    if del_confirm == 'CANCELED':
        return gr.update(visible=False), gr.update(visible=True), gr.update(), gr.update()
    else:
        kb_name = list(base.kb_list_to_dict([kb_name]).keys())[0]
        response = kb_api.delete_kb(kb_name)
        if response.code != 200:
            raise gr.Error(json.dumps(response.__dict__, indent=4, ensure_ascii=False))
        kb_name_list = base.kb_dict_to_list(base.kb_details_to_dict()) + ['æ–°å»ºçŸ¥è¯†åº“']
        kb_name_tm = base.kb_details_to_dict()
        return (gr.update(visible=True), gr.update(visible=False), gr.update(choices=kb_name_list, value='æ–°å»ºçŸ¥è¯†åº“'),
                gr.update(choices=list(kb_name_tm.keys())))


def kb_docs_file_source_del(kb_name, kb_file, _):
    """ åˆ é™¤çŸ¥è¯†åº“å†…æ–‡ä»¶
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        kb_file: çŸ¥è¯†åº“æ–‡ä»¶ï¼Œç”±jsä¼ é€’è¿‡æ¥çš„thiså€¼
        _: å ä½ç¬¦
    Returns:
        [Spike-Toastï¼ŒçŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨ï¼ŒçŸ¥è¯†åº“è¯¦æƒ…ï¼ŒçŸ¥è¯†åº“ç‰‡æ®µ]
    """
    if kb_file == 'CANCELED':
        return gr.update(), gr.update(), gr.update(), gr.update()
    kb_name_d = list(base.kb_list_to_dict([kb_name]).keys())[0]
    resp = kb_doc_api.delete_docs(knowledge_base_name=kb_name_d, file_names=[kb_file],
                                  delete_content=True, not_refresh_vs_cache=False)
    if resp.code == 200 and not resp.data.get('failed_files'):
        toast = gr.update(
            value=spike_toast('å½»åº•åˆ é™¤æˆåŠŸ ğŸ´â€â˜ ï¸'),
            visible=True)
        _, _, file_list, details, fragment = kb_name_select_then(kb_name)
        yield toast, file_list, details, fragment
        time.sleep(1)
        yield gr.update(visible=False), file_list, details, fragment
    else:
        yield gr.Error(f'åˆ é™¤å¤±è´¥, {resp.__dict__}')


def kb_vector_del(kb_name, kb_file):
    """ ä»…åˆ é™¤çŸ¥è¯†åº“å†…å‘é‡ï¼Œä¸åˆ é™¤æ–‡ä»¶
    Args:
        kb_name:  çŸ¥è¯†åº“åç§°
        kb_file: çŸ¥è¯†åº“æ–‡ä»¶
    Returns:
        [Spike-Toastï¼Œæ–‡ä»¶è¯¦æƒ…ï¼Œæ–‡ä»¶ç‰‡æ®µ]
    """
    kb_name_d = list(base.kb_list_to_dict([kb_name]).keys())[0]
    resp = kb_doc_api.delete_docs(knowledge_base_name=kb_name_d, file_names=[kb_file],
                                  delete_content=False, not_refresh_vs_cache=False)
    if resp.code == 200 and not resp.data.get('failed_files'):
        toast = gr.update(
            value=spike_toast('ä»…ä»å‘é‡åº“ä¸­åˆ é™¤ï¼Œåç»­è¯¥æ–‡æ¡£ä¸ä¼šè¢«å‘é‡å¬å›ï¼Œå¦‚éœ€é‡æ–°å¼•ç”¨ï¼Œé‡è½½å‘é‡æ•°æ®å³å¯'),
            visible=True)
        yield toast, *kb_select_file(kb_name, kb_file)
        time.sleep(1)
        yield gr.update(visible=False), gr.update(), gr.update()
    else:
        yield gr.Error(f'åˆ é™¤å¤±è´¥, {resp.__dict__}')


def kb_vector_reload(_, kb_name, kb_info, kb_max, kb_similarity, kb_tokenizer, kb_loader, kb_file):
    """ æ ¹æ®æ–‡ä»¶è¯¦æƒ…æœªåˆ é™¤æ–‡ä»¶çš„é‡è½½çŸ¥è¯†åº“
    Args:
        _: ä¸Šä¼ æ–‡ä»¶ï¼Œå› ä¸ºé‡è½½çš„æ˜¯æ‰€é€‰æ–‡ä»¶ï¼Œæ‰€ä»¥ä¸ä¼šæäº¤
        kb_name: çŸ¥è¯†åº“åç§°
        kb_info: çŸ¥è¯†åº“ç®€ä»‹
        kb_max: æœ€é•¿æ–‡æœ¬æ£€æµ‹
        kb_similarity: ç›¸é‚»æ–‡æœ¬æ£€æµ‹
        kb_tokenizer: åˆ†è¯å™¨
        kb_loader: è¯»å–å™¨
        kb_file: æ‰€é€‰æ–‡ä»¶
    Returns:
        [Spike-Toastï¼Œæ–‡ä»¶è¯¦æƒ…ï¼Œæ–‡ä»¶ç‰‡æ®µ]
    """
    kb_name_k = list(base.kb_list_to_dict([kb_name]).keys())[0]
    resp = kb_doc_api.update_docs(
        knowledge_base_name=kb_name_k, file_names=[kb_file], docs={},
        chunk_size=kb_max, chunk_overlap=kb_similarity, override_custom_docs=True,
        loader_enhance=kb_loader, not_refresh_vs_cache=False,
        text_splitter_name=kb_tokenizer
    )
    if resp.code == 200 and not resp.data.get('failed_files'):
        yield gr.update(value=spike_toast('é‡è½½å‘é‡æ•°æ®æˆåŠŸ'), visible=True), *kb_select_file(kb_name, kb_file)
        time.sleep(1)
        yield gr.update(visible=False), gr.update(), gr.update()
    else:
        yield gr.Error(f'é‡è½½å‘é‡æ•°æ®å¤±è´¥, {resp.__dict__}'), gr.update(), gr.update()


def kb_base_changed_save(_, kb_name, kb_info, kb_max, kb_similarity, kb_tokenizer, kb_loader,
                         kb_file, kb_dataFrame: pd.DataFrame):
    """ æ›´æ”¹ç‰‡æ®µåä¿å­˜
    Args:
        _: ä¸Šä¼ æ–‡ä»¶ï¼Œå› ä¸ºé‡è½½çš„æ˜¯æ‰€é€‰æ–‡ä»¶ï¼Œæ‰€ä»¥ä¸ä¼šæäº¤
        kb_name: çŸ¥è¯†åº“åç§°
        kb_info: çŸ¥è¯†åº“ç®€ä»‹
        kb_max: æœ€é•¿æ–‡æœ¬æ£€æµ‹
        kb_similarity: ç›¸é‚»æ–‡æœ¬æ£€æµ‹
        kb_tokenizer: åˆ†è¯å™¨
        kb_loader: è¯»å–å™¨
        kb_file: æ‰€é€‰æ–‡ä»¶
        kb_dataFrame: ä¿®æ”¹åçš„çŸ¥è¯†åº“æ–‡ä»¶
    Returns:
        [Spike-Toastï¼ŒçŸ¥è¯†åº“ç‰‡æ®µ]
    """
    kb_name = list(base.kb_list_to_dict([kb_name]).keys())[0]
    info_fragment = kb_doc_api.search_docs(query='', knowledge_base_name=kb_name,
                                           top_k=1, score_threshold=1,
                                           file_name=kb_file, metadata={})

    origin_docs = {}
    for x in info_fragment:
        origin_docs[x.id] = {"page_content": x.page_content,
                             "type": x.type,
                             "metadata": x.metadata}
    changed_docs = []
    for index, row in kb_dataFrame.iterrows():
        origin_doc = origin_docs.get(row['id'])
        if origin_doc:
            if row["åˆ é™¤"] not in ["Y", "y", 1, '1']:
                changed_docs.append({
                    "page_content": row["å†…å®¹"],
                    "type": origin_doc['type'],
                    "metadata": origin_doc["metadata"],
                })
            else:
                logger.warning(f'åˆ é™¤{row.get("id")}ç‰‡æ®µï¼š{origin_doc.get("page_content")}')
        else:
            changed_docs.append({
                "page_content": row["å†…å®¹"],
                "type": 'Document',
                "metadata": {"metadata": kb_file},
            })
    if changed_docs:
        resp = kb_doc_api.update_docs(
            knowledge_base_name=kb_name, file_names=[kb_file], docs={kb_file: changed_docs},
            chunk_size=kb_max, chunk_overlap=kb_similarity, override_custom_docs=False,
            loader_enhance=kb_loader, not_refresh_vs_cache=False,
            text_splitter_name=kb_tokenizer
        )
        if resp.code == 200 and not resp.data.get('failed_files'):
            last_fragment = __get_kb_fragment_df(kb_name, kb_file)
            yield gr.update(value=spike_toast('æ›´æ–°æˆåŠŸ'), visible=True), gr.update(value=last_fragment)
            time.sleep(1)
            yield gr.update(value=spike_toast('æ›´æ–°æˆåŠŸ'), visible=False), gr.update(value=last_fragment)
        else:
            yield gr.Error(f'æ›´æ–°å¤±è´¥, {resp.__dict__}')
