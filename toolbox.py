
import importlib
import time
import inspect
import re
import os
import base64
import gradio
import shutil
import glob
import json
import uuid
from loguru import logger
from functools import wraps
from textwrap import dedent
from shared_utils.config_loader import get_conf
from shared_utils.config_loader import set_conf
from shared_utils.config_loader import set_multi_conf
from shared_utils.config_loader import read_single_conf_with_lru_cache
from shared_utils.advanced_markdown_format import format_io
from shared_utils.advanced_markdown_format import markdown_convertion
from shared_utils.key_pattern_manager import select_api_key
from shared_utils.key_pattern_manager import is_any_api_key
from shared_utils.key_pattern_manager import what_keys
from shared_utils.connect_void_terminal import get_chat_handle
from shared_utils.connect_void_terminal import get_plugin_handle
from shared_utils.connect_void_terminal import get_plugin_default_kwargs
from shared_utils.connect_void_terminal import get_chat_default_kwargs
from shared_utils.text_mask import apply_gpt_academic_string_mask
from shared_utils.text_mask import build_gpt_academic_masked_string
from shared_utils.text_mask import apply_gpt_academic_string_mask_langbased
from shared_utils.text_mask import build_gpt_academic_masked_string_langbased
from shared_utils.map_names import map_friendly_names_to_model
from shared_utils.map_names import map_model_to_friendly_names
from shared_utils.map_names import read_one_api_model_name
from shared_utils.handle_upload import html_local_file
from shared_utils.handle_upload import html_local_img
from shared_utils.handle_upload import file_manifest_filter_type
from shared_utils.handle_upload import extract_archive
from typing import List
pj = os.path.join
default_user_name = "default_user"

"""
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
ç¬¬ä¸€éƒ¨åˆ†
å‡½æ•°æ’ä»¶è¾“å…¥è¾“å‡ºæ¥é©³åŒº
    - ChatBotWithCookies:   å¸¦Cookiesçš„Chatbotç±»ï¼Œä¸ºå®ç°æ›´å¤šå¼ºå¤§çš„åŠŸèƒ½åšåŸºç¡€
    - ArgsGeneralWrapper:   è£…é¥°å™¨å‡½æ•°ï¼Œç”¨äºé‡ç»„è¾“å…¥å‚æ•°ï¼Œæ”¹å˜è¾“å…¥å‚æ•°çš„é¡ºåºä¸ç»“æ„
    - update_ui:            åˆ·æ–°ç•Œé¢ç”¨ yield from update_ui(chatbot, history)
    - CatchException:       å°†æ’ä»¶ä¸­å‡ºçš„æ‰€æœ‰é—®é¢˜æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Š
    - HotReload:            å®ç°æ’ä»¶çš„çƒ­æ›´æ–°
    - trimmed_format_exc:   æ‰“å°tracebackï¼Œä¸ºäº†å®‰å…¨è€Œéšè—ç»å¯¹åœ°å€
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
"""


class ChatBotWithCookies(list):
    def __init__(self, cookie):
        """
        cookies = {
            'top_p': top_p,
            'temperature': temperature,
            'lock_plugin': bool,
            "files_to_promote": ["file1", "file2"],
            "most_recent_uploaded": {
                "path": "uploaded_path",
                "time": time.time(),
                "time_str": "timestr",
            }
        }
        """
        self._cookies = cookie

    def write_list(self, list):
        for t in list:
            self.append(t)

    def get_list(self):
        return [t for t in self]

    def get_cookies(self):
        return self._cookies

    def get_user(self):
        return self._cookies.get("user_name", default_user_name)

def ArgsGeneralWrapper(f):
    """
    è£…é¥°å™¨å‡½æ•°ArgsGeneralWrapperï¼Œç”¨äºé‡ç»„è¾“å…¥å‚æ•°ï¼Œæ”¹å˜è¾“å…¥å‚æ•°çš„é¡ºåºä¸ç»“æ„ã€‚
    è¯¥è£…é¥°å™¨æ˜¯å¤§å¤šæ•°åŠŸèƒ½è°ƒç”¨çš„å…¥å£ã€‚
    å‡½æ•°ç¤ºæ„å›¾ï¼šhttps://mermaid.live/edit#pako:eNqNVFtPGkEY_StkntoEDQtLoTw0sWqapjQxVWPabmOm7AiEZZcsQ9QiiW012qixqdeqqIn10geBh6ZR8PJnmAWe-hc6l3VhrWnLEzNzzvnO953ZyYOYoSIQAWOaMR5LQBN7hvoU3UN_g5iu7imAXEyT4wUF3Pd0dT3y9KGYYUJsmK8V0GPGs0-QjkyojZgwk0Fm82C2dVghX08U8EaoOHjOfoEMU0XmADRhOksVWnNLjdpM82qFzB6S5Q_WWsUhuqCc3JtAsVR_OoMnhyZwXgHWwbS1d4gnsLVZJp-P6mfVxveqAgqC70Jz_pQCOGDKM5xFdNNPDdilF6uSU_hOYqu4a3MHYDZLDzq5fodrC3PWcEaFGPUaRiqJWK_W9g9rvRITa4dhy_0nw67SiePMp3oSR6PPn41DGgllkvkizYwsrmtaejTFd8V4yekGmT1zqrt4XGlAy8WTuiPULF01LksZvukSajfQQRAxmYi5S0D81sDcyzapVdn6sYFHkjhhGyel3frVQnvsnbR23lEjlhIlaOJiFPWzU5G4tfNJo8ejwp47-TbvJkKKZvmxA6SKo16oaazJysfG6klr9T0pbTW2ZqzlL_XaT8fYbQLXe4mSmvoCZXMaa7FePW6s7jVqK9bujvse3WFjY5_Z4KfsA4oiPY4T7Drvn1tLJTbG1to1qR79ulgk89-oJbvZzbIwJty6u20LOReWa9BvwserUd9s9MIKc3x5TUWEoAhUyJK5y85w_yG-dFu_R9waoU7K581y8W_qLle35-rG9Nxcrz8QHRsc0K-r9NViYRT36KsFvCCNzDRMqvSVyzOKAnACpZECIvSvCs2UAhS9QHEwh43BST0GItjMIS_I8e-sLwnj9A262cxA_ZVh0OUY1LJiDSJ5MAEiUijYLUtBORR6KElyQPaCSRDpksNSd8AfluSgHPaFC17wjrOlbgbzyyFf4IFPDvoD_sJvnkdK-g
    """
    def decorated(request: gradio.Request, cookies:dict, max_length:int, llm_model:str,
                  txt:str, txt2:str, top_p:float, temperature:float, chatbot:list,
                  json_history:str, system_prompt:str, plugin_advanced_arg:dict, *args):
        txt_passon = txt
        history = json.loads(json_history) if json_history else []
        if txt == "" and txt2 != "": txt_passon = txt2
        # å¼•å…¥ä¸€ä¸ªæœ‰cookieçš„chatbot
        if request.username is not None:
            user_name = request.username
        else:
            user_name = default_user_name
        embed_model = get_conf("EMBEDDING_MODEL")
        cookies.update({
            'top_p': top_p,
            'api_key': cookies['api_key'],
            'llm_model': llm_model,
            'embed_model': embed_model,
            'temperature': temperature,
            'user_name': user_name,
        })
        llm_kwargs = {
            'api_key': cookies['api_key'],
            'llm_model': llm_model,
            'embed_model': embed_model,
            'top_p': top_p,
            'max_length': max_length,
            'temperature': temperature,
            'client_ip': request.client.host,
            'most_recent_uploaded': cookies.get('most_recent_uploaded')
        }
        if isinstance(plugin_advanced_arg, str):
            plugin_kwargs = {"advanced_arg": plugin_advanced_arg}
        else:
            plugin_kwargs = plugin_advanced_arg
        chatbot_with_cookie = ChatBotWithCookies(cookies)
        chatbot_with_cookie.write_list(chatbot)

        if cookies.get('lock_plugin', None) is None:
            # æ­£å¸¸çŠ¶æ€
            if len(args) == 0:  # æ’ä»¶é€šé“
                yield from f(txt_passon, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, system_prompt, request)
            else:               # å¯¹è¯é€šé“ï¼Œæˆ–è€…åŸºç¡€åŠŸèƒ½é€šé“
                yield from f(txt_passon, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, system_prompt, *args)
        else:
            # å¤„ç†å°‘æ•°æƒ…å†µä¸‹çš„ç‰¹æ®Šæ’ä»¶çš„é”å®šçŠ¶æ€
            module, fn_name = cookies['lock_plugin'].split('->')
            f_hot_reload = getattr(importlib.import_module(module, fn_name), fn_name)
            yield from f_hot_reload(txt_passon, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, system_prompt, request)
            # åˆ¤æ–­ä¸€ä¸‹ç”¨æˆ·æ˜¯å¦é”™è¯¯åœ°é€šè¿‡å¯¹è¯é€šé“è¿›å…¥ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿›è¡Œæé†’
            final_cookies = chatbot_with_cookie.get_cookies()
            # len(args) != 0 ä»£è¡¨â€œæäº¤â€é”®å¯¹è¯é€šé“ï¼Œæˆ–è€…åŸºç¡€åŠŸèƒ½é€šé“
            if len(args) != 0 and 'files_to_promote' in final_cookies and len(final_cookies['files_to_promote']) > 0:
                chatbot_with_cookie.append(
                    ["æ£€æµ‹åˆ°**æ»ç•™çš„ç¼“å­˜æ–‡æ¡£**ï¼Œè¯·åŠæ—¶å¤„ç†ã€‚", "è¯·åŠæ—¶ç‚¹å‡»â€œ**ä¿å­˜å½“å‰å¯¹è¯**â€è·å–æ‰€æœ‰æ»ç•™æ–‡æ¡£ã€‚"])
                yield from update_ui(chatbot_with_cookie, final_cookies['history'], msg="æ£€æµ‹åˆ°è¢«æ»ç•™çš„ç¼“å­˜æ–‡æ¡£")

    return decorated


def update_ui(chatbot:ChatBotWithCookies, history:list, msg:str="æ­£å¸¸", **kwargs):  # åˆ·æ–°ç•Œé¢
    """
    åˆ·æ–°ç”¨æˆ·ç•Œé¢
    """
    assert isinstance(history, list), "historyå¿…é¡»æ˜¯ä¸€ä¸ªlist"
    assert isinstance(
        chatbot, ChatBotWithCookies
    ), "åœ¨ä¼ é€’chatbotçš„è¿‡ç¨‹ä¸­ä¸è¦å°†å…¶ä¸¢å¼ƒã€‚å¿…è¦æ—¶, å¯ç”¨clearå°†å…¶æ¸…ç©º, ç„¶åç”¨for+appendå¾ªç¯é‡æ–°èµ‹å€¼ã€‚"
    cookies = chatbot.get_cookies()
    # å¤‡ä»½ä¸€ä»½Historyä½œä¸ºè®°å½•
    cookies.update({"history": history})
    # è§£å†³æ’ä»¶é”å®šæ—¶çš„ç•Œé¢æ˜¾ç¤ºé—®é¢˜
    if cookies.get("lock_plugin", None):
        label = (
            cookies.get("llm_model", "")
            + " | "
            + "æ­£åœ¨é”å®šæ’ä»¶"
            + cookies.get("lock_plugin", None)
        )
        chatbot_gr = gradio.update(value=chatbot, label=label)
        if cookies.get("label", "") != label:
            cookies["label"] = label  # è®°ä½å½“å‰çš„label
    elif cookies.get("label", None):
        chatbot_gr = gradio.update(value=chatbot, label=cookies.get("llm_model", ""))
        cookies["label"] = None  # æ¸…ç©ºlabel
    else:
        chatbot_gr = chatbot

    history = [str(history_item) for history_item in history] # ensure all items are string
    json_history = json.dumps(history, ensure_ascii=False)
    yield cookies, chatbot_gr, json_history, msg


def update_ui_lastest_msg(lastmsg:str, chatbot:ChatBotWithCookies, history:list, delay:float=1, msg:str="æ­£å¸¸"):  # åˆ·æ–°ç•Œé¢
    """
    åˆ·æ–°ç”¨æˆ·ç•Œé¢
    """
    if len(chatbot) == 0:
        chatbot.append(["update_ui_last_msg", lastmsg])
    chatbot[-1] = list(chatbot[-1])
    chatbot[-1][-1] = lastmsg
    yield from update_ui(chatbot=chatbot, history=history, msg=msg)
    time.sleep(delay)


def trimmed_format_exc():
    import os, traceback

    str = traceback.format_exc()
    current_path = os.getcwd()
    replace_path = "."
    return str.replace(current_path, replace_path)


def trimmed_format_exc_markdown():
    return '\n\n```\n' + trimmed_format_exc() + '```'


class FriendlyException(Exception):
    def generate_error_html(self):
        return dedent(f"""
            <div class="center-div" style="color: crimson;text-align: center;">
                {"<br>".join(self.args)}
            </div>
        """)


def CatchException(f):
    """
    è£…é¥°å™¨å‡½æ•°ï¼Œæ•æ‰å‡½æ•°fä¸­çš„å¼‚å¸¸å¹¶å°è£…åˆ°ä¸€ä¸ªç”Ÿæˆå™¨ä¸­è¿”å›ï¼Œå¹¶æ˜¾ç¤ºåˆ°èŠå¤©å½“ä¸­ã€‚
    """

    @wraps(f)
    def decorated(main_input:str, llm_kwargs:dict, plugin_kwargs:dict,
                  chatbot_with_cookie:ChatBotWithCookies, history:list, *args, **kwargs):
        try:
            yield from f(main_input, llm_kwargs, plugin_kwargs, chatbot_with_cookie, history, *args, **kwargs)
        except FriendlyException as e:
            tb_str = '```\n' + trimmed_format_exc() + '```'
            if len(chatbot_with_cookie) == 0:
                chatbot_with_cookie.clear()
                chatbot_with_cookie.append(["æ’ä»¶è°ƒåº¦å¼‚å¸¸:\n" + tb_str, None])
            chatbot_with_cookie[-1] = [chatbot_with_cookie[-1][0], e.generate_error_html()]
            yield from update_ui(chatbot=chatbot_with_cookie, history=history, msg=f'å¼‚å¸¸')  # åˆ·æ–°ç•Œé¢
        except Exception as e:
            tb_str = '```\n' + trimmed_format_exc() + '```'
            if len(chatbot_with_cookie) == 0:
                chatbot_with_cookie.clear()
                chatbot_with_cookie.append(["æ’ä»¶è°ƒåº¦å¼‚å¸¸", "å¼‚å¸¸åŸå› "])
            chatbot_with_cookie[-1] = [chatbot_with_cookie[-1][0], f"[Local Message] æ’ä»¶è°ƒç”¨å‡ºé”™: \n\n{tb_str} \n"]
            yield from update_ui(chatbot=chatbot_with_cookie, history=history, msg=f'å¼‚å¸¸ {e}')  # åˆ·æ–°ç•Œé¢

    return decorated


def HotReload(f):
    """
    HotReloadçš„è£…é¥°å™¨å‡½æ•°ï¼Œç”¨äºå®ç°Pythonå‡½æ•°æ’ä»¶çš„çƒ­æ›´æ–°ã€‚
    å‡½æ•°çƒ­æ›´æ–°æ˜¯æŒ‡åœ¨ä¸åœæ­¢ç¨‹åºè¿è¡Œçš„æƒ…å†µä¸‹ï¼Œæ›´æ–°å‡½æ•°ä»£ç ï¼Œä»è€Œè¾¾åˆ°å®æ—¶æ›´æ–°åŠŸèƒ½ã€‚
    åœ¨è£…é¥°å™¨å†…éƒ¨ï¼Œä½¿ç”¨wraps(f)æ¥ä¿ç•™å‡½æ•°çš„å…ƒä¿¡æ¯ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªåä¸ºdecoratedçš„å†…éƒ¨å‡½æ•°ã€‚
    å†…éƒ¨å‡½æ•°é€šè¿‡ä½¿ç”¨importlibæ¨¡å—çš„reloadå‡½æ•°å’Œinspectæ¨¡å—çš„getmoduleå‡½æ•°æ¥é‡æ–°åŠ è½½å¹¶è·å–å‡½æ•°æ¨¡å—ï¼Œ
    ç„¶åé€šè¿‡getattrå‡½æ•°è·å–å‡½æ•°åï¼Œå¹¶åœ¨æ–°æ¨¡å—ä¸­é‡æ–°åŠ è½½å‡½æ•°ã€‚
    æœ€åï¼Œä½¿ç”¨yield fromè¯­å¥è¿”å›é‡æ–°åŠ è½½è¿‡çš„å‡½æ•°ï¼Œå¹¶åœ¨è¢«è£…é¥°çš„å‡½æ•°ä¸Šæ‰§è¡Œã€‚
    æœ€ç»ˆï¼Œè£…é¥°å™¨å‡½æ•°è¿”å›å†…éƒ¨å‡½æ•°ã€‚è¿™ä¸ªå†…éƒ¨å‡½æ•°å¯ä»¥å°†å‡½æ•°çš„åŸå§‹å®šä¹‰æ›´æ–°ä¸ºæœ€æ–°ç‰ˆæœ¬ï¼Œå¹¶æ‰§è¡Œå‡½æ•°çš„æ–°ç‰ˆæœ¬ã€‚
    """
    if get_conf("PLUGIN_HOT_RELOAD"):

        @wraps(f)
        def decorated(*args, **kwargs):
            fn_name = f.__name__
            f_hot_reload = getattr(importlib.reload(inspect.getmodule(f)), fn_name)
            yield from f_hot_reload(*args, **kwargs)

        return decorated
    else:
        return f


"""
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
ç¬¬äºŒéƒ¨åˆ†
å…¶ä»–å°å·¥å…·:
    - write_history_to_file:    å°†ç»“æœå†™å…¥markdownæ–‡ä»¶ä¸­
    - regular_txt_to_markdown:  å°†æ™®é€šæ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æœ¬ã€‚
    - report_exception:         å‘chatbotä¸­æ·»åŠ ç®€å•çš„æ„å¤–é”™è¯¯ä¿¡æ¯
    - text_divide_paragraph:    å°†æ–‡æœ¬æŒ‰ç…§æ®µè½åˆ†éš”ç¬¦åˆ†å‰²å¼€ï¼Œç”Ÿæˆå¸¦æœ‰æ®µè½æ ‡ç­¾çš„HTMLä»£ç ã€‚
    - markdown_convertion:      ç”¨å¤šç§æ–¹å¼ç»„åˆï¼Œå°†markdownè½¬åŒ–ä¸ºå¥½çœ‹çš„html
    - format_io:                æ¥ç®¡gradioé»˜è®¤çš„markdownå¤„ç†æ–¹å¼
    - on_file_uploaded:         å¤„ç†æ–‡ä»¶çš„ä¸Šä¼ ï¼ˆè‡ªåŠ¨è§£å‹ï¼‰
    - on_report_generated:      å°†ç”Ÿæˆçš„æŠ¥å‘Šè‡ªåŠ¨æŠ•å°„åˆ°æ–‡ä»¶ä¸Šä¼ åŒº
    - clip_history:             å½“å†å²ä¸Šä¸‹æ–‡è¿‡é•¿æ—¶ï¼Œè‡ªåŠ¨æˆªæ–­
    - get_conf:                 è·å–è®¾ç½®
    - select_api_key:           æ ¹æ®å½“å‰çš„æ¨¡å‹ç±»åˆ«ï¼ŒæŠ½å–å¯ç”¨çš„api-key
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
"""


def get_reduce_token_percent(text:str):
    """
    * æ­¤å‡½æ•°æœªæ¥å°†è¢«å¼ƒç”¨
    """
    try:
        # text = "maximum context length is 4097 tokens. However, your messages resulted in 4870 tokens"
        pattern = r"(\d+)\s+tokens\b"
        match = re.findall(pattern, text)
        EXCEED_ALLO = 500  # ç¨å¾®ç•™ä¸€ç‚¹ä½™åœ°ï¼Œå¦åˆ™åœ¨å›å¤æ—¶ä¼šå› ä½™é‡å¤ªå°‘å‡ºé—®é¢˜
        max_limit = float(match[0]) - EXCEED_ALLO
        current_tokens = float(match[1])
        ratio = max_limit / current_tokens
        assert ratio > 0 and ratio < 1
        return ratio, str(int(current_tokens - max_limit))
    except:
        return 0.5, "ä¸è¯¦"


def write_history_to_file(
    history:list, file_basename:str=None, file_fullname:str=None, auto_caption:bool=True
):
    """
    å°†å¯¹è¯è®°å½•historyä»¥Markdownæ ¼å¼å†™å…¥æ–‡ä»¶ä¸­ã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶åï¼Œåˆ™ä½¿ç”¨å½“å‰æ—¶é—´ç”Ÿæˆæ–‡ä»¶åã€‚
    """
    import os
    import time

    if file_fullname is None:
        if file_basename is not None:
            file_fullname = pj(get_log_folder(), file_basename)
        else:
            file_fullname = pj(get_log_folder(), f"GPT-Academic-{gen_time_str()}.md")
    os.makedirs(os.path.dirname(file_fullname), exist_ok=True)
    with open(file_fullname, "w", encoding="utf8") as f:
        f.write("# GPT-Academic Report\n")
        for i, content in enumerate(history):
            try:
                if type(content) != str:
                    content = str(content)
            except:
                continue
            if i % 2 == 0 and auto_caption:
                f.write("## ")
            try:
                f.write(content)
            except:
                # remove everything that cannot be handled by utf8
                f.write(content.encode("utf-8", "ignore").decode())
            f.write("\n\n")
    res = os.path.abspath(file_fullname)
    return res


def regular_txt_to_markdown(text:str):
    """
    å°†æ™®é€šæ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æœ¬ã€‚
    """
    text = text.replace("\n", "\n\n")
    text = text.replace("\n\n\n", "\n\n")
    text = text.replace("\n\n\n", "\n\n")
    return text


def report_exception(chatbot:ChatBotWithCookies, history:list, a:str, b:str):
    """
    å‘chatbotä¸­æ·»åŠ é”™è¯¯ä¿¡æ¯
    """
    chatbot.append((a, b))
    history.extend([a, b])


def find_free_port()->int:
    """
    è¿”å›å½“å‰ç³»ç»Ÿä¸­å¯ç”¨çš„æœªä½¿ç”¨ç«¯å£ã€‚
    """
    import socket
    from contextlib import closing

    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(("", 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def find_recent_files(directory:str)->List[str]:
    """
    Find files that is created with in one minutes under a directory with python, write a function
    """
    import os
    import time

    current_time = time.time()
    one_minute_ago = current_time - 60
    recent_files = []
    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
    for filename in os.listdir(directory):
        file_path = pj(directory, filename)
        if file_path.endswith(".log"):
            continue
        created_time = os.path.getmtime(file_path)
        if created_time >= one_minute_ago:
            if os.path.isdir(file_path):
                continue
            recent_files.append(file_path)

    return recent_files


def file_already_in_downloadzone(file:str, user_path:str):
    try:
        parent_path = os.path.abspath(user_path)
        child_path = os.path.abspath(file)
        if os.path.samefile(os.path.commonpath([parent_path, child_path]), parent_path):
            return True
        else:
            return False
    except:
        return False


def promote_file_to_downloadzone(file:str, rename_file:str=None, chatbot:ChatBotWithCookies=None):
    # å°†æ–‡ä»¶å¤åˆ¶ä¸€ä»½åˆ°ä¸‹è½½åŒº
    import shutil

    if chatbot is not None:
        user_name = get_user(chatbot)
    else:
        user_name = default_user_name
    if not os.path.exists(file):
        raise FileNotFoundError(f"æ–‡ä»¶{file}ä¸å­˜åœ¨")
    user_path = get_log_folder(user_name, plugin_name=None)
    if file_already_in_downloadzone(file, user_path):
        new_path = file
    else:
        user_path = get_log_folder(user_name, plugin_name="downloadzone")
        if rename_file is None:
            rename_file = f"{gen_time_str()}-{os.path.basename(file)}"
        new_path = pj(user_path, rename_file)
        # å¦‚æœå·²ç»å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(new_path) and not os.path.samefile(new_path, file):
            os.remove(new_path)
        # æŠŠæ–‡ä»¶å¤åˆ¶è¿‡å»
        if not os.path.exists(new_path):
            shutil.copyfile(file, new_path)
    # å°†æ–‡ä»¶æ·»åŠ åˆ°chatbot cookieä¸­
    if chatbot is not None:
        if "files_to_promote" in chatbot._cookies:
            current = chatbot._cookies["files_to_promote"]
        else:
            current = []
        if new_path not in current:  # é¿å…æŠŠåŒä¸€ä¸ªæ–‡ä»¶æ·»åŠ å¤šæ¬¡
            chatbot._cookies.update({"files_to_promote": [new_path] + current})
    return new_path


def disable_auto_promotion(chatbot:ChatBotWithCookies):
    chatbot._cookies.update({"files_to_promote": []})
    return


def del_outdated_uploads(outdate_time_seconds:float, target_path_base:str=None):
    if target_path_base is None:
        user_upload_dir = get_conf("PATH_PRIVATE_UPLOAD")
    else:
        user_upload_dir = target_path_base
    current_time = time.time()
    one_hour_ago = current_time - outdate_time_seconds
    # Get a list of all subdirectories in the user_upload_dir folder
    # Remove subdirectories that are older than one hour
    for subdirectory in glob.glob(f"{user_upload_dir}/*"):
        subdirectory_time = os.path.getmtime(subdirectory)
        if subdirectory_time < one_hour_ago:
            try:
                shutil.rmtree(subdirectory)
            except:
                pass
    return



def to_markdown_tabs(head: list, tabs: list, alignment=":---:", column=False, omit_path=None):
    """
    Args:
        head: è¡¨å¤´ï¼š[]
        tabs: è¡¨å€¼ï¼š[[åˆ—1], [åˆ—2], [åˆ—3], [åˆ—4]]
        alignment: :--- å·¦å¯¹é½ï¼Œ :---: å±…ä¸­å¯¹é½ï¼Œ ---: å³å¯¹é½
        column: True to keep data in columns, False to keep data in rows (default).
    Returns:
        A string representation of the markdown table.
    """
    if column:
        transposed_tabs = list(map(list, zip(*tabs)))
    else:
        transposed_tabs = tabs
    # Find the maximum length among the columns
    max_len = max(len(column) for column in transposed_tabs)

    tab_format = "| %s "
    tabs_list = "".join([tab_format % i for i in head]) + "|\n"
    tabs_list += "".join([tab_format % alignment for i in head]) + "|\n"

    for i in range(max_len):
        row_data = [tab[i] if i < len(tab) else "" for tab in transposed_tabs]
        row_data = file_manifest_filter_type(row_data, filter_=None)
        # for dat in row_data:
        #     if (omit_path is not None) and os.path.exists(dat):
        #         dat = os.path.relpath(dat, omit_path)
        tabs_list += "".join([tab_format % i for i in row_data]) + "|\n"

    return tabs_list


def on_file_uploaded(
    request: gradio.Request, files:List[str], chatbot:ChatBotWithCookies,
    txt:str, txt2:str, checkboxes:List[str], cookies:dict
):
    """
    å½“æ–‡ä»¶è¢«ä¸Šä¼ æ—¶çš„å›è°ƒå‡½æ•°
    """
    if len(files) == 0:
        return chatbot, txt

    # åˆ›å»ºå·¥ä½œè·¯å¾„
    user_name = default_user_name if not request.username else request.username
    time_tag = gen_time_str()
    target_path_base = get_upload_folder(user_name, tag=time_tag)
    os.makedirs(target_path_base, exist_ok=True)

    # ç§»é™¤è¿‡æ—¶çš„æ—§æ–‡ä»¶ä»è€ŒèŠ‚çœç©ºé—´&ä¿æŠ¤éšç§
    outdate_time_seconds = 3600  # ä¸€å°æ—¶
    del_outdated_uploads(outdate_time_seconds, get_upload_folder(user_name))

    # é€ä¸ªæ–‡ä»¶è½¬ç§»åˆ°ç›®æ ‡è·¯å¾„
    upload_msg = ""
    for file in files:
        file_origin_name = os.path.basename(file.orig_name)
        this_file_path = pj(target_path_base, file_origin_name)
        shutil.move(file.name, this_file_path)
        upload_msg += extract_archive(
            file_path=this_file_path, dest_dir=this_file_path + ".extract"
        )

    # æ•´ç†æ–‡ä»¶é›†åˆ è¾“å‡ºæ¶ˆæ¯
    files = glob.glob(f"{target_path_base}/**/*", recursive=True)
    moved_files = [fp for fp in files]
    max_file_to_show = 10
    if len(moved_files) > max_file_to_show:
        moved_files = moved_files[:max_file_to_show//2] + [f'... ( ğŸ“Œçœç•¥{len(moved_files) - max_file_to_show}ä¸ªæ–‡ä»¶çš„æ˜¾ç¤º ) ...'] + \
                      moved_files[-max_file_to_show//2:]
    moved_files_str = to_markdown_tabs(head=["æ–‡ä»¶"], tabs=[moved_files], omit_path=target_path_base)
    chatbot.append(
        [
            "æˆ‘ä¸Šä¼ äº†æ–‡ä»¶ï¼Œè¯·æŸ¥æ”¶",
            f"[Local Message] æ”¶åˆ°ä»¥ä¸‹æ–‡ä»¶ ï¼ˆä¸Šä¼ åˆ°è·¯å¾„ï¼š{target_path_base}ï¼‰: " +
            f"\n\n{moved_files_str}" +
            f"\n\nè°ƒç”¨è·¯å¾„å‚æ•°å·²è‡ªåŠ¨ä¿®æ­£åˆ°: \n\n{txt}" +
            f"\n\nç°åœ¨æ‚¨ç‚¹å‡»ä»»æ„å‡½æ•°æ’ä»¶æ—¶ï¼Œä»¥ä¸Šæ–‡ä»¶å°†è¢«ä½œä¸ºè¾“å…¥å‚æ•°" +
            upload_msg,
        ]
    )

    txt, txt2 = target_path_base, ""
    if "æµ®åŠ¨è¾“å…¥åŒº" in checkboxes:
        txt, txt2 = txt2, txt

    # è®°å½•è¿‘æœŸæ–‡ä»¶
    cookies.update(
        {
            "most_recent_uploaded": {
                "path": target_path_base,
                "time": time.time(),
                "time_str": time_tag,
            }
        }
    )
    return chatbot, txt, txt2, cookies


def generate_file_link(report_files:List[str]):
    file_links = ""
    for f in report_files:
        file_links += (
            f'<br/><a href="file={os.path.abspath(f)}" target="_blank">{f}</a>'
        )
    return file_links


def on_report_generated(cookies:dict, files:List[str], chatbot:ChatBotWithCookies):
    if "files_to_promote" in cookies:
        report_files = cookies["files_to_promote"]
        cookies.pop("files_to_promote")
    else:
        report_files = []
    if len(report_files) == 0:
        return cookies, None, chatbot
    file_links = ""
    for f in report_files:
        file_links += (
            f'<br/><a href="file={os.path.abspath(f)}" target="_blank">{f}</a>'
        )
    chatbot.append(["æŠ¥å‘Šå¦‚ä½•è¿œç¨‹è·å–ï¼Ÿ", f"æŠ¥å‘Šå·²ç»æ·»åŠ åˆ°å³ä¾§â€œæ–‡ä»¶ä¸‹è½½åŒºâ€ï¼ˆå¯èƒ½å¤„äºæŠ˜å çŠ¶æ€ï¼‰ï¼Œè¯·æŸ¥æ”¶ã€‚{file_links}"])
    return cookies, report_files, chatbot


def load_chat_cookies():
    API_KEY, LLM_MODEL, AZURE_API_KEY = get_conf(
        "API_KEY", "LLM_MODEL", "AZURE_API_KEY"
    )
    AZURE_CFG_ARRAY, NUM_CUSTOM_BASIC_BTN = get_conf(
        "AZURE_CFG_ARRAY", "NUM_CUSTOM_BASIC_BTN"
    )

    # deal with azure openai key
    if is_any_api_key(AZURE_API_KEY):
        if is_any_api_key(API_KEY):
            API_KEY = API_KEY + "," + AZURE_API_KEY
        else:
            API_KEY = AZURE_API_KEY
    if len(AZURE_CFG_ARRAY) > 0:
        for azure_model_name, azure_cfg_dict in AZURE_CFG_ARRAY.items():
            if not azure_model_name.startswith("azure"):
                raise ValueError("AZURE_CFG_ARRAYä¸­é…ç½®çš„æ¨¡å‹å¿…é¡»ä»¥azureå¼€å¤´")
            AZURE_API_KEY_ = azure_cfg_dict["AZURE_API_KEY"]
            if is_any_api_key(AZURE_API_KEY_):
                if is_any_api_key(API_KEY):
                    API_KEY = API_KEY + "," + AZURE_API_KEY_
                else:
                    API_KEY = AZURE_API_KEY_

    customize_fn_overwrite_ = {}
    for k in range(NUM_CUSTOM_BASIC_BTN):
        customize_fn_overwrite_.update(
            {
                "è‡ªå®šä¹‰æŒ‰é’®"
                + str(k + 1): {
                    "Title": r"",
                    "Prefix": r"è¯·åœ¨è‡ªå®šä¹‰èœå•ä¸­å®šä¹‰æç¤ºè¯å‰ç¼€.",
                    "Suffix": r"è¯·åœ¨è‡ªå®šä¹‰èœå•ä¸­å®šä¹‰æç¤ºè¯åç¼€",
                }
            }
        )

    EMBEDDING_MODEL = get_conf("EMBEDDING_MODEL")
    return {
        "api_key": API_KEY,
        "llm_model": LLM_MODEL,
        "embed_model": EMBEDDING_MODEL,
        "customize_fn_overwrite": customize_fn_overwrite_,
    }


def clear_line_break(txt):
    txt = txt.replace("\n", " ")
    txt = txt.replace("  ", " ")
    txt = txt.replace("  ", " ")
    return txt


class DummyWith:
    """
    è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºDummyWithçš„ç©ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œ
    å®ƒçš„ä½œç”¨æ˜¯â€¦â€¦é¢â€¦â€¦å°±æ˜¯ä¸èµ·ä½œç”¨ï¼Œå³åœ¨ä»£ç ç»“æ„ä¸å˜å¾—æƒ…å†µä¸‹å–ä»£å…¶ä»–çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ˜¯ä¸€ç§Pythonå¯¹è±¡ï¼Œç”¨äºä¸withè¯­å¥ä¸€èµ·ä½¿ç”¨ï¼Œ
    ä»¥ç¡®ä¿ä¸€äº›èµ„æºåœ¨ä»£ç å—æ‰§è¡ŒæœŸé—´å¾—åˆ°æ­£ç¡®çš„åˆå§‹åŒ–å’Œæ¸…ç†ã€‚
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¿…é¡»å®ç°ä¸¤ä¸ªæ–¹æ³•ï¼Œåˆ†åˆ«ä¸º __enter__()å’Œ __exit__()ã€‚
    åœ¨ä¸Šä¸‹æ–‡æ‰§è¡Œå¼€å§‹çš„æƒ…å†µä¸‹ï¼Œ__enter__()æ–¹æ³•ä¼šåœ¨ä»£ç å—è¢«æ‰§è¡Œå‰è¢«è°ƒç”¨ï¼Œ
    è€Œåœ¨ä¸Šä¸‹æ–‡æ‰§è¡Œç»“æŸæ—¶ï¼Œ__exit__()æ–¹æ³•åˆ™ä¼šè¢«è°ƒç”¨ã€‚
    """

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        return


def run_gradio_in_subpath(demo, auth, port, custom_path):
    """
    æŠŠgradioçš„è¿è¡Œåœ°å€æ›´æ”¹åˆ°æŒ‡å®šçš„äºŒæ¬¡è·¯å¾„ä¸Š
    """

    def is_path_legal(path: str) -> bool:
        """
        check path for sub url
        path: path to check
        return value: do sub url wrap
        """
        if path == "/":
            return True
        if len(path) == 0:
            logger.info(
                "ilegal custom path: {}\npath must not be empty\ndeploy on root url".format(
                    path
                )
            )
            return False
        if path[0] == "/":
            if path[1] != "/":
                logger.info("deploy on sub-path {}".format(path))
                return True
            return False
        logger.info(
            "ilegal custom path: {}\npath should begin with '/'\ndeploy on root url".format(
                path
            )
        )
        return False

    if not is_path_legal(custom_path):
        raise RuntimeError("Ilegal custom path")
    import uvicorn
    import gradio as gr
    from fastapi import FastAPI

    app = FastAPI()
    if custom_path != "/":

        @app.get("/")
        def read_main():
            return {"message": f"Gradio is running at: {custom_path}"}

    app = gr.mount_gradio_app(app, demo, path=custom_path)
    uvicorn.run(app, host="0.0.0.0", port=port)  # , auth=auth


def clip_history(inputs, history, tokenizer, max_token_limit):
    """
    reduce the length of history by clipping.
    this function search for the longest entries to clip, little by little,
    until the number of token of history is reduced under threshold.
    é€šè¿‡è£å‰ªæ¥ç¼©çŸ­å†å²è®°å½•çš„é•¿åº¦ã€‚
    æ­¤å‡½æ•°é€æ¸åœ°æœç´¢æœ€é•¿çš„æ¡ç›®è¿›è¡Œå‰ªè¾‘ï¼Œ
    ç›´åˆ°å†å²è®°å½•çš„æ ‡è®°æ•°é‡é™ä½åˆ°é˜ˆå€¼ä»¥ä¸‹ã€‚
    """
    import numpy as np
    from request_llms.bridge_all import model_info

    def get_token_num(txt):
        return len(tokenizer.encode(txt, disallowed_special=()))

    input_token_num = get_token_num(inputs)

    if max_token_limit < 5000:
        output_token_expect = 256  # 4k & 2k models
    elif max_token_limit < 9000:
        output_token_expect = 512  # 8k models
    else:
        output_token_expect = 1024  # 16k & 32k models

    if input_token_num < max_token_limit * 3 / 4:
        # å½“è¾“å…¥éƒ¨åˆ†çš„tokenå æ¯”å°äºé™åˆ¶çš„3/4æ—¶ï¼Œè£å‰ªæ—¶
        # 1. æŠŠinputçš„ä½™é‡ç•™å‡ºæ¥
        max_token_limit = max_token_limit - input_token_num
        # 2. æŠŠè¾“å‡ºç”¨çš„ä½™é‡ç•™å‡ºæ¥
        max_token_limit = max_token_limit - output_token_expect
        # 3. å¦‚æœä½™é‡å¤ªå°äº†ï¼Œç›´æ¥æ¸…é™¤å†å²
        if max_token_limit < output_token_expect:
            history = []
            return history
    else:
        # å½“è¾“å…¥éƒ¨åˆ†çš„tokenå æ¯” > é™åˆ¶çš„3/4æ—¶ï¼Œç›´æ¥æ¸…é™¤å†å²
        history = []
        return history

    everything = [""]
    everything.extend(history)
    n_token = get_token_num("\n".join(everything))
    everything_token = [get_token_num(e) for e in everything]

    # æˆªæ–­æ—¶çš„é¢—ç²’åº¦
    delta = max(everything_token) // 16

    while n_token > max_token_limit:
        where = np.argmax(everything_token)
        encoded = tokenizer.encode(everything[where], disallowed_special=())
        clipped_encoded = encoded[: len(encoded) - delta]
        everything[where] = tokenizer.decode(clipped_encoded)[
            :-1
        ]  # -1 to remove the may-be illegal char
        everything_token[where] = get_token_num(everything[where])
        n_token = get_token_num("\n".join(everything))

    history = everything[1:]
    return history


"""
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
ç¬¬ä¸‰éƒ¨åˆ†
å…¶ä»–å°å·¥å…·:
    - zip_folder:    æŠŠæŸä¸ªè·¯å¾„ä¸‹æ‰€æœ‰æ–‡ä»¶å‹ç¼©ï¼Œç„¶åè½¬ç§»åˆ°æŒ‡å®šçš„å¦ä¸€ä¸ªè·¯å¾„ä¸­ï¼ˆgptå†™çš„ï¼‰
    - gen_time_str:  ç”Ÿæˆæ—¶é—´æˆ³
    - ProxyNetworkActivate: ä¸´æ—¶åœ°å¯åŠ¨ä»£ç†ç½‘ç»œï¼ˆå¦‚æœæœ‰ï¼‰
    - objdump/objload: å¿«æ·çš„è°ƒè¯•å‡½æ•°
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
"""


def zip_folder(source_folder, dest_folder, zip_name):
    import zipfile
    import os

    # Make sure the source folder exists
    if not os.path.exists(source_folder):
        logger.info(f"{source_folder} does not exist")
        return

    # Make sure the destination folder exists
    if not os.path.exists(dest_folder):
        logger.info(f"{dest_folder} does not exist")
        return

    # Create the name for the zip file
    zip_file = pj(dest_folder, zip_name)

    # Create a ZipFile object
    with zipfile.ZipFile(zip_file, "w", zipfile.ZIP_DEFLATED) as zipf:
        # Walk through the source folder and add files to the zip file
        for foldername, subfolders, filenames in os.walk(source_folder):
            for filename in filenames:
                filepath = pj(foldername, filename)
                zipf.write(filepath, arcname=os.path.relpath(filepath, source_folder))

    # Move the zip file to the destination folder (if it wasn't already there)
    if os.path.dirname(zip_file) != dest_folder:
        os.rename(zip_file, pj(dest_folder, os.path.basename(zip_file)))
        zip_file = pj(dest_folder, os.path.basename(zip_file))

    logger.info(f"Zip file created at {zip_file}")


def zip_result(folder):
    t = gen_time_str()
    zip_folder(folder, get_log_folder(), f"{t}-result.zip")
    return pj(get_log_folder(), f"{t}-result.zip")


def gen_time_str():
    import time

    return time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())


def get_log_folder(user=default_user_name, plugin_name="shared"):
    if user is None:
        user = default_user_name
    PATH_LOGGING = get_conf("PATH_LOGGING")
    if plugin_name is None:
        _dir = pj(PATH_LOGGING, user)
    else:
        _dir = pj(PATH_LOGGING, user, plugin_name)
    if not os.path.exists(_dir):
        os.makedirs(_dir)
    return _dir


def get_upload_folder(user=default_user_name, tag=None):
    PATH_PRIVATE_UPLOAD = get_conf("PATH_PRIVATE_UPLOAD")
    if user is None:
        user = default_user_name
    if tag is None or len(tag) == 0:
        target_path_base = pj(PATH_PRIVATE_UPLOAD, user)
    else:
        target_path_base = pj(PATH_PRIVATE_UPLOAD, user, tag)
    return target_path_base


def is_the_upload_folder(string):
    PATH_PRIVATE_UPLOAD = get_conf("PATH_PRIVATE_UPLOAD")
    pattern = r"^PATH_PRIVATE_UPLOAD[\\/][A-Za-z0-9_-]+[\\/]\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2}$"
    pattern = pattern.replace("PATH_PRIVATE_UPLOAD", PATH_PRIVATE_UPLOAD)
    if re.match(pattern, string):
        return True
    else:
        return False


def get_user(chatbotwithcookies:ChatBotWithCookies):
    return chatbotwithcookies._cookies.get("user_name", default_user_name)


class ProxyNetworkActivate:
    """
    è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºProxyNetworkActivateçš„ç©ºä¸Šä¸‹æ–‡ç®¡ç†å™¨, ç”¨äºç»™ä¸€å°æ®µä»£ç ä¸Šä»£ç†
    """

    def __init__(self, task=None) -> None:
        self.task = task
        if not task:
            # ä¸ç»™å®štask, é‚£ä¹ˆæˆ‘ä»¬é»˜è®¤ä»£ç†ç”Ÿæ•ˆ
            self.valid = True
        else:
            # ç»™å®šäº†task, æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹
            from toolbox import get_conf

            WHEN_TO_USE_PROXY = get_conf("WHEN_TO_USE_PROXY")
            self.valid = task in WHEN_TO_USE_PROXY

    def __enter__(self):
        if not self.valid:
            return self
        from toolbox import get_conf

        proxies = get_conf("proxies")
        if "no_proxy" in os.environ:
            os.environ.pop("no_proxy")
        if proxies is not None:
            if "http" in proxies:
                os.environ["HTTP_PROXY"] = proxies["http"]
            if "https" in proxies:
                os.environ["HTTPS_PROXY"] = proxies["https"]
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        os.environ["no_proxy"] = "*"
        if "HTTP_PROXY" in os.environ:
            os.environ.pop("HTTP_PROXY")
        if "HTTPS_PROXY" in os.environ:
            os.environ.pop("HTTPS_PROXY")
        return


def Singleton(cls):
    """
    ä¸€ä¸ªå•å®ä¾‹è£…é¥°å™¨
    """
    _instance = {}

    def _singleton(*args, **kargs):
        if cls not in _instance:
            _instance[cls] = cls(*args, **kargs)
        return _instance[cls]

    return _singleton


def get_pictures_list(path):
    file_manifest = [f for f in glob.glob(f"{path}/**/*.jpg", recursive=True)]
    file_manifest += [f for f in glob.glob(f"{path}/**/*.jpeg", recursive=True)]
    file_manifest += [f for f in glob.glob(f"{path}/**/*.png", recursive=True)]
    return file_manifest


def have_any_recent_upload_image_files(chatbot:ChatBotWithCookies, pop:bool=False):
    _5min = 5 * 60
    if chatbot is None:
        return False, None  # chatbot is None
    if pop:
        most_recent_uploaded = chatbot._cookies.pop("most_recent_uploaded", None)
    else:
        most_recent_uploaded = chatbot._cookies.get("most_recent_uploaded", None)
    # most_recent_uploaded æ˜¯ä¸€ä¸ªæ”¾ç½®æœ€æ–°ä¸Šä¼ å›¾åƒçš„è·¯å¾„
    if not most_recent_uploaded:
        return False, None  # most_recent_uploaded is None
    if time.time() - most_recent_uploaded["time"] < _5min:
        path = most_recent_uploaded["path"]
        file_manifest = get_pictures_list(path)
        if len(file_manifest) == 0:
            return False, None
        return True, file_manifest  # most_recent_uploaded is new
    else:
        return False, None  # most_recent_uploaded is too old

# Claude3 model supports graphic context dialogue, reads all images
def every_image_file_in_path(chatbot:ChatBotWithCookies):
    if chatbot is None:
        return False, []  # chatbot is None
    most_recent_uploaded = chatbot._cookies.get("most_recent_uploaded", None)
    if not most_recent_uploaded:
        return False, []  # most_recent_uploaded is None
    path = most_recent_uploaded["path"]
    file_manifest = get_pictures_list(path)
    if len(file_manifest) == 0:
        return False, []
    return True, file_manifest

# Function to encode the image
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def get_max_token(llm_kwargs):
    from request_llms.bridge_all import model_info

    return model_info[llm_kwargs["llm_model"]]["max_token"]


def check_packages(packages=[]):
    import importlib.util

    for p in packages:
        spam_spec = importlib.util.find_spec(p)
        if spam_spec is None:
            raise ModuleNotFoundError


def map_file_to_sha256(file_path):
    import hashlib

    with open(file_path, 'rb') as file:
        content = file.read()

    # Calculate the SHA-256 hash of the file contents
    sha_hash = hashlib.sha256(content).hexdigest()

    return sha_hash


def check_repeat_upload(new_pdf_path, pdf_hash):
    '''
    æ£€æŸ¥å†å²ä¸Šä¼ çš„æ–‡ä»¶æ˜¯å¦ä¸æ–°ä¸Šä¼ çš„æ–‡ä»¶ç›¸åŒï¼Œå¦‚æœç›¸åŒåˆ™è¿”å›(True, é‡å¤æ–‡ä»¶è·¯å¾„)ï¼Œå¦åˆ™è¿”å›(Falseï¼ŒNone)
    '''
    from toolbox import get_conf
    import PyPDF2

    user_upload_dir = os.path.dirname(os.path.dirname(new_pdf_path))
    file_name = os.path.basename(new_pdf_path)

    file_manifest = [f for f in glob.glob(f'{user_upload_dir}/**/{file_name}', recursive=True)]

    for saved_file in file_manifest:
        with open(new_pdf_path, 'rb') as file1, open(saved_file, 'rb') as file2:
            reader1 = PyPDF2.PdfFileReader(file1)
            reader2 = PyPDF2.PdfFileReader(file2)

            # æ¯”è¾ƒé¡µæ•°æ˜¯å¦ç›¸åŒ
            if reader1.getNumPages() != reader2.getNumPages():
                continue

            # æ¯”è¾ƒæ¯ä¸€é¡µçš„å†…å®¹æ˜¯å¦ç›¸åŒ
            for page_num in range(reader1.getNumPages()):
                page1 = reader1.getPage(page_num).extractText()
                page2 = reader2.getPage(page_num).extractText()
                if page1 != page2:
                    continue

        maybe_project_dir = glob.glob('{}/**/{}'.format(get_log_folder(), pdf_hash + ".tag"), recursive=True)


        if len(maybe_project_dir) > 0:
            return True, os.path.dirname(maybe_project_dir[0])

    # å¦‚æœæ‰€æœ‰é¡µçš„å†…å®¹éƒ½ç›¸åŒï¼Œè¿”å› True
    return False, None

def log_chat(llm_model: str, input_str: str, output_str: str):
    try:
        if output_str and input_str and llm_model:
            uid = str(uuid.uuid4().hex)
            input_str = input_str.rstrip('\n')
            output_str = output_str.rstrip('\n')
            logger.bind(chat_msg=True).info(dedent(
            """
            â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
            [UID]
            {uid}
            [Model]
            {llm_model}
            [Query]
            {input_str}
            [Response]
            {output_str}
            â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
            """).format(uid=uid, llm_model=llm_model, input_str=input_str, output_str=output_str))
    except:
        logger.error(trimmed_format_exc())
