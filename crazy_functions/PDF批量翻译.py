from toolbox import CatchException, report_exception, get_log_folder, gen_time_str, check_packages
from toolbox import update_ui, promote_file_to_downloadzone, update_ui_lastest_msg, disable_auto_promotion
from toolbox import write_history_to_file, promote_file_to_downloadzone, get_conf, extract_archive
from toolbox import generate_file_link, zip_folder, trimmed_format_exc, trimmed_format_exc_markdown
from .crazy_utils import request_gpt_model_in_new_thread_with_ui_alive
from .crazy_utils import request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency
from .crazy_utils import read_and_clean_pdf_text
from .crazy_utils import get_files_from_everything
from .pdf_fns.parse_pdf import parse_pdf, get_avail_grobid_url, translate_pdf
from colorful import *
import os


@CatchException
def æ‰¹é‡ç¿»è¯‘PDFæ–‡æ¡£(txt, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, user_request):

    disable_auto_promotion(chatbot)
    # åŸºæœ¬ä¿¡æ¯ï¼šåŠŸèƒ½ã€è´¡çŒ®è€…
    chatbot.append([None, "æ’ä»¶åŠŸèƒ½ï¼šæ‰¹é‡ç¿»è¯‘PDFæ–‡æ¡£ã€‚å‡½æ•°æ’ä»¶è´¡çŒ®è€…: Binary-Husky"])
    yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢

    # å°è¯•å¯¼å…¥ä¾èµ–ï¼Œå¦‚æœç¼ºå°‘ä¾èµ–ï¼Œåˆ™ç»™å‡ºå®‰è£…å»ºè®®
    try:
        check_packages(["fitz", "tiktoken", "scipdf"])
    except:
        report_exception(chatbot, history,
                         a=f"è§£æé¡¹ç›®: {txt}",
                         b=f"å¯¼å…¥è½¯ä»¶ä¾èµ–å¤±è´¥ã€‚ä½¿ç”¨è¯¥æ¨¡å—éœ€è¦é¢å¤–ä¾èµ–ï¼Œå®‰è£…æ–¹æ³•```pip install --upgrade pymupdf tiktoken scipdf_parser```ã€‚")
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
        return

    # æ¸…ç©ºå†å²ï¼Œä»¥å…è¾“å…¥æº¢å‡º
    history = []

    success, file_manifest, project_folder = get_files_from_everything(txt, type='.pdf')
    # æ£€æµ‹è¾“å…¥å‚æ•°ï¼Œå¦‚æ²¡æœ‰ç»™å®šè¾“å…¥å‚æ•°ï¼Œç›´æ¥é€€å‡º
    if not success:
        if txt == "": txt = 'ç©ºç©ºå¦‚ä¹Ÿçš„è¾“å…¥æ '

    # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•æ–‡ä»¶
    if len(file_manifest) == 0:
        report_exception(chatbot, history,
                         a=f"è§£æé¡¹ç›®: {txt}", b=f"æ‰¾ä¸åˆ°ä»»ä½•.pdfæ‹“å±•åçš„æ–‡ä»¶: {txt}")
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
        return

    # å¼€å§‹æ­£å¼æ‰§è¡Œä»»åŠ¡
    DOC2X_API_KEY = get_conf("DOC2X_API_KEY")
    # ------- ç¬¬ä¸€ç§æ–¹æ³•ï¼Œæ•ˆæœæœ€å¥½ï¼Œä½†æ˜¯éœ€è¦DOC2XæœåŠ¡ -------
    if len(DOC2X_API_KEY) != 0:
        try:
            yield from è§£æPDF_DOC2X(file_manifest, project_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, DOC2X_API_KEY, user_request)
            return
        except:
            chatbot.append([None, f"DOC2XæœåŠ¡ä¸å¯ç”¨ï¼Œç°åœ¨å°†æ‰§è¡Œæ•ˆæœç¨å·®çš„æ—§ç‰ˆä»£ç ã€‚{trimmed_format_exc_markdown()}"])
            yield from update_ui(chatbot=chatbot, history=history)

    # ------- ç¬¬äºŒç§æ–¹æ³•ï¼Œæ•ˆæœæ¬¡ä¼˜ -------
    grobid_url = get_avail_grobid_url()
    if grobid_url is not None:
        yield from è§£æPDF_åŸºäºGROBID(file_manifest, project_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, grobid_url)
        return

    # ------- ç¬¬ä¸‰ç§æ–¹æ³•ï¼Œæ—©æœŸä»£ç ï¼Œæ•ˆæœä¸ç†æƒ³ -------
    yield from update_ui_lastest_msg("GROBIDæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥configä¸­çš„GROBID_URLã€‚ä½œä¸ºæ›¿ä»£ï¼Œç°åœ¨å°†æ‰§è¡Œæ•ˆæœç¨å·®çš„æ—§ç‰ˆä»£ç ã€‚", chatbot, history, delay=3)
    yield from è§£æPDF(file_manifest, project_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt)
    return



def è§£æPDF_DOC2X_å•æ–‡ä»¶(fp, project_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, DOC2X_API_KEY, user_request):

    def refresh_key(doc2x_api_key):
        import requests, json
        url = "https://api.doc2x.noedgeai.com/api/token/refresh"
        res = requests.post(
            url,
            headers={"Authorization": "Bearer " + doc2x_api_key}
        )
        res_json = []
        if res.status_code == 200:
            decoded = res.content.decode("utf-8")
            res_json = json.loads(decoded)
            doc2x_api_key = res_json['data']['token']
        else:
            raise RuntimeError(format("[ERROR] status code: %d, body: %s" % (res.status_code, res.text)))
        return doc2x_api_key

    def pdf2markdown(filepath):
        import requests, json, os
        markdown_dir = get_log_folder(plugin_name="pdf_ocr")
        doc2x_api_key = DOC2X_API_KEY
        if doc2x_api_key.startswith('sk-'):
            url = "https://api.doc2x.noedgeai.com/api/v1/pdf"
        else:
            doc2x_api_key = refresh_key(doc2x_api_key)
            url = "https://api.doc2x.noedgeai.com/api/platform/pdf"

        chatbot.append((None, "åŠ è½½PDFæ–‡ä»¶ï¼Œå‘é€è‡³DOC2Xè§£æ..."))
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢

        res = requests.post(
            url,
            files={"file": open(filepath, "rb")},
            data={"ocr": "1"},
            headers={"Authorization": "Bearer " + doc2x_api_key}
        )
        res_json = []
        if res.status_code == 200:
            decoded = res.content.decode("utf-8")
            for z_decoded in decoded.split('\n'):
                if len(z_decoded) == 0: continue
                assert z_decoded.startswith("data: ")
                z_decoded = z_decoded[len("data: "):]
                decoded_json = json.loads(z_decoded)
                res_json.append(decoded_json)
        else:
            raise RuntimeError(format("[ERROR] status code: %d, body: %s" % (res.status_code, res.text)))
        uuid = res_json[0]['uuid']
        to = "md" # latex, md, docx
        url = "https://api.doc2x.noedgeai.com/api/export"+"?request_id="+uuid+"&to="+to

        chatbot.append((None, f"è¯»å–è§£æ: {url} ..."))
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢

        res = requests.get(url, headers={"Authorization": "Bearer " + doc2x_api_key})
        md_zip_path = os.path.join(markdown_dir, gen_time_str() + '.zip')
        if res.status_code == 200:
            with open(md_zip_path, "wb") as f: f.write(res.content)
        else:
            raise RuntimeError(format("[ERROR] status code: %d, body: %s" % (res.status_code, res.text)))
        promote_file_to_downloadzone(md_zip_path, chatbot=chatbot)
        chatbot.append((None, f"å®Œæˆè§£æ {md_zip_path} ..."))
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
        return md_zip_path

    def deliver_to_markdown_plugin(md_zip_path, user_request):
        from crazy_functions.æ‰¹é‡Markdownç¿»è¯‘ import Markdownè‹±è¯‘ä¸­
        import shutil, re

        time_tag = gen_time_str()
        target_path_base = get_log_folder(chatbot.get_user())
        file_origin_name = os.path.basename(md_zip_path)
        this_file_path = os.path.join(target_path_base, file_origin_name)
        os.makedirs(target_path_base, exist_ok=True)
        shutil.copyfile(md_zip_path, this_file_path)
        ex_folder = this_file_path + ".extract"
        extract_archive(
            file_path=this_file_path, dest_dir=ex_folder
        )

        # edit markdown files
        success, file_manifest, project_folder = get_files_from_everything(ex_folder, type='.md')
        for generated_fp in file_manifest:
            # ä¿®æ­£ä¸€äº›å…¬å¼é—®é¢˜
            with open(generated_fp, 'r', encoding='utf8') as f:
                content = f.read()
            # å°†å…¬å¼ä¸­çš„\[ \]æ›¿æ¢æˆ$$
            content = content.replace(r'\[', r'$$').replace(r'\]', r'$$')
            # å°†å…¬å¼ä¸­çš„\( \)æ›¿æ¢æˆ$
            content = content.replace(r'\(', r'$').replace(r'\)', r'$')
            content = content.replace('```markdown', '\n').replace('```', '\n')
            with open(generated_fp, 'w', encoding='utf8') as f:
                f.write(content)
            promote_file_to_downloadzone(generated_fp, chatbot=chatbot)
            yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢

            # ç”Ÿæˆåœ¨çº¿é¢„è§ˆhtml
            file_name = 'åœ¨çº¿é¢„è§ˆç¿»è¯‘ï¼ˆåŸæ–‡ï¼‰' + gen_time_str() + '.html'
            preview_fp = os.path.join(ex_folder, file_name)
            from shared_utils.advanced_markdown_format import markdown_convertion_for_file
            with open(generated_fp, "r", encoding="utf-8") as f:
                md = f.read()
                # Markdownä¸­ä½¿ç”¨ä¸æ ‡å‡†çš„è¡¨æ ¼ï¼Œéœ€è¦åœ¨è¡¨æ ¼å‰åŠ ä¸Šä¸€ä¸ªemojiï¼Œä»¥ä¾¿å…¬å¼æ¸²æŸ“
                md = re.sub(r'^<table>', r'ğŸ˜ƒ<table>', md, flags=re.MULTILINE)
            html = markdown_convertion_for_file(md)
            with open(preview_fp, "w", encoding="utf-8") as f: f.write(html)
            chatbot.append([None, f"ç”Ÿæˆåœ¨çº¿é¢„è§ˆï¼š{generate_file_link([preview_fp])}"])
            promote_file_to_downloadzone(preview_fp, chatbot=chatbot)



        chatbot.append((None, f"è°ƒç”¨Markdownæ’ä»¶ {ex_folder} ..."))
        plugin_kwargs['markdown_expected_output_dir'] = ex_folder

        translated_f_name = 'translated_markdown.md'
        generated_fp = plugin_kwargs['markdown_expected_output_path'] = os.path.join(ex_folder, translated_f_name)
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
        yield from Markdownè‹±è¯‘ä¸­(ex_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, user_request)
        if os.path.exists(generated_fp):
            # ä¿®æ­£ä¸€äº›å…¬å¼é—®é¢˜
            with open(generated_fp, 'r', encoding='utf8') as f: content = f.read()
            content = content.replace('```markdown', '\n').replace('```', '\n')
            # Markdownä¸­ä½¿ç”¨ä¸æ ‡å‡†çš„è¡¨æ ¼ï¼Œéœ€è¦åœ¨è¡¨æ ¼å‰åŠ ä¸Šä¸€ä¸ªemojiï¼Œä»¥ä¾¿å…¬å¼æ¸²æŸ“
            content = re.sub(r'^<table>', r'ğŸ˜ƒ<table>', content, flags=re.MULTILINE)
            with open(generated_fp, 'w', encoding='utf8') as f: f.write(content)
            # ç”Ÿæˆåœ¨çº¿é¢„è§ˆhtml
            file_name = 'åœ¨çº¿é¢„è§ˆç¿»è¯‘' + gen_time_str() + '.html'
            preview_fp = os.path.join(ex_folder, file_name)
            from shared_utils.advanced_markdown_format import markdown_convertion_for_file
            with open(generated_fp, "r", encoding="utf-8") as f:
                md = f.read()
            html = markdown_convertion_for_file(md)
            with open(preview_fp, "w", encoding="utf-8") as f: f.write(html)
            promote_file_to_downloadzone(preview_fp, chatbot=chatbot)
            # ç”ŸæˆåŒ…å«å›¾ç‰‡çš„å‹ç¼©åŒ…
            dest_folder = get_log_folder(chatbot.get_user())
            zip_name = 'ç¿»è¯‘åçš„å¸¦å›¾æ–‡æ¡£.zip'
            zip_folder(source_folder=ex_folder, dest_folder=dest_folder, zip_name=zip_name)
            zip_fp = os.path.join(dest_folder, zip_name)
            promote_file_to_downloadzone(zip_fp, chatbot=chatbot)
            yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
    md_zip_path = yield from pdf2markdown(fp)
    yield from deliver_to_markdown_plugin(md_zip_path, user_request)

def è§£æPDF_DOC2X(file_manifest, *args):
    for index, fp in enumerate(file_manifest):
        yield from è§£æPDF_DOC2X_å•æ–‡ä»¶(fp, *args)
    return

def è§£æPDF_åŸºäºGROBID(file_manifest, project_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt, grobid_url):
    import copy, json
    TOKEN_LIMIT_PER_FRAGMENT = 1024
    generated_conclusion_files = []
    generated_html_files = []
    DST_LANG = "ä¸­æ–‡"
    from crazy_functions.pdf_fns.report_gen_html import construct_html
    for index, fp in enumerate(file_manifest):
        chatbot.append(["å½“å‰è¿›åº¦ï¼š", f"æ­£åœ¨è¿æ¥GROBIDæœåŠ¡ï¼Œè¯·ç¨å€™: {grobid_url}\nå¦‚æœç­‰å¾…æ—¶é—´è¿‡é•¿ï¼Œè¯·ä¿®æ”¹configä¸­çš„GROBID_URLï¼Œå¯ä¿®æ”¹æˆæœ¬åœ°GROBIDæœåŠ¡ã€‚"]); yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢
        article_dict = parse_pdf(fp, grobid_url)
        grobid_json_res = os.path.join(get_log_folder(), gen_time_str() + "grobid.json")
        with open(grobid_json_res, 'w+', encoding='utf8') as f:
            f.write(json.dumps(article_dict, indent=4, ensure_ascii=False))
        promote_file_to_downloadzone(grobid_json_res, chatbot=chatbot)

        if article_dict is None: raise RuntimeError("è§£æPDFå¤±è´¥ï¼Œè¯·æ£€æŸ¥PDFæ˜¯å¦æŸåã€‚")
        yield from translate_pdf(article_dict, llm_kwargs, chatbot, fp, generated_conclusion_files, TOKEN_LIMIT_PER_FRAGMENT, DST_LANG)
    chatbot.append(("ç»™å‡ºè¾“å‡ºæ–‡ä»¶æ¸…å•", str(generated_conclusion_files + generated_html_files)))
    yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢


def è§£æPDF(file_manifest, project_folder, llm_kwargs, plugin_kwargs, chatbot, history, system_prompt):
    """
    æ­¤å‡½æ•°å·²ç»å¼ƒç”¨
    """
    import copy
    TOKEN_LIMIT_PER_FRAGMENT = 1024
    generated_conclusion_files = []
    generated_html_files = []
    from crazy_functions.pdf_fns.report_gen_html import construct_html
    for index, fp in enumerate(file_manifest):
        # è¯»å–PDFæ–‡ä»¶
        file_content, page_one = read_and_clean_pdf_text(fp)
        file_content = file_content.encode('utf-8', 'ignore').decode()   # avoid reading non-utf8 chars
        page_one = str(page_one).encode('utf-8', 'ignore').decode()      # avoid reading non-utf8 chars

        # é€’å½’åœ°åˆ‡å‰²PDFæ–‡ä»¶
        from crazy_functions.pdf_fns.breakdown_txt import breakdown_text_to_satisfy_token_limit
        paper_fragments = breakdown_text_to_satisfy_token_limit(txt=file_content, limit=TOKEN_LIMIT_PER_FRAGMENT, llm_model=llm_kwargs['llm_model'])
        page_one_fragments = breakdown_text_to_satisfy_token_limit(txt=page_one, limit=TOKEN_LIMIT_PER_FRAGMENT//4, llm_model=llm_kwargs['llm_model'])

        # ä¸ºäº†æ›´å¥½çš„æ•ˆæœï¼Œæˆ‘ä»¬å‰¥ç¦»Introductionä¹‹åçš„éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        paper_meta = page_one_fragments[0].split('introduction')[0].split('Introduction')[0].split('INTRODUCTION')[0]

        # å•çº¿ï¼Œè·å–æ–‡ç« metaä¿¡æ¯
        paper_meta_info = yield from request_gpt_model_in_new_thread_with_ui_alive(
            inputs=f"ä»¥ä¸‹æ˜¯ä¸€ç¯‡å­¦æœ¯è®ºæ–‡çš„åŸºç¡€ä¿¡æ¯ï¼Œè¯·ä»ä¸­æå–å‡ºâ€œæ ‡é¢˜â€ã€â€œæ”¶å½•ä¼šè®®æˆ–æœŸåˆŠâ€ã€â€œä½œè€…â€ã€â€œæ‘˜è¦â€ã€â€œç¼–å·â€ã€â€œä½œè€…é‚®ç®±â€è¿™å…­ä¸ªéƒ¨åˆ†ã€‚è¯·ç”¨markdownæ ¼å¼è¾“å‡ºï¼Œæœ€åç”¨ä¸­æ–‡ç¿»è¯‘æ‘˜è¦éƒ¨åˆ†ã€‚è¯·æå–ï¼š{paper_meta}",
            inputs_show_user=f"è¯·ä»{fp}ä¸­æå–å‡ºâ€œæ ‡é¢˜â€ã€â€œæ”¶å½•ä¼šè®®æˆ–æœŸåˆŠâ€ç­‰åŸºæœ¬ä¿¡æ¯ã€‚",
            llm_kwargs=llm_kwargs,
            chatbot=chatbot, history=[],
            sys_prompt="Your job is to collect information from materialsã€‚",
        )

        # å¤šçº¿ï¼Œç¿»è¯‘
        gpt_response_collection = yield from request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency(
            inputs_array=[
                f"ä½ éœ€è¦ç¿»è¯‘ä»¥ä¸‹å†…å®¹ï¼š\n{frag}" for frag in paper_fragments],
            inputs_show_user_array=[f"\n---\n åŸæ–‡ï¼š \n\n {frag.replace('#', '')}  \n---\n ç¿»è¯‘ï¼š\n " for frag in paper_fragments],
            llm_kwargs=llm_kwargs,
            chatbot=chatbot,
            history_array=[[paper_meta] for _ in paper_fragments],
            sys_prompt_array=[
                "è¯·ä½ ä½œä¸ºä¸€ä¸ªå­¦æœ¯ç¿»è¯‘ï¼Œè´Ÿè´£æŠŠå­¦æœ¯è®ºæ–‡å‡†ç¡®ç¿»è¯‘æˆä¸­æ–‡ã€‚æ³¨æ„æ–‡ç« ä¸­çš„æ¯ä¸€å¥è¯éƒ½è¦ç¿»è¯‘ã€‚" for _ in paper_fragments],
            # max_workers=5  # OpenAIæ‰€å…è®¸çš„æœ€å¤§å¹¶è¡Œè¿‡è½½
        )
        gpt_response_collection_md = copy.deepcopy(gpt_response_collection)
        # æ•´ç†æŠ¥å‘Šçš„æ ¼å¼
        for i,k in enumerate(gpt_response_collection_md):
            if i%2==0:
                gpt_response_collection_md[i] = f"\n\n---\n\n ## åŸæ–‡[{i//2}/{len(gpt_response_collection_md)//2}]ï¼š \n\n {paper_fragments[i//2].replace('#', '')}  \n\n---\n\n ## ç¿»è¯‘[{i//2}/{len(gpt_response_collection_md)//2}]ï¼š\n "
            else:
                gpt_response_collection_md[i] = gpt_response_collection_md[i]
        final = ["ä¸€ã€è®ºæ–‡æ¦‚å†µ\n\n---\n\n", paper_meta_info.replace('# ', '### ') + '\n\n---\n\n', "äºŒã€è®ºæ–‡ç¿»è¯‘", ""]
        final.extend(gpt_response_collection_md)
        create_report_file_name = f"{os.path.basename(fp)}.trans.md"
        res = write_history_to_file(final, create_report_file_name)
        promote_file_to_downloadzone(res, chatbot=chatbot)

        # æ›´æ–°UI
        generated_conclusion_files.append(f'{get_log_folder()}/{create_report_file_name}')
        chatbot.append((f"{fp}å®Œæˆäº†å—ï¼Ÿ", res))
        yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢

        # write html
        try:
            ch = construct_html()
            orig = ""
            trans = ""
            gpt_response_collection_html = copy.deepcopy(gpt_response_collection)
            for i,k in enumerate(gpt_response_collection_html):
                if i%2==0:
                    gpt_response_collection_html[i] = paper_fragments[i//2].replace('#', '')
                else:
                    gpt_response_collection_html[i] = gpt_response_collection_html[i]
            final = ["è®ºæ–‡æ¦‚å†µ", paper_meta_info.replace('# ', '### '),  "äºŒã€è®ºæ–‡ç¿»è¯‘",  ""]
            final.extend(gpt_response_collection_html)
            for i, k in enumerate(final):
                if i%2==0:
                    orig = k
                if i%2==1:
                    trans = k
                    ch.add_row(a=orig, b=trans)
            create_report_file_name = f"{os.path.basename(fp)}.trans.html"
            generated_html_files.append(ch.save_file(create_report_file_name))
        except:
            from toolbox import trimmed_format_exc
            print('writing html result failed:', trimmed_format_exc())

    # å‡†å¤‡æ–‡ä»¶çš„ä¸‹è½½
    for pdf_path in generated_conclusion_files:
        # é‡å‘½åæ–‡ä»¶
        rename_file = f'ç¿»è¯‘-{os.path.basename(pdf_path)}'
        promote_file_to_downloadzone(pdf_path, rename_file=rename_file, chatbot=chatbot)
    for html_path in generated_html_files:
        # é‡å‘½åæ–‡ä»¶
        rename_file = f'ç¿»è¯‘-{os.path.basename(html_path)}'
        promote_file_to_downloadzone(html_path, rename_file=rename_file, chatbot=chatbot)
    chatbot.append(("ç»™å‡ºè¾“å‡ºæ–‡ä»¶æ¸…å•", str(generated_conclusion_files + generated_html_files)))
    yield from update_ui(chatbot=chatbot, history=history) # åˆ·æ–°ç•Œé¢


