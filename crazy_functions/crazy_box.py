#! .\venv\
# encoding: utf-8
# @Time   : 2023/6/14
# @Author : Spike
# @Descr   :
import re, os
import json, time
from bs4 import BeautifulSoup
import requests
from comm_tools import func_box, ocr_tools, toolbox, prompt_generator
from openpyxl import load_workbook
from crazy_functions import crazy_utils
import urllib.parse
from request_llm import bridge_all


class Utils:

    def __init__(self):
        self.find_keys_type = 'type'
        self.find_picture_source = ['caption', 'imgID', 'sourceKey']
        self.find_document_source = ['wpsDocumentLink', 'wpsDocumentName', 'wpsDocumentType']
        self.find_document_tags = ['WPSDocument']
        self.find_picture_tags = ['picture', 'processon']
        self.picture_format = ['.jpeg', '.jpg', '.png', '.gif', '.bmp', '.tiff']

    def find_all_text_keys(self, dictionary, parent_type=None, text_values=None, filter_type=''):
        """
        Args:
            dictionary: å­—å…¸æˆ–åˆ—è¡¨
            parent_type: åŒ¹é…çš„typeï¼Œä½œä¸ºæ–°åˆ—è¡¨çš„keyï¼Œç”¨äºåˆ†ç±»
            text_values: å­˜å‚¨åˆ—è¡¨
            filter_type: å½“å‰å±‚çº§find_keys_type==filter_typeæ—¶ï¼Œä¸ç»§ç»­å¾€ä¸‹åµŒå¥—
        Returns:
            text_valueså’Œæ’åºåçš„context_
        """
        # åˆå§‹åŒ– text_values ä¸ºç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ‰¾åˆ°çš„æ‰€æœ‰textå€¼
        if text_values is None:
            text_values = []
        # å¦‚æœè¾“å…¥çš„dictionaryä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè¿”å›å·²æ”¶é›†åˆ°çš„textå€¼
        if not isinstance(dictionary, dict):
            return text_values
        # è·å–å½“å‰å±‚çº§çš„ type å€¼
        current_type = dictionary.get(self.find_keys_type, parent_type)
        # å¦‚æœå­—å…¸ä¸­åŒ…å« 'text' æˆ– 'caption' é”®ï¼Œå°†å¯¹åº”çš„å€¼æ·»åŠ åˆ° text_values åˆ—è¡¨ä¸­
        if 'text' in dictionary:
            content_value = dictionary.get('text', None)
            text_values.append({current_type: content_value})
        if 'caption' in dictionary:
            temp = {}
            for key in self.find_picture_source:
                temp[key] = dictionary.get(key, None)
            text_values.append({current_type: temp})
        if 'wpsDocumentId' in dictionary:
            temp = {}
            for key in self.find_document_source:
                temp[key] = dictionary.get(key, None)
            text_values.append({current_type: temp})
        # å¦‚æœå½“å‰ç±»å‹ä¸ç­‰äº filter_typeï¼Œåˆ™ç»§ç»­éå†å­çº§å±æ€§
        if current_type != filter_type:
            for key, value in dictionary.items():
                if isinstance(value, dict):
                    self.find_all_text_keys(value, current_type, text_values, filter_type)
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict):
                            self.find_all_text_keys(item, current_type, text_values, filter_type)
        return text_values

    def statistical_results(self, text_values, img_proce=False):
        context_ = []
        pic_dict = {}
        file_dict = {}
        for i in text_values:
            for key, value in i.items():
                if key in self.find_picture_tags:
                    if img_proce:
                        mark = f'{key}OCRç»“æœ: """{value["sourceKey"]}"""\n'
                        if value["caption"]: mark += f'{key}æè¿°: {value["caption"]}\n'
                        context_.append(mark)
                        pic_dict[value['sourceKey']] = value['imgID']
                    else:
                        if value["caption"]: context_.append(f'{key}æè¿°: {value["caption"]}\n')
                        pic_dict[value['sourceKey']] = value['imgID']
                elif key in self.find_document_tags:
                    mark = f'{value["wpsDocumentName"]}: {value["wpsDocumentLink"]}'
                    context_.append(mark)
                else:
                    context_.append(value)
        context_ = '\n'.join(context_)
        return text_values, context_, pic_dict, file_dict

    def write_markdown(self, data, hosts, file_name):
        user_path = os.path.join(func_box.users_path, hosts, 'markdown')
        os.makedirs(user_path, exist_ok=True)
        md_file = os.path.join(user_path, f"{file_name}.md")
        with open(file=md_file, mode='w') as f:
            f.write(data)
        return md_file

    def markdown_to_flow_chart(self, data, hosts, file_name):
        user_path = os.path.join(func_box.users_path, hosts, 'mark_map')
        md_file = self.write_markdown(data, hosts, file_name)
        html_file = os.path.join(user_path, f"{file_name}.html")
        func_box.Shell(f'npx markmap-cli --no-open "{md_file}" -o "{html_file}"').read()
        return md_file, html_file

    def split_startswith_txt(self, link_limit, start='http'):
        link = str(link_limit).split()
        links = []
        for i in link:
            if i.startswith(start):
                links.append(i)
        return links

    def global_search_for_files(self, file_path, matching: list):
        file_list = []
        for root, dirs, files in os.walk(file_path):
            for file in files:
                for math in matching:
                    if str(math).lower() in str(file).lower():
                        file_list.append(os.path.join(root, file))
        return file_list



class ExcelHandle:

    def __init__(self, ipaddr, is_client=True):
        if type(is_client) is bool and is_client:
            self.template_excel = os.path.join(func_box.base_path, 'docs/template/ã€Tempã€‘æµ‹è¯•è¦ç‚¹.xlsx')
        elif not is_client:
            self.template_excel = os.path.join(func_box.base_path, 'docs/template/æ¥å£æµ‹è¯•ç”¨ä¾‹æ¨¡æ¿.xlsx')
        elif type(type) is str:
            if os.path.exists(is_client):
                self.template_excel = is_client
            else:
                self.template_excel = os.path.join(func_box.base_path, 'docs/template/ã€Tempã€‘æµ‹è¯•è¦ç‚¹.xlsx')
        self.user_path = os.path.join(func_box.base_path, 'private_upload', ipaddr, 'test_case')
        os.makedirs(f'{self.user_path}', exist_ok=True)

    def lpvoid_lpbuffe(self, data_list: list, filename='', decs=''):
        # åŠ è½½ç°æœ‰çš„ Excel æ–‡ä»¶
        workbook = load_workbook(self.template_excel)
        # é€‰æ‹©è¦æ“ä½œçš„å·¥ä½œè¡¨
        worksheet = workbook['æµ‹è¯•è¦ç‚¹']
        try:
            decs_sheet = workbook['è¯´æ˜']
            decs_sheet['C2'] = decs
        except:
            print('æ–‡æ¡£æ²¡æœ‰è¯´æ˜çš„sheet')
        # å®šä¹‰èµ·å§‹è¡Œå·
        start_row = 4
        # éå†æ•°æ®åˆ—è¡¨
        for row_data in data_list:
            # å†™å…¥æ¯ä¸€è¡Œçš„æ•°æ®åˆ°æŒ‡å®šçš„å•å…ƒæ ¼èŒƒå›´
            for col_num, value in enumerate(row_data, start=1):
                cell = worksheet.cell(row=start_row, column=col_num)
                cell.value = value
            # å¢åŠ èµ·å§‹è¡Œå·
            start_row += 1
        # ä¿å­˜ Excel æ–‡ä»¶
        time_stamp = time.strftime("%Y-%m-%d-%H", time.localtime())
        if filename == '': filename = time.strftime("%Y-%m-%d-%H", time.localtime()) + '_temp'
        else: f"{time_stamp}_{filename}"
        test_case_path = f'{os.path.join(self.user_path, filename)}.xlsx'
        workbook.save(test_case_path)
        return test_case_path


class Kdocs:

    def __init__(self, url):
        WPS_COOKIES, = toolbox.get_conf('WPS_COOKIES',)
        self.url = url
        self.cookies = WPS_COOKIES
        self.headers = {
            'accept-language': 'en-US,en;q=0.9,ja;q=0.8',
            'content-type': 'text/plain;charset=UTF-8',
            'x-csrf-rand': '',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'}
        self.ex_headers = {
            'Host': 'www.kdocs.cn',
            'accept': 'application/json, text/plain, */*',
            'content-type': 'application/json',
            'sec-ch-ua-platform': '"macOS"',
            'origin': 'https://www.kdocs.cn',
        }
        self.dzip_header = {
            'Host': 'kdzip-download.kdocs.cn',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.82',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',

            }
        self.parm_otl_data = {"connid": "",
                              "args": {"password": "", "readonly": False, "modifyPassword": "", "sync": True,
                                       "startVersion": 0, "endVersion": 0},
                              "ex_args": {"queryInitArgs": {"enableCopyComments": False, "checkAuditRule": False}},
                              "group": "", "front_ver": ""}
        self.parm_shapes_data = {"objects": [], "expire": 86400000, "support_webp": True, "with_thumbnail": True,
                                 "support_lossless": True}
        self.parm_export_preload = {"ver": "56"}
        self.parm_bulk_download = {'file_ids': []}
        self.params_task = {'task_id': ''}
        self.params_continue = {"task_id": "", "download_as": [
                            {"suffix": ".otl", "as": ".pdf"},
                            {"suffix": ".ksheet", "as": ".xlsx"},
                            {"suffix": ".pof", "as": ".png"},
                            {"suffix": ".pom", "as": ".png"}]}
        self.tol_url = 'https://www.kdocs.cn/api/v3/office/file/%v/open/otl'
        self.shapes_url = 'https://www.kdocs.cn/api/v3/office/file/%v/attachment/shapes'
        self.kdocs_download_url = 'https://drive.kdocs.cn/api/v5/groups/%g/files/%f/download?isblocks=false&support_checksums=md5,sha1,sha224,sha256,sha384,sha512'
        self.drive_download_url = 'https://drive.wps.cn/api/v3/groups/%g/files/%f/download?isblocks=false'
        self.group_url = 'https://drive.wps.cn/api/v5/links/%v?review=true'
        self.export_url = 'https://www.kdocs.cn/api/v3/office/file/%f/export/%t/result'
        self.preload_url = 'https://www.kdocs.cn/api/v3/office/file/%f/export/%t/preload'
        self.bulk_download_url = 'https://www.kdocs.cn/kfc/batch/v2/files/download'
        self.bulk_continue_url = 'https://www.kdocs.cn/kfc/batch/v2/files/download/continue'
        self.task_result_url = 'https://www.kdocs.cn/kfc/batch/v2/files/download/progress'
        self.url_share_tag = ''
        self.url_dirs_tag = ''
        self.split_link_tags()
        if self.url_share_tag:
            self.file_info_parm = self.get_file_info_parm()
        self.docs_old_type = ['.docs', '.doc', '.pptx', '.ppt', '.xls', '.xlsx', '.pdf', '.csv', '.txt', '.pom', '.pof']
        self.to_img_type = {'.pom': '.png', '.pof': '.png'}
        self.media_type = ['.mp4', '.m4a', '.wav', '.mpga', '.mpeg', '.mp3', '.avi', '.mkv', '.flac', '.aac']
        self.smart_type = {'.otl': '.pdf', '.ksheet': '.xlsx'}

    def get_file_info_html(self):
        """
        è·å–ä¼ é€’è¿‡æ¥çš„æ–‡æ¡£HTMLä¿¡æ¯
        Returns:
            HTMLä¿¡æ¯
        """
        response = requests.get(self.url, cookies=self.cookies, headers=self.headers)
        return response.text

    def get_file_info_parm(self):
        response = requests.get(self.group_url.replace("%v", self.url_share_tag),
                                cookies=self.cookies,
                                headers=self.headers, verify=False).json()
        try:
            file_info = response['fileinfo']
        except KeyError:
            file_info = {}
        return file_info

    def submit_batch_download_tasks(self):
        self.parm_bulk_download.update({'file_ids': [self.url_dirs_tag]})
        dw_response = requests.post(self.bulk_download_url, cookies=self.cookies, headers=self.ex_headers,
                                 json=self.parm_bulk_download, verify=False).json()
        if dw_response.get('data', False):
            task_id = dw_response['data']['task_id']
            task_info = dw_response['data']['online_file'], dw_response['data']['online_fnum']
        else:
            print(dw_response['result'])
            task_id = None
            task_info = None
        if task_id:
            self.params_continue.update({'task_id': task_id})
            requests.post(self.bulk_continue_url, cookies=self.cookies, headers=self.ex_headers,
                          json=self.params_continue, verify=False).json()
        return task_id, task_info

    def polling_batch_download_tasks(self, task_id):
        self.params_task.update({'task_id': task_id})
        link = ''
        faillist = ''
        if task_id:
            for i in range(600):
                response = requests.get(url=self.task_result_url,
                                        params=self.params_task,
                                        cookies=self.cookies,
                                        headers=self.ex_headers, verify=False).json()
                if response['data'].get('url', False):
                    link = response['data'].get('url', '')
                    faillist = str(response['data'].get('faillist', ''))
                    break
                time.sleep(3)
        return link, faillist

    def wps_file_download(self, url):
        response = requests.get(url=url, cookies=self.cookies, headers=self.dzip_header, verify=False)
        return response

    def document_aggregation_download(self, file_type=''):
        link_name = self.file_info_parm['fname']
        for t in self.to_img_type:
            if t in link_name:
                link_name = link_name+self.to_img_type[t]
        link = ''
        for t in self.docs_old_type:
            if t in link_name and file_type in link_name:
                link = self.get_docs_old_link()
        for t in self.media_type:
            if t in link_name and file_type in link_name:
                link = self.get_media_link()
        for t in self.smart_type:
            if t in link_name and file_type in link_name:
                link = self.get_kdocs_intelligence_link(type=self.smart_type[t])
                link_name = link_name+f"{self.smart_type[t]}"
        return link, link_name

    def get_media_link(self):
        response = requests.get(self.drive_download_url.replace("%g", str(self.file_info_parm['groupid'])
                                                                ).replace('%f', str(self.file_info_parm['id'])),
                                cookies=self.cookies,
                                headers=self.headers, verify=False)
        link = response.json()['fileinfo']['url']
        return self.url_decode(link)

    def get_docs_old_link(self):
        response = requests.get(self.kdocs_download_url.replace("%g", str(self.file_info_parm['groupid'])
                                                                ).replace('%f', str(self.file_info_parm['id'])),
                                cookies=self.cookies,
                                headers=self.headers, verify=False)
        try:
            link = response.json()['fileinfo']['url']
        except:
            link = response.json()['url']
        return self.url_decode(link)

    def get_kdocs_intelligence_link(self, type='xlsx'):
        response_task = requests.post(
            self.preload_url.replace('%f', str(self.file_info_parm['id'])).replace('%t', type),
            cookies=self.cookies,
            headers=self.ex_headers,
            json=self.parm_export_preload, verify=False
        )
        self.parm_export_preload.update(response_task.json())
        for i in range(20):
            response_link = requests.post(
                self.export_url.replace('%f', str(self.file_info_parm['id'])).replace('%t', type),
                cookies=self.cookies,
                headers=self.ex_headers,
                json=self.parm_export_preload, verify=False
            )
            if response_link.json()['status'] == 'finished':
                return response_link.json()['data']['url']
        return None

    def split_link_tags(self):
        url_parts = re.split('[/\?&#]+', self.url)
        try:
            try:
                l_index = url_parts.index('l')
                otl_url_str = url_parts[l_index + 1]
                self.url_share_tag = otl_url_str
            except ValueError:
                l_index = url_parts.index('ent')
                otl_url_str = url_parts[-1]
                self.url_dirs_tag = otl_url_str
        except ValueError:
            print('æ—¢ä¸æ˜¯åœ¨çº¿æ–‡æ¡£ï¼Œä¹Ÿä¸æ˜¯æ–‡æ¡£ç›®å½•')
            return ''

    def get_file_content(self):
        """
        çˆ¬è™«è§£ææ–‡æ¡£å†…å®¹
        Returns:
            æ–‡æ¡£å†…å®¹
        """
        otl_url_str = self.url_share_tag
        if otl_url_str is None: return
        html_content = self.get_file_info_html()
        self.bs4_file_info(html_content)  # è°ƒç”¨ bs4_file_info() æ–¹æ³•è§£æ html_contentï¼Œè·å–æ–‡ä»¶ä¿¡æ¯# æ›´æ–°ç±»çš„parm_data å’Œ headers
        json_data = json.dumps(self.parm_otl_data)
        response = requests.post(
            str(self.tol_url).replace('%v', otl_url_str),
            cookies=self.cookies,
            headers=self.headers,
            data=json_data,)
        return response.json(), response.text

    def get_file_pic_url(self, pic_dict: dict):
        otl_url_str = self.url_share_tag
        if otl_url_str is None: return
        for pic in pic_dict:
            pic_parm = {'attachment_id': pic, "imgId": pic_dict[pic], "max_edge": 1180, "source": ""}
            self.parm_shapes_data['objects'].append(pic_parm)
        json_data = json.dumps(self.parm_shapes_data)
        response = requests.post(
            str(self.shapes_url).replace('%v', otl_url_str),
            cookies=self.cookies,
            headers=self.headers,
            data=json_data,)
        url_data = response.json()['data']
        for pic in url_data:
            try:
                pic_dict[pic] = self.url_decode(url_data[pic]['url'])
            except Exception as f:
                pass
        return pic_dict

    @staticmethod
    def url_decode(url):
        decoded_url = urllib.parse.unquote(url)
        return decoded_url


    def bs4_file_info(self, html_str):
        """
        bs4çˆ¬è™«æ–‡æ¡£ä¿¡æ¯ï¼Œæ²¡æœ‰è¿™ä¸ªå¯ä¸è¡ŒğŸ¤¨
        Args:
            html_str: HTMLä¿¡æ¯
        Returns:
            {'connid': æ–‡æ¡£id, 'group': æ–‡æ¡£çš„ç¾¤ç»„, 'front_ver': æ–‡æ¡£ç‰ˆæœ¬}
        """
        html = BeautifulSoup(html_str, "html.parser")
        # Find all script tags in the HTML
        script_tags = html.find_all("script")
        json_string = None
        # Iterate through script tags to find the one containing required data
        for tag in script_tags:
            if tag.string and "window.__WPSENV__" in tag.string:
                json_string = re.search(r"window\.__WPSENV__=(.*);", tag.string).group(1)
                break
        if json_string:
            # Load the JSON data from the found string
            json_data = json.loads(json_string)
            file_connid = json_data['conn_id']
            file_group = json_data['user_group']
            file_front_ver = json_data['file_version']
            file_id = json_data['root_file_id']
            group_id = json_data['file_info']['file']['group_id']
            self.headers['x-csrf-rand'] = json_data['csrf_token']
            self.parm_otl_data.update({'connid': file_connid, 'group': file_group, 'front_ver': file_front_ver,
                                       'file_id': file_id, 'group_id':group_id})
            return True
        else:
            return None


def get_docs_content(url, image_processing=False):
    kdocs = Kdocs(url)
    utils = Utils()
    json_data, json_dict = kdocs.get_file_content()
    text_values = utils.find_all_text_keys(json_data, filter_type='')
    _all, content, pic_dict, file_dict = utils.statistical_results(text_values, img_proce=image_processing)
    pic_dict_convert = kdocs.get_file_pic_url(pic_dict)
    empty_picture_count = sum(1 for item in _all if 'picture' in item and not item['picture']['caption'])
    return _all, content, empty_picture_count, pic_dict_convert


def batch_recognition_images_to_md(img_list, ipaddr):
    temp_list = []
    for img in img_list:
        if os.path.exists(img):
            img_content, img_result = ocr_tools.Paddle_ocr_select(ipaddr=ipaddr, trust_value=True
                                                                  ).img_def_content(img_path=img)
            temp_file = os.path.join(func_box.users_path, ipaddr, 'ocr_to_md', img_content.splitlines()[0][:20]+'.md')
            with open(temp_file, mode='w') as f:
                f.write(f"{func_box.html_view_blank(temp_file)}\n\n"+img_content)
            temp_list.append(temp_list)
        else:
            print(img, 'æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨')
    return temp_list


def get_kdocs_dir(limit, project_folder, type, ipaddr):
    """
    Args:
        limit: æ–‡æ¡£ç›®å½•è·¯å¾„
        project_folder: å†™å…¥çš„æ–‡ä»¶
        type: æ–‡ä»¶ç±»å‹, ä¸è¿‡è¿™é‡Œæ²¡ç”¨åˆ°
        ipaddr:  æ–‡ä»¶æ‰€å±æ ‡è¯†
    Returns: [æ–‡ä»¶åˆ—è¡¨], ç›®å½•å†…æ–‡ä»¶ä¿¡æ¯, å¤±è´¥ä¿¡æ¯
    """
    kdocs = Kdocs(limit)
    task_id, task_info = kdocs.submit_batch_download_tasks()
    link, task_faillist = kdocs.polling_batch_download_tasks(task_id)
    resp = kdocs.wps_file_download(link)
    content = resp.content
    temp_file = os.path.join(project_folder, kdocs.url_dirs_tag + '.zip')
    with open(temp_file, 'wb') as f: f.write(content)
    decompress_directory = os.path.join(project_folder, 'extract', kdocs.url_dirs_tag)
    toolbox.extract_archive(temp_file, decompress_directory)
    file_list = []
    img_list = []
    for f_t in kdocs.docs_old_type:
        _, file_, _ = crazy_utils.get_files_from_everything(decompress_directory, type=f_t, ipaddr=ipaddr)
        file_list += file_
    for i_t in Utils().picture_format:
        _, file_, _ = crazy_utils.get_files_from_everything(decompress_directory, type=i_t, ipaddr=ipaddr)
        file_list += file_
    file_list += batch_recognition_images_to_md(img_list, ipaddr)
    return file_list, task_info, task_faillist


def get_kdocs_files(limit, project_folder, type, ipaddr):
    if type == 'otl':
        _, content, _, pic_dict = get_docs_content(limit)
        name = 'temp.md'
        tag = content.splitlines()[0][:20]
        for i in pic_dict:  # å¢åŠ OCRé€‰é¡¹
            img_content, img_result = ocr_tools.Paddle_ocr_select(ipaddr=ipaddr, trust_value=True
                                                                  ).img_def_content(img_path=pic_dict[i])
            content = str(content).replace(f"{i}", f"{func_box.html_local_img(img_result)}\n```{img_content}```")
            name = tag + '.md'
            content = content.encode('utf-8')
    elif type or type == '':
        kdocs = Kdocs(limit)
        link, name = kdocs.document_aggregation_download(file_type=type)
        tag = kdocs.url_share_tag
        if link:
            resp = requests.get(url=link, verify=False)
            content = resp.content
        else:
            return []
    else:
        return []
    if content:
        tag_path = os.path.join(project_folder, tag)
        temp_file = os.path.join(os.path.join(project_folder, tag, name))
        os.makedirs(tag_path, exist_ok=True)
        with open(temp_file, 'wb') as f: f.write(content)
        return [temp_file]


def get_kdocs_from_everything(txt, type='', ipaddr='temp'):
    """
    Args:
        txt: kudos æ–‡ä»¶åˆ†äº«ç 
        type: type=='' æ—¶ï¼Œå°†æ”¯æŒæ‰€æœ‰æ–‡ä»¶ç±»å‹
        ipaddr:
    Returns:
    """
    link_limit = Utils().split_startswith_txt(link_limit=txt)
    file_manifest = []
    success = ''
    project_folder = os.path.join(func_box.users_path, ipaddr, 'kdocs')
    os.makedirs(project_folder, exist_ok=True)
    if link_limit:
        for limit in link_limit:
            if '/ent/' in limit:
                file_list, info, fail = get_kdocs_dir(limit, project_folder, type, ipaddr)
                file_manifest += file_list
                success += f"{limit}æ–‡ä»¶ä¿¡æ¯å¦‚ä¸‹ï¼š{info}\n\n ä¸‹è½½ä»»åŠ¡çŠ¶å†µï¼š{fail}\n\n"
            else:
                file_manifest += get_kdocs_files(limit, project_folder, type, ipaddr)
    return success, file_manifest, project_folder

def json_args_return(kwargs, keys: list) -> list:
    temp = [False for i in range(len(keys))]
    for i in range(len(keys)):
        try:
            temp[i] = json.loads(kwargs['advanced_arg'])[keys[i]]
        except Exception as f:
            try:
                temp[i] = kwargs['parameters_def'][keys[i]]
            except Exception as f:
                temp[i] = False
    return temp


def ocr_batch_processing(file_manifest, chatbot, history, llm_kwargs):
    ocr_process = f'> çº¢æ¡†ä¸ºé‡‡ç”¨çš„æ–‡æ¡ˆ,å¯ä¿¡åº¦ä½äº {func_box.html_tag_color(llm_kwargs["ocr"])} å°†ä¸é‡‡ç”¨, å¯åœ¨Setting ä¸­è¿›è¡Œé…ç½®\n\n'
    i_say = f'{file_manifest}\n\nORCå¼€å§‹å·¥ä½œ'
    chatbot.append([i_say, ocr_process])
    for pic_path in file_manifest:
        yield from toolbox.update_ui(chatbot, history, 'æ­£åœ¨è°ƒç”¨OCRç»„ä»¶ï¼Œå›¾ç‰‡å¤šå¯èƒ½ä¼šæ¯”è¾ƒæ…¢')
        img_content, img_result = ocr_tools.Paddle_ocr_select(ipaddr=llm_kwargs['ipaddr'],
                                                              trust_value=llm_kwargs['ocr']
                                                              ).img_def_content(img_path=pic_path)
        ocr_process += f'{pic_path} è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«æ•ˆæœå¦‚ä¸‹ {func_box.html_local_img(img_result)} \n\n' \
                       f'```\n{img_content}\n```'
        chatbot[-1] = [i_say, ocr_process]
        yield from toolbox.update_ui(chatbot, history)
    ocr_process += f'\n\n---\n\nè§£ææˆåŠŸï¼Œç°åœ¨æˆ‘å·²ç†è§£ä¸Šè¿°å†…å®¹ï¼Œæœ‰ä»€ä¹ˆä¸æ‡‚å¾—åœ°æ–¹ä½ å¯ä»¥é—®æˆ‘ï½'
    chatbot[-1] = [i_say, ocr_process]
    history.extend([i_say, ocr_process])
    yield from toolbox.update_ui(chatbot, history)


import re
def replace_special_chars(file_name):
    # é™¤äº†ä¸­æ–‡å¤–ï¼Œè¯¥æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»»ä½•ä¸€ä¸ªä¸æ˜¯æ•°å­—ã€å­—æ¯ã€ä¸‹åˆ’çº¿ã€.ã€ç©ºæ ¼çš„å­—ç¬¦
    return re.sub(r'[^\u4e00-\u9fa5\d\w\s\.\_]', '_', file_name)

def long_name_processing(file_name):
    if type(file_name) is list: file_name = file_name[0]
    if len(file_name) > 50:
        if file_name.find('"""') != -1:
            temp = file_name.split('"""')[1].splitlines()
            for i in temp:
                if i:
                    file_name = replace_special_chars(i)
                    break
        else:
            file_name = file_name[:20]
    return file_name


def write_test_cases(gpt_response_collection, inputs_show_user_array, llm_kwargs, chatbot, history, is_client=True):
    gpt_response = gpt_response_collection[1::2]
    chat_file_list = ''
    test_case = []
    file_name = long_name_processing(inputs_show_user_array)
    for k in range(len(gpt_response)):
        gpt_response_split = gpt_response[k].splitlines()[2:]  # è¿‡æ»¤æ‰è¡¨å¤´
        for i in gpt_response_split:
            if i.find('|') != -1:
                test_case.append([func_box.clean_br_string(i) for i in i.split('|')[1:]])
            elif i.find('ï½œ') != -1:
                test_case.append([func_box.clean_br_string(i) for i in i.split('ï½œ')[1:]])
            else:
                test_case.append([i])
    file_path = ExcelHandle(ipaddr=llm_kwargs['ipaddr'], is_client=is_client).lpvoid_lpbuffe(test_case, filename=file_name)
    chat_file_list += f'{file_name}ç”Ÿæˆç»“æœå¦‚ä¸‹:\t {func_box.html_download_blank(__href=file_path, dir_name=file_path.split("/")[-1])}\n\n'
    chatbot.append(['Done', chat_file_list])
    yield from toolbox.update_ui(chatbot, history)


def split_content_limit(inputs: str, llm_kwargs, chatbot, history) -> list:
    model = llm_kwargs['llm_model']
    all_tokens = bridge_all.model_info[llm_kwargs['llm_model']]['max_token']
    max_token = all_tokens/2 - all_tokens/4  # è€ƒè™‘åˆ°å¯¹è¯+å›ç­”ä¼šè¶…è¿‡tokens,æ‰€ä»¥/2
    get_token_num = bridge_all.model_info[model]['token_cnt']
    inputs = inputs.split('\n---\n')
    segments = []
    for input_ in inputs:
        if get_token_num(input_) > max_token:
            chatbot.append([None, f'{func_box.html_tag_color(input_[:10])}...å¯¹è¯é¢„è®¡è¶…å‡ºtokensé™åˆ¶, æ‹†åˆ†ä¸­...'])
            yield from toolbox.update_ui(chatbot, history)
            segments.extend(crazy_utils.breakdown_txt_to_satisfy_token_limit(input_, get_token_num, max_token))
        else:
            segments.append(input_)
    yield from toolbox.update_ui(chatbot, history)
    return segments


def input_output_processing(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history, default_prompt: str = False, all_chat: bool = True):
    """
    Args:
        gpt_response_collection:  å¤šçº¿ç¨‹GPTçš„è¿”å›ç»“æœ
        plugin_kwargs: å¯¹è¯ä½¿ç”¨çš„æ’ä»¶å‚æ•°
        llm_kwargs:  å¯¹è¯+ç”¨æˆ·ä¿¡æ¯
        default_prompt: é»˜è®¤Prompt, å¦‚æœä¸ºFalseï¼Œåˆ™ä¸æ·»åŠ æç¤ºè¯
    Returns: ä¸‹æ¬¡ä½¿ç”¨ï¼Ÿ
        inputs_arrayï¼Œ inputs_show_user_array
    """
    inputs_array = []
    inputs_show_user_array = []
    kwargs_prompt, prompt_cls = json_args_return(plugin_kwargs, ['prompt', 'prompt_cls'])
    if default_prompt: kwargs_prompt = default_prompt
    chatbot.append([None, f'æ¥ä¸‹æ¥ä½¿ç”¨çš„Promptæ˜¯ {func_box.html_tag_color(kwargs_prompt)} ï¼Œ'
                          f'ä½ å¯ä»¥ä¿å­˜ä¸€ä¸ªåŒåçš„Promptï¼Œæˆ–åœ¨{func_box.html_tag_color("è‡ªå®šä¹‰æ’ä»¶å‚æ•°")}ä¸­æŒ‡å®šå¦ä¸€ä¸ªPrmoptå“¦ï½',])
    time.sleep(1)
    prompt = prompt_generator.SqliteHandle(table=f'prompt_{llm_kwargs["ipaddr"]}').find_prompt_result(kwargs_prompt, prompt_cls)
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        content_limit = yield from split_content_limit(inputs, llm_kwargs, chatbot, history)
        for limit in content_limit:
            inputs_array.append(prompt.replace('{{{v}}}', limit))
            inputs_show_user_array.append(you_say)
    yield from toolbox.update_ui(chatbot, history)
    if all_chat:
        inputs_show_user_array = inputs_array
    else:
        inputs_show_user_array = default_prompt + ': ' + gpt_response_collection[0::2]
    return inputs_array, inputs_show_user_array


def submit_multithreaded_tasks(inputs_array, inputs_show_user_array, llm_kwargs, chatbot, history, plugin_kwargs):
    # æäº¤å¤šçº¿ç¨‹ä»»åŠ¡
    if len(inputs_array) == 1:
        # æŠ˜å è¾“å‡º
        if len(inputs_array[0]) > 200: inputs_show_user = \
            inputs_array[0][:100]+f"\n\n{func_box.html_tag_color('......è¶…è¿‡200ä¸ªå­—ç¬¦æŠ˜å ......')}\n\n"+inputs_array[0][-100:]
        else: inputs_show_user = inputs_array[0]
        gpt_say = yield from crazy_utils.request_gpt_model_in_new_thread_with_ui_alive(
            inputs=inputs_array[0], inputs_show_user=inputs_show_user,
            llm_kwargs=llm_kwargs, chatbot=chatbot, history=[],
            sys_prompt="", refresh_interval=0.1
        )
        gpt_response_collection = [inputs_show_user_array[0], gpt_say]
        history.extend(gpt_response_collection)
    else:
        gpt_response_collection = yield from crazy_utils.request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency(
            inputs_array=inputs_array,
            inputs_show_user_array=inputs_show_user_array,
            llm_kwargs=llm_kwargs,
            chatbot=chatbot,
            history_array=[[""] for _ in range(len(inputs_array))],
            sys_prompt_array=["" for _ in range(len(inputs_array))],
            # max_workers=5,  # OpenAIæ‰€å…è®¸çš„æœ€å¤§å¹¶è¡Œè¿‡è½½
            scroller_max_len=80
        )
        # æ˜¯å¦å±•ç¤ºä»»åŠ¡ç»“æœ
        kwargs_is_show,  = json_args_return(plugin_kwargs, ['is_show'])
        if kwargs_is_show:
            for results in list(zip(gpt_response_collection[0::2], gpt_response_collection[1::2])):
                chatbot.append(results)
                history.extend(results)
                yield from toolbox.update_ui(chatbot, history)
    return gpt_response_collection


def transfer_flow_chart(gpt_response_collection, llm_kwargs, chatbot, history):
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        chatbot.append([None, f'{long_name_processing(you_say)} ğŸƒğŸ»â€æ­£åœ¨åŠªåŠ›å°†Markdownè½¬æ¢ä¸ºæµç¨‹å›¾~'])
        yield from toolbox.update_ui(chatbot=chatbot, history=history)
        inputs = str(inputs).lstrip('```').rstrip('```')  # å»é™¤å¤´éƒ¨å’Œå°¾éƒ¨çš„ä»£ç å—, é¿å…æµç¨‹å›¾å †åœ¨ä¸€å—
        md, html = Utils().markdown_to_flow_chart(data=inputs, hosts=llm_kwargs['ipaddr'], file_name=long_name_processing(you_say))
        chatbot.append(("View: " + func_box.html_view_blank(md), f'{func_box.html_iframe_code(html_file=html)}'
                                                               f'tips: åŒå‡»ç©ºç™½å¤„å¯ä»¥æ”¾å¤§ï½'
                                                               f'\n\n--- \n\n Download: {func_box.html_download_blank(html)}' 
                                                              '\n\n--- \n\n View: ' + func_box.html_view_blank(html)))
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æˆåŠŸå†™å…¥æ–‡ä»¶ï¼')


def result_written_to_markdwon(gpt_response_collection, llm_kwargs, chatbot, history):
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        md = Utils().write_markdown(data=inputs, hosts=llm_kwargs['ipaddr'], file_name=long_name_processing(you_say))
        chatbot.append((None, f'markdownå·²å†™å…¥æ–‡ä»¶ï¼Œä¸‹æ¬¡å¯ä»¥ç›´æ¥æäº¤markdownæ–‡ä»¶ï¼Œå°±å¯ä»¥èŠ‚çœtomarkdownçš„æ—¶é—´å•¦ {func_box.html_view_blank(md)}'))
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æˆåŠŸå†™å…¥æ–‡ä»¶ï¼')


previously_on_plugins = f'å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»ã€ğŸ”—ã€‘å…ˆä¸Šä¼ ï¼Œå¤šä¸ªæ–‡ä»¶è¯·ä¸Šä¼ å‹ç¼©åŒ…ï¼Œ'\
                  f'å¦‚æœæ˜¯ç½‘ç»œæ–‡ä»¶æˆ–é‡‘å±±æ–‡æ¡£é“¾æ¥ï¼Œ{func_box.html_tag_color("è¯·ç²˜è´´åˆ°è¾“å…¥æ¡†, ç„¶åå†æ¬¡ç‚¹å‡»è¯¥æ’ä»¶")}'\
                  f'å¤šä¸ªæ–‡ä»¶{func_box.html_tag_color("è¯·ä½¿ç”¨æ¢è¡Œæˆ–ç©ºæ ¼åŒºåˆ†")}'


if __name__ == '__main__':
    import time
    print(get_kdocs_dir('https://www.kdocs.cn/ent/41000207/1349351159/130730080903',
                        project_folder=os.path.join(func_box.logs_path)))