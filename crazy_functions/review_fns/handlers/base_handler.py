import asyncio
from datetime import datetime
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from crazy_functions.review_fns.query_analyzer import SearchCriteria
from crazy_functions.review_fns.data_sources.arxiv_source import ArxivSource
from crazy_functions.review_fns.data_sources.semantic_source import SemanticScholarSource
from crazy_functions.review_fns.data_sources.pubmed_source import PubMedSource
from crazy_functions.review_fns.paper_processor.paper_llm_ranker import PaperLLMRanker
from crazy_functions.pdf_fns.breakdown_pdf_txt import cut_from_end_to_satisfy_token_limit
from request_llms.bridge_all import model_info
from crazy_functions.review_fns.data_sources.crossref_source import CrossrefSource
from crazy_functions.review_fns.data_sources.adsabs_source import AdsabsSource
from toolbox import get_conf


class BaseHandler(ABC):
    """å¤„ç†å™¨åŸºç±»"""

    def __init__(self, arxiv: ArxivSource, semantic: SemanticScholarSource, llm_kwargs: Dict = None):
        self.arxiv = arxiv
        self.semantic = semantic
        self.pubmed = PubMedSource()
        self.crossref = CrossrefSource()  # æ·»åŠ  Crossref å®ä¾‹
        self.adsabs = AdsabsSource()  # æ·»åŠ  ADS å®ä¾‹
        self.paper_ranker = PaperLLMRanker(llm_kwargs=llm_kwargs)
        self.ranked_papers = []  # å­˜å‚¨æ’åºåçš„è®ºæ–‡åˆ—è¡¨
        self.llm_kwargs = llm_kwargs or {}  # ä¿å­˜llm_kwargs

    def _get_search_params(self, plugin_kwargs: Dict) -> Dict:
        """è·å–æœç´¢å‚æ•°"""
        return {
            'max_papers': plugin_kwargs.get('max_papers', 100),  # æœ€å¤§è®ºæ–‡æ•°é‡
            'min_year': plugin_kwargs.get('min_year', 2015),  # æœ€æ—©å¹´ä»½
            'search_multiplier': plugin_kwargs.get('search_multiplier', 3),  # æ£€ç´¢å€æ•°
        }

    @abstractmethod
    async def handle(
            self,
            criteria: SearchCriteria,
            chatbot: List[List[str]],
            history: List[List[str]],
            system_prompt: str,
            llm_kwargs: Dict[str, Any],
            plugin_kwargs: Dict[str, Any],
    ) -> List[List[str]]:
        """å¤„ç†æŸ¥è¯¢"""
        pass

    async def _search_arxiv(self, params: Dict, limit_multiplier: int = 1, min_year: int = 2015) -> List:
        """ä½¿ç”¨arXivä¸“ç”¨å‚æ•°æœç´¢"""
        try:
            original_limit = params.get("limit", 20)
            params["limit"] = original_limit * limit_multiplier
            papers = []

            # é¦–å…ˆå°è¯•åŸºç¡€æœç´¢
            query = params.get("query", "")
            if query:
                papers = await self.arxiv.search(
                    query,
                    limit=params["limit"],
                    sort_by=params.get("sort_by", "relevance"),
                    sort_order=params.get("sort_order", "descending"),
                    start_year=min_year
                )

            # å¦‚æœåŸºç¡€æœç´¢æ²¡æœ‰ç»“æœï¼Œå°è¯•åˆ†ç±»æœç´¢
            if not papers:
                categories = params.get("categories", [])
                for category in categories:
                    category_papers = await self.arxiv.search_by_category(
                        category,
                        limit=params["limit"],
                        sort_by=params.get("sort_by", "relevance"),
                        sort_order=params.get("sort_order", "descending"),
                    )
                    if category_papers:
                        papers.extend(category_papers)

            return papers or []

        except Exception as e:
            print(f"arXivæœç´¢å‡ºé”™: {str(e)}")
            return []

    async def _search_semantic(self, params: Dict, limit_multiplier: int = 1, min_year: int = 2015) -> List:
        """ä½¿ç”¨Semantic Scholarä¸“ç”¨å‚æ•°æœç´¢"""
        try:
            original_limit = params.get("limit", 20)
            params["limit"] = original_limit * limit_multiplier

            # åªä½¿ç”¨åŸºæœ¬çš„æœç´¢å‚æ•°
            papers = await self.semantic.search(
                query=params.get("query", ""),
                limit=params["limit"]
            )

            # åœ¨å†…å­˜ä¸­è¿›è¡Œè¿‡æ»¤
            if papers and min_year:
                papers = [p for p in papers if getattr(p, 'year', 0) and p.year >= min_year]

            return papers or []

        except Exception as e:
            print(f"Semantic Scholaræœç´¢å‡ºé”™: {str(e)}")
            return []

    async def _search_pubmed(self, params: Dict, limit_multiplier: int = 1, min_year: int = 2015) -> List:
        """ä½¿ç”¨PubMedä¸“ç”¨å‚æ•°æœç´¢"""
        try:
            # å¦‚æœä¸éœ€è¦PubMedæœç´¢ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
            if params.get("search_type") == "none":
                return []

            original_limit = params.get("limit", 20)
            params["limit"] = original_limit * limit_multiplier
            papers = []

            # æ ¹æ®æœç´¢ç±»å‹é€‰æ‹©æœç´¢æ–¹æ³•
            if params.get("search_type") == "basic":
                papers = await self.pubmed.search(
                    query=params.get("query", ""),
                    limit=params["limit"],
                    start_year=min_year
                )
            elif params.get("search_type") == "author":
                papers = await self.pubmed.search_by_author(
                    author=params.get("query", ""),
                    limit=params["limit"],
                    start_year=min_year
                )
            elif params.get("search_type") == "journal":
                papers = await self.pubmed.search_by_journal(
                    journal=params.get("query", ""),
                    limit=params["limit"],
                    start_year=min_year
                )

            return papers or []

        except Exception as e:
            print(f"PubMedæœç´¢å‡ºé”™: {str(e)}")
            return []

    async def _search_crossref(self, params: Dict, limit_multiplier: int = 1, min_year: int = 2015) -> List:
        """ä½¿ç”¨Crossrefä¸“ç”¨å‚æ•°æœç´¢"""
        try:
            original_limit = params.get("limit", 20)
            params["limit"] = original_limit * limit_multiplier
            papers = []

            # æ ¹æ®æœç´¢ç±»å‹é€‰æ‹©æœç´¢æ–¹æ³•
            if params.get("search_type") == "basic":
                papers = await self.crossref.search(
                    query=params.get("query", ""),
                    limit=params["limit"],
                    start_year=min_year
                )
            elif params.get("search_type") == "author":
                papers = await self.crossref.search_by_authors(
                    authors=[params.get("query", "")],
                    limit=params["limit"],
                    start_year=min_year
                )
            elif params.get("search_type") == "journal":
                # å®ç°æœŸåˆŠæœç´¢é€»è¾‘
                pass

            return papers or []

        except Exception as e:
            print(f"Crossrefæœç´¢å‡ºé”™: {str(e)}")
            return []

    async def _search_adsabs(self, params: Dict, limit_multiplier: int = 1, min_year: int = 2015) -> List:
        """ä½¿ç”¨ADSä¸“ç”¨å‚æ•°æœç´¢"""
        try:
            original_limit = params.get("limit", 20)
            params["limit"] = original_limit * limit_multiplier
            papers = []

            # æ‰§è¡Œæœç´¢
            if params.get("search_type") == "basic":
                papers = await self.adsabs.search(
                    query=params.get("query", ""),
                    limit=params["limit"],
                    start_year=min_year
                )

            return papers or []

        except Exception as e:
            print(f"ADSæœç´¢å‡ºé”™: {str(e)}")
            return []

    async def _search_all_sources(self, criteria: SearchCriteria, search_params: Dict) -> List:
        """ä»æ‰€æœ‰æ•°æ®æºæœç´¢è®ºæ–‡"""
        search_tasks = []

        # # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡ŒPubMedæœç´¢
        # is_using_pubmed = criteria.pubmed_params.get("search_type") != "none" and criteria.pubmed_params.get("query") != "none"
        is_using_pubmed = False # å¼€æºç‰ˆæœ¬ä¸å†æœç´¢pubmed

        # å¦‚æœä½¿ç”¨PubMedï¼Œåˆ™åªæ‰§è¡ŒPubMedå’ŒSemantic Scholaræœç´¢
        if is_using_pubmed:
            search_tasks.append(
                self._search_pubmed(
                    criteria.pubmed_params,
                    limit_multiplier=search_params['search_multiplier'],
                    min_year=criteria.start_year
                )
            )

            # Semantic Scholaræ€»æ˜¯æ‰§è¡Œæœç´¢
            search_tasks.append(
                self._search_semantic(
                    criteria.semantic_params,
                    limit_multiplier=search_params['search_multiplier'],
                    min_year=criteria.start_year
                )
            )
        else:

            # å¦‚æœä¸ä½¿ç”¨ADSï¼Œåˆ™æ‰§è¡ŒCrossrefæœç´¢
            if criteria.crossref_params.get("search_type") != "none" and criteria.crossref_params.get("query") != "none":
                search_tasks.append(
                    self._search_crossref(
                        criteria.crossref_params,
                        limit_multiplier=search_params['search_multiplier'],
                        min_year=criteria.start_year
                    )
                )

            search_tasks.append(
                self._search_arxiv(
                    criteria.arxiv_params,
                    limit_multiplier=search_params['search_multiplier'],
                    min_year=criteria.start_year
                )
            )
            if get_conf("SEMANTIC_SCHOLAR_KEY"):
                search_tasks.append(
                    self._search_semantic(
                        criteria.semantic_params,
                        limit_multiplier=search_params['search_multiplier'],
                        min_year=criteria.start_year
                    )
                )

        # æ‰§è¡Œæ‰€æœ‰éœ€è¦çš„æœç´¢ä»»åŠ¡
        papers = await asyncio.gather(*search_tasks)

        # åˆå¹¶æ‰€æœ‰æ¥æºçš„è®ºæ–‡å¹¶ç»Ÿè®¡å„æ¥æºçš„æ•°é‡
        all_papers = []
        source_counts = {
            'arxiv': 0,
            'semantic': 0,
            'pubmed': 0,
            'crossref': 0,
            'adsabs': 0
        }

        for source_papers in papers:
            if source_papers:
                for paper in source_papers:
                    source = getattr(paper, 'source', 'unknown')
                    if source in source_counts:
                        source_counts[source] += 1
                all_papers.extend(source_papers)

        # æ‰“å°å„æ¥æºçš„è®ºæ–‡æ•°é‡
        print("\n=== å„æ•°æ®æºæ‰¾åˆ°çš„è®ºæ–‡æ•°é‡ ===")
        for source, count in source_counts.items():
            if count > 0:  # åªæ‰“å°æœ‰è®ºæ–‡çš„æ¥æº
                print(f"{source.capitalize()}: {count} ç¯‡")
        print(f"æ€»è®¡: {len(all_papers)} ç¯‡")
        print("===========================\n")

        return all_papers

    def _format_paper_time(self, paper) -> str:
        """æ ¼å¼åŒ–è®ºæ–‡æ—¶é—´ä¿¡æ¯"""
        year = getattr(paper, 'year', None)
        if not year:
            return ""

        # å¦‚æœæœ‰å…·ä½“çš„å‘è¡¨æ—¥æœŸï¼Œä½¿ç”¨å…·ä½“æ—¥æœŸ
        if hasattr(paper, 'published') and paper.published:
            return f"(å‘è¡¨äº {paper.published.strftime('%Y-%m')})"
        # å¦‚æœåªæœ‰å¹´ä»½ï¼Œåªæ˜¾ç¤ºå¹´ä»½
        return f"({year})"

    def _format_papers(self, papers: List) -> str:
        """æ ¼å¼åŒ–è®ºæ–‡åˆ—è¡¨ï¼Œä½¿ç”¨tokené™åˆ¶æ§åˆ¶é•¿åº¦"""
        formatted = []

        for i, paper in enumerate(papers, 1):
            # åªä¿ç•™å‰ä¸‰ä¸ªä½œè€…
            authors = paper.authors[:3]
            if len(paper.authors) > 3:
                authors.append("et al.")

            # æ„å»ºæ‰€æœ‰å¯èƒ½çš„ä¸‹è½½é“¾æ¥
            download_links = []

            # æ·»åŠ arXivé“¾æ¥
            if hasattr(paper, 'doi') and paper.doi:
                if paper.doi.startswith("10.48550/arXiv."):
                    # ä»DOIä¸­æå–å®Œæ•´çš„arXiv ID
                    arxiv_id = paper.doi.split("arXiv.")[-1]
                    # ç§»é™¤å¤šä½™çš„ç‚¹å·å¹¶ç¡®ä¿æ ¼å¼æ­£ç¡®
                    arxiv_id = arxiv_id.replace("..", ".")  # ç§»é™¤é‡å¤çš„ç‚¹å·
                    if arxiv_id.startswith("."):  # ç§»é™¤å¼€å¤´çš„ç‚¹å·
                        arxiv_id = arxiv_id[1:]
                    if arxiv_id.endswith("."):  # ç§»é™¤ç»“å°¾çš„ç‚¹å·
                        arxiv_id = arxiv_id[:-1]

                    download_links.append(f"[arXiv PDF](https://arxiv.org/pdf/{arxiv_id}.pdf)")
                    download_links.append(f"[arXiv Page](https://arxiv.org/abs/{arxiv_id})")
                elif "arxiv.org/abs/" in paper.doi:
                    # ç›´æ¥ä»URLä¸­æå–arXiv ID
                    arxiv_id = paper.doi.split("arxiv.org/abs/")[-1]
                    if "v" in arxiv_id:  # ç§»é™¤ç‰ˆæœ¬å·
                        arxiv_id = arxiv_id.split("v")[0]

                    download_links.append(f"[arXiv PDF](https://arxiv.org/pdf/{arxiv_id}.pdf)")
                    download_links.append(f"[arXiv Page](https://arxiv.org/abs/{arxiv_id})")
                else:
                    download_links.append(f"[DOI](https://doi.org/{paper.doi})")

            # æ·»åŠ ç›´æ¥URLé“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸åŒäºå‰é¢çš„é“¾æ¥ï¼‰
            if hasattr(paper, 'url') and paper.url:
                if not any(paper.url in link for link in download_links):
                    download_links.append(f"[Source]({paper.url})")

            # æ„å»ºä¸‹è½½é“¾æ¥å­—ç¬¦ä¸²
            download_section = " | ".join(download_links) if download_links else "No direct download link available"

            # æ„å»ºæ¥æºä¿¡æ¯
            source_info = []
            if hasattr(paper, 'venue_type') and paper.venue_type and paper.venue_type != 'preprint':
                source_info.append(f"Type: {paper.venue_type}")
            if hasattr(paper, 'venue_name') and paper.venue_name:
                source_info.append(f"Venue: {paper.venue_name}")

            # æ·»åŠ IFæŒ‡æ•°å’Œåˆ†åŒºä¿¡æ¯
            if hasattr(paper, 'if_factor') and paper.if_factor:
                source_info.append(f"IF: {paper.if_factor}")
            if hasattr(paper, 'cas_division') and paper.cas_division:
                source_info.append(f"ä¸­ç§‘é™¢åˆ†åŒº: {paper.cas_division}")
            if hasattr(paper, 'jcr_division') and paper.jcr_division:
                source_info.append(f"JCRåˆ†åŒº: {paper.jcr_division}")

            if hasattr(paper, 'venue_info') and paper.venue_info:
                if paper.venue_info.get('journal_ref'):
                    source_info.append(f"Journal Reference: {paper.venue_info['journal_ref']}")
                if paper.venue_info.get('publisher'):
                    source_info.append(f"Publisher: {paper.venue_info['publisher']}")

            # æ„å»ºå½“å‰è®ºæ–‡çš„æ ¼å¼åŒ–æ–‡æœ¬
            paper_text = (
                    f"{i}. **{paper.title}**\n" +
                    f"   Authors: {', '.join(authors)}\n" +
                    f"   Year: {paper.year}\n" +
                    f"   Citations: {paper.citations if paper.citations else 'N/A'}\n" +
                    (f"   Source: {'; '.join(source_info)}\n" if source_info else "") +
                    # æ·»åŠ PubMedç‰¹æœ‰ä¿¡æ¯
                    (f"   MeSH Terms: {'; '.join(paper.mesh_terms)}\n" if hasattr(paper,
                                                                                  'mesh_terms') and paper.mesh_terms else "") +
                    f"   ğŸ“¥ PDF Downloads: {download_section}\n" +
                    f"   Abstract: {paper.abstract}\n"
            )

            formatted.append(paper_text)

        full_text = "\n".join(formatted)

        # æ ¹æ®ä¸åŒæ¨¡å‹è®¾ç½®ä¸åŒçš„tokené™åˆ¶
        model_name = getattr(self, 'llm_kwargs', {}).get('llm_model', 'gpt-3.5-turbo')

        token_limit = model_info[model_name]['max_token'] * 3 // 4
        # ä½¿ç”¨tokené™åˆ¶æ§åˆ¶é•¿åº¦
        return cut_from_end_to_satisfy_token_limit(full_text, limit=token_limit, reserve_token=0, llm_model=model_name)

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´ä¿¡æ¯"""
        now = datetime.now()
        return now.strftime("%Yå¹´%mæœˆ%dæ—¥")

    def _generate_apology_prompt(self, criteria: SearchCriteria) -> str:
        """ç”Ÿæˆé“æ­‰æç¤º"""
        return f"""å¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬æœªèƒ½æ‰¾åˆ°ä¸"{criteria.main_topic}"ç›¸å…³çš„æœ‰æ•ˆæ–‡çŒ®ã€‚

å¯èƒ½çš„åŸå› ï¼š
1. æœç´¢è¯è¿‡äºå…·ä½“æˆ–ä¸“ä¸š
2. æ—¶é—´èŒƒå›´é™åˆ¶è¿‡ä¸¥

å»ºè®®è§£å†³æ–¹æ¡ˆï¼š
   1. å°è¯•ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
   2. æ‰©å¤§æœç´¢æ—¶é—´èŒƒå›´
   3. ä½¿ç”¨åŒä¹‰è¯æˆ–ç›¸å…³æœ¯è¯­
è¯·æ ¹æ®ä»¥ä¸Šå»ºè®®è°ƒæ•´åé‡è¯•ã€‚"""

    def get_ranked_papers(self) -> str:
        """è·å–æ’åºåçš„è®ºæ–‡åˆ—è¡¨çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
        return self._format_papers(self.ranked_papers) if self.ranked_papers else ""

    def _is_pubmed_paper(self, paper) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºPubMedè®ºæ–‡"""
        return (paper.url and 'pubmed.ncbi.nlm.nih.gov' in paper.url)