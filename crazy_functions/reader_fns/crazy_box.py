# encoding: utf-8
# @Time   : 2023/6/14
# @Author : Spike
# @Descr   :
import os
import json
import re
from common import func_box, toolbox, db_handler, Langchain_cn
from crazy_functions import crazy_utils
from request_llms import bridge_all
from moviepy.editor import AudioFileClip
from common.path_handler import init_path
from crazy_functions import reader_fns

class Utils:

    def __init__(self):
        self.find_keys_type = 'type'
        self.find_picture_source = ['caption', 'imgID', 'sourceKey']
        self.find_document_source = ['wpsDocumentLink', 'wpsDocumentName', 'wpsDocumentType']
        self.find_document_tags = ['WPSDocument']
        self.find_picture_tags = ['picture', 'processon']
        self.picture_format = func_box.valid_img_extensions
        self.comments = []

    def find_all_text_keys(self, dictionary, parent_type=None, text_values=None, filter_type=''):
        """
        Args:
            dictionary: å­—å…¸æˆ–åˆ—è¡¨
            parent_type: åŒ¹é…çš„typeï¼Œä½œä¸ºæ–°åˆ—è¡¨çš„keyï¼Œç”¨äºåˆ†ç±»
            text_values: å­˜å‚¨åˆ—è¡¨
            filter_type: å½“å‰å±‚çº§find_keys_type==filter_typeæ—¶ï¼Œä¸ç»§ç»­å¾€ä¸‹åµŒå¥—
        Returns:
            text_valueså’Œæ’åºåçš„context_
        """
        # åˆå§‹åŒ– text_values ä¸ºç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ‰¾åˆ°çš„æ‰€æœ‰textå€¼
        if text_values is None:
            text_values = []
        # å¦‚æœè¾“å…¥çš„dictionaryä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè¿”å›å·²æ”¶é›†åˆ°çš„textå€¼
        if not isinstance(dictionary, dict):
            return text_values
        # è·å–å½“å‰å±‚çº§çš„ type å€¼
        current_type = dictionary.get(self.find_keys_type, parent_type)
        # å¦‚æœå­—å…¸ä¸­åŒ…å« 'text' æˆ– 'caption' é”®ï¼Œå°†å¯¹åº”çš„å€¼æ·»åŠ åˆ° text_values åˆ—è¡¨ä¸­
        if 'comments' in dictionary:
            temp = dictionary.get('comments', [])
            for t in temp:
                if type(t) is dict:
                    self.comments.append(t.get('key'))
        if 'text' in dictionary:
            content_value = dictionary.get('text', None)
            text_values.append({current_type: content_value})
        if 'caption' in dictionary:
            temp = {}
            for key in self.find_picture_source:
                temp[key] = dictionary.get(key, None)
            text_values.append({current_type: temp})
        if 'wpsDocumentId' in dictionary:
            temp = {}
            for key in self.find_document_source:
                temp[key] = dictionary.get(key, None)
            text_values.append({current_type: temp})
        # å¦‚æœå½“å‰ç±»å‹ä¸ç­‰äº filter_typeï¼Œåˆ™ç»§ç»­éå†å­çº§å±æ€§
        if current_type != filter_type:
            for key, value in dictionary.items():
                if isinstance(value, dict):
                    self.find_all_text_keys(value, current_type, text_values, filter_type)
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict):
                            self.find_all_text_keys(item, current_type, text_values, filter_type)
        return text_values

    def statistical_results(self, text_values, img_proce=False):
        """
        Args: æå–çˆ¬è™«å†…åµŒå›¾ç‰‡ã€æ–‡ä»¶ç­‰ç­‰ä¿¡æ¯
            text_values: dict
            img_proce: å›¾ç‰‡æ ‡è¯†
        Returns: ï¼ˆå…ƒæ•°æ®ï¼Œ ç»„åˆæ•°æ®ï¼Œ å›¾ç‰‡dictï¼Œ æ–‡ä»¶dictï¼‰
        """
        context_ = []
        pic_dict = {}
        file_dict = {}
        for i in text_values:
            for key, value in i.items():
                if key in self.find_picture_tags:
                    if img_proce:
                        mark = f'{key}"""\n{value["sourceKey"]}\n"""\n'
                        if value["caption"]: mark += f'\n{key}:{value["caption"]}\n\n'
                        context_.append(mark)
                        pic_dict[value['sourceKey']] = value['imgID']
                    else:
                        if value["caption"]: context_.append(f'{key}æè¿°: {value["caption"]}\n')
                        pic_dict[value['sourceKey']] = value['imgID']
                elif key in self.find_document_tags:
                    mark = f'{value["wpsDocumentName"]}: {value["wpsDocumentLink"]}'
                    file_dict.update({value["wpsDocumentName"]: value["wpsDocumentLink"]})
                    context_.append(mark)
                else:
                    context_.append(value)
        context_ = '\n'.join(context_)
        return text_values, context_, pic_dict, file_dict

    def write_markdown(self, data, hosts, file_name):
        """
        Args: å°†dataå†™å…¥mdæ–‡ä»¶
            data: æ•°æ®
            hosts: ç”¨æˆ·æ ‡è¯†
            file_name: å¦å–æ–‡ä»¶å
        Returns: å†™å…¥çš„æ–‡ä»¶åœ°å€
        """
        user_path = os.path.join(init_path.private_files_path, hosts, 'markdown')
        os.makedirs(user_path, exist_ok=True)
        md_file = os.path.join(user_path, f"{file_name}.md")
        with open(file=md_file, mode='w', encoding='utf-8') as f:
            f.write(data)
        return md_file

    def markdown_to_flow_chart(self, data, hosts, file_name):
        """
        Args: è°ƒç”¨markmap-cli
            data: è¦å†™å…¥mdçš„æ•°æ®
            hosts: ç”¨æˆ·æ ‡è¯†
            file_name: è¦å†™å…¥çš„æ–‡ä»¶å
        Returns: [md, æµç¨‹å›¾] æ–‡ä»¶
        """
        user_path = os.path.join(init_path.private_files_path, hosts, 'mark_map')
        os.makedirs(user_path, exist_ok=True)
        md_file = self.write_markdown(data, hosts, file_name)
        html_file = os.path.join(user_path, f"{file_name}.html")
        func_box.Shell(f'npx markmap-cli --no-open "{md_file}" -o "{html_file}"').start()
        return md_file, html_file

    def global_search_for_files(self, file_path, matching: list):
        file_list = []
        if os.path.isfile(file_path):
            file_list.append(file_path)
        for root, dirs, files in os.walk(file_path):
            for file in files:
                for math in matching:
                    if str(math).lower() in str(file).lower():
                        file_list.append(os.path.join(root, file))
        return file_list


# <---------------------------------------ä¹±ä¸ƒå…«ç³Ÿçš„æ–¹æ³•ï¼Œæœ‰ç‚¹ç”¨ï¼Œå¾ˆå¥½ç”¨----------------------------------------->
def find_index_inlist(data_list: list, search_terms: list) -> int:
    """ åœ¨data_listæ‰¾åˆ°ç¬¦åˆsearch_termså­—ç¬¦ä¸²ï¼Œæ‰¾åˆ°åç›´æ¥è¿”å›ä¸‹æ ‡
    Args:
        data_list: listæ•°æ®ï¼Œæœ€å¤šå¾€é‡Œé¢æ‰¾ä¸¤å±‚
        search_terms: listæ•°æ®ï¼Œç¬¦åˆä¸€ä¸ªå°±è¿”å›æ•°æ®
    Returns: å¯¹åº”çš„ä¸‹æ ‡
    """
    for i, sublist in enumerate(data_list):
        if any(term in str(sublist) for term in search_terms):
            return i
        for j, item in enumerate(sublist):
            if any(term in str(item) for term in search_terms):
                return i
    return 0  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å…ƒç´ ï¼Œåˆ™è¿”å›åˆå§‹åæ ‡


def file_extraction_intype(file_mapping, chatbot, history, llm_kwargs, plugin_kwargs):
    # æ–‡ä»¶è¯»å–
    file_limit = {}
    for file_path in file_mapping:
        chatbot[-1][1] = chatbot[-1][1] + f'\n\n`{file_path.replace(init_path.base_path, ".")}`\t...æ­£åœ¨è§£ææœ¬åœ°æ–‡ä»¶\n\n'
        yield from toolbox.update_ui(chatbot, history)
        save_path = os.path.join(init_path.private_files_path, llm_kwargs['ipaddr'])
        if file_path.endswith('pdf'):
            _content, _ = crazy_utils.read_and_clean_pdf_text(file_path)
            file_content = "".join(_content)
        elif file_path.endswith('docx') or file_path.endswith('doc'):
            file_content = reader_fns.DocxHandler(file_path, save_path).get_markdown()
        elif file_path.endswith('xmind'):
            file_content = reader_fns.XmindHandle(file_path, save_path).get_markdown()
        elif file_path.endswith('mp4'):
            file_content = yield from audio_comparison_of_video_converters([file_path], chatbot, history)
        elif file_path.endswith('xlsx') or file_path.endswith('xls'):
            sheet, = json_args_return(plugin_kwargs, keys=['è¯»å–æŒ‡å®šSheet'], default='æµ‹è¯•è¦ç‚¹')
            # åˆ›å»ºæ–‡ä»¶å¯¹è±¡
            ex_handle = reader_fns.XlsxHandler(file_path, save_path, sheet=sheet)
            if sheet in ex_handle.workbook.sheetnames:
                ex_handle.split_merged_cells()
                xlsx_dict = ex_handle.read_as_dict()
                file_content = xlsx_dict.get(sheet)
            else:
                active_sheet = ex_handle.workbook.active.title
                ex_handle.sheet = active_sheet
                ex_handle.split_merged_cells()
                xlsx_dict = ex_handle.read_as_dict()
                file_content = xlsx_dict.get(active_sheet)
                chatbot.append([None,
                                f'æ— æ³•åœ¨`{os.path.basename(file_path)}`æ‰¾åˆ°`{sheet}`å·¥ä½œè¡¨ï¼Œ'
                                f'å°†è¯»å–ä¸Šæ¬¡é¢„è§ˆçš„æ´»åŠ¨å·¥ä½œè¡¨`{active_sheet}`ï¼Œ'
                                f'è‹¥ä½ çš„ç”¨ä¾‹å·¥ä½œè¡¨æ˜¯å…¶ä»–åç§°, è¯·åŠæ—¶æš‚åœæ’ä»¶è¿è¡Œï¼Œå¹¶åœ¨è‡ªå®šä¹‰æ’ä»¶é…ç½®ä¸­æ›´æ”¹'
                                f'{func_box.html_tag_color("è¯»å–æŒ‡å®šSheet")}ã€‚'])
            plugin_kwargs['å†™å…¥æŒ‡å®šæ¨¡ç‰ˆ'] = file_path
            plugin_kwargs['å†™å…¥æŒ‡å®šSheet'] = ex_handle.sheet
            yield from toolbox.update_ui(chatbot, history)
        else:
            with open(file_path, mode='r', encoding='utf-8') as f:
                file_content = f.read()
        file_limit[file_path] = file_content
        yield from toolbox.update_ui(chatbot, history)
    return file_limit


def json_args_return(kwargs, keys: list, default=None) -> list:
    """
    Args: æå–æ’ä»¶çš„è°ƒä¼˜å‚æ•°ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¿”å›å–åˆ°çš„å€¼ï¼Œå¦‚æœæ— ï¼Œåˆ™è¿”å›False
        kwargs: ä¸€èˆ¬æ˜¯plugin_kwargs
        keys: éœ€è¦å–å¾—key
        default: æ‰¾ä¸åˆ°æ—¶æ€»å¾—è¿”å›ä»€ä¹ˆä¸œè¥¿
    Returns: æœ‰keyè¿”valueï¼Œæ— keyè¿”False
    """
    temp = [default for i in range(len(keys))]
    for i in range(len(keys)):
        try:
            temp[i] = kwargs[keys[i]]
        except Exception:
            try:
                temp[i] = json.loads(kwargs['advanced_arg'])[keys[i]]
            except Exception as f:
                try:
                    temp[i] = kwargs['parameters_def'][keys[i]]
                except Exception as f:
                    temp[i] = default
    return temp


def long_name_processing(file_name):
    """
    Args:
        file_name: æ–‡ä»¶åå–æï¼Œå¦‚æœæ˜¯listï¼Œåˆ™å–ä¸‹æ ‡0ï¼Œè½¬æ¢ä¸ºstrï¼Œ å¦‚æœæ˜¯stråˆ™å–æœ€å¤š20ä¸ªå­—ç¬¦
    Returns: è¿”å›å¤„ç†è¿‡çš„æ–‡ä»¶å
    """
    if type(file_name) is list:
        file_name = file_name[0]
    if len(file_name) > 20:
        temp = file_name.splitlines()
        for i in temp:
            if i:
                file_name = func_box.replace_special_chars(i)
                break
    if file_name.find('.') != -1:
        file_name = "".join(file_name.split('.')[:-1])
    return file_name


# <---------------------------------------æ’ä»¶ç”¨äº†éƒ½è¯´å¥½æ–¹æ³•----------------------------------------->
def split_list_token_limit(data, get_num, max_num=500):
    header_index = find_index_inlist(data_list=data, search_terms=['æ“ä½œæ­¥éª¤', 'å‰ç½®æ¡ä»¶', 'é¢„æœŸç»“æœ'])
    header_data = data[header_index]
    max_num -= len(str(header_data))
    temp_list = []
    split_data = []
    for index in data[header_index + 1:]:
        if get_num(str(temp_list)) > max_num:
            temp_list.insert(0, header_data)
            split_data.append(json.dumps(temp_list, ensure_ascii=False))
            temp_list = []
        else:
            temp_list.append(index)
    return split_data


def split_content_limit(inputs: str, llm_kwargs, chatbot, history) -> list:
    """
    Args:
        inputs: éœ€è¦æå–æ‹†åˆ†çš„æé—®ä¿¡æ¯
        llm_kwargs: è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²è®°å½•
    Returns: [æ‹†åˆ†1ï¼Œ æ‹†åˆ†2]
    """
    model = llm_kwargs['llm_model']
    if model.find('&') != -1:  # åˆ¤æ–­æ˜¯å¦å¤šæ¨¡å‹ï¼Œå¦‚æœå¤šæ¨¡å‹ï¼Œé‚£ä¹ˆä½¿ç”¨tokensæœ€å°‘çš„è¿›è¡Œæ‹†åˆ†
        models = str(model).split('&')
        _tokens = []
        _num_func = {}
        for _model in models:
            num_s = bridge_all.model_info[_model]['max_token']
            _tokens.append(num_s)
            _num_func[num_s] = _model
        all_tokens = min(_tokens)
        get_token_num = bridge_all.model_info[_num_func[all_tokens]]['token_cnt']
    else:
        all_tokens = bridge_all.model_info[model]['max_token']
        get_token_num = bridge_all.model_info[model]['token_cnt']
    max_token = all_tokens / 2 - all_tokens / 4  # è€ƒè™‘åˆ°å¯¹è¯+å›ç­”ä¼šè¶…è¿‡tokens,æ‰€ä»¥/2
    segments = []
    if type(inputs) is list:
        if get_token_num(str(inputs)) > max_token:
            chatbot.append([None,
                            f'{func_box.html_tag_color(inputs[0][:10])}... å¯¹è¯æ•°æ®é¢„è®¡ä¼šè¶…å‡º`{all_tokens}v token`é™åˆ¶, æ‹†åˆ†ä¸­'])
            segments.extend(split_list_token_limit(data=inputs, get_num=get_token_num, max_num=max_token))
        else:
            segments.append(json.dumps(inputs, ensure_ascii=False))
    else:
        inputs = inputs.split('\n---\n')
        for input_ in inputs:
            if get_token_num(input_) > max_token:
                chatbot.append([None,
                                f'{func_box.html_tag_color(input_[:10])}... å¯¹è¯æ•°æ®é¢„è®¡ä¼šè¶…å‡º`{all_tokens}token`é™åˆ¶, æ‹†åˆ†ä¸­'])
                yield from toolbox.update_ui(chatbot, history)
                segments.extend(
                    crazy_utils.breakdown_txt_to_satisfy_token_limit_for_pdf(input_, get_token_num, max_token))
            else:
                segments.append(input_)
    yield from toolbox.update_ui(chatbot, history)
    return segments


def input_output_processing(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history,
                            kwargs_prompt: str = False, knowledge_base: bool = False):
    """
    Args:
        gpt_response_collection:  å¤šçº¿ç¨‹GPTçš„è¿”å›ç»“æœoræ–‡ä»¶è¯»å–å¤„ç†åçš„ç»“æœ
        plugin_kwargs: å¯¹è¯ä½¿ç”¨çš„æ’ä»¶å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²å¯¹è¯
        llm_kwargs:  è°ƒä¼˜å‚æ•°
        kwargs_prompt: Promptåç§°, å¦‚æœä¸ºFalseï¼Œåˆ™ä¸æ·»åŠ æç¤ºè¯
        knowledge_base: æ˜¯å¦å¯ç”¨çŸ¥è¯†åº“
    Returns: ä¸‹æ¬¡ä½¿ç”¨ï¼Ÿ
        inputs_arrayï¼Œ inputs_show_user_array
    """
    inputs_array = []
    inputs_show_user_array = []
    prompt_cls, = json_args_return(plugin_kwargs, ['æç¤ºè¯åˆ†ç±»'])
    prompt_cls_tab = func_box.prompt_personal_tag(prompt_cls, ipaddr=llm_kwargs["ipaddr"])
    if kwargs_prompt:
        prompt = db_handler.PromptDb(table=prompt_cls_tab).find_prompt_result(kwargs_prompt)
    else:
        prompt = ''
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        content_limit = yield from split_content_limit(inputs, llm_kwargs, chatbot, history)
        try:
            plugin_kwargs['ä¸Šé˜¶æ®µæ–‡ä»¶'] = you_say
            plugin_kwargs[you_say] = {}
            plugin_kwargs[you_say]['åŸæµ‹è¯•ç”¨ä¾‹æ•°æ®'] = [json.loads(limit)[1:] for limit in content_limit]
            plugin_kwargs[you_say]['åŸæµ‹è¯•ç”¨ä¾‹è¡¨å¤´'] = json.loads(content_limit[0])[0]
        except Exception as f:
            print(f'è¯»å–åŸæµ‹è¯•ç”¨ä¾‹æŠ¥é”™ {f}')
        for limit in content_limit:
            if knowledge_base:
                try:
                    limit = yield from Langchain_cn.knowledge_base_query(limit, chatbot, history, llm_kwargs,
                                                                         plugin_kwargs)
                except Exception as f:
                    func_box.é€šçŸ¥æœºå™¨äºº(f'è¯»å–çŸ¥è¯†åº“å¤±è´¥ï¼Œè¯·æ£€æŸ¥{f}')
            # æ‹¼æ¥å†…å®¹ä¸æç¤ºè¯
            plugin_prompt = func_box.replace_expected_text(prompt, content=limit, expect='{{{v}}}')
            user_prompt = plugin_kwargs.get('user_input_prompt', '')
            inputs_array.append(plugin_prompt + user_prompt)
            inputs_show_user_array.append(you_say)
    plugin_kwargs['user_input_prompt'] = ''  # ç»„åˆåå»é™¤ user_input_prompt
    yield from toolbox.update_ui(chatbot, history)
    return inputs_array, inputs_show_user_array


def submit_multithreaded_tasks(inputs_array, inputs_show_user_array, llm_kwargs, chatbot, history, plugin_kwargs):
    """
    Args: æäº¤å¤šçº¿ç¨‹ä»»åŠ¡
        inputs_array: éœ€è¦æäº¤ç»™gptçš„ä»»åŠ¡åˆ—è¡¨
        inputs_show_user_array: æ˜¾ç¤ºåœ¨useré¡µé¢ä¸Šä¿¡æ¯
        llm_kwargs: è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²å¯¹è¯
        plugin_kwargs: æ’ä»¶è°ƒä¼˜å‚æ•°
    Returns:  å°†å¯¹è¯ç»“æœè¿”å›[è¾“å…¥, è¾“å‡º]
    """
    apply_history, = json_args_return(plugin_kwargs, ['ä¸Šä¸‹æ–‡å¤„ç†'])
    if apply_history:
        history_array = [[history] for _ in range(len(inputs_array))]
    else:
        history_array = [[""] for _ in range(len(inputs_array))]
    # æ˜¯å¦è¦å¤šçº¿ç¨‹å¤„ç†
    if len(inputs_array) == 1:
        inputs_show_user = None  # ä¸é‡å¤å±•ç¤º
        gpt_say = yield from crazy_utils.request_gpt_model_in_new_thread_with_ui_alive(
            inputs=inputs_array[0], inputs_show_user=inputs_show_user,
            llm_kwargs=llm_kwargs, chatbot=chatbot, history=history_array[0],
            sys_prompt="", refresh_interval=0.1
        )
        gpt_response_collection = [inputs_show_user_array[0], gpt_say]
        history.extend(gpt_response_collection)
    else:
        gpt_response_collection = yield from crazy_utils.request_gpt_model_multi_threads_with_very_awesome_ui_and_high_efficiency(
            inputs_array=inputs_array,
            inputs_show_user_array=inputs_show_user_array,
            llm_kwargs=llm_kwargs,
            chatbot=chatbot,
            history_array=history_array,
            sys_prompt_array=["" for _ in range(len(inputs_array))],
            # max_workers=5,  # OpenAIæ‰€å…è®¸çš„æœ€å¤§å¹¶è¡Œè¿‡è½½
            scroller_max_len=80,
        )
    if apply_history:
        history.extend(gpt_response_collection)
    return gpt_response_collection


def func_æ‹†åˆ†ä¸æé—®(file_limit, llm_kwargs, plugin_kwargs, chatbot, history, plugin_prompt, knowledge_base,
                    task_tag: str = ''):
    split_content_limit = yield from input_output_processing(file_limit, llm_kwargs, plugin_kwargs,
                                                             chatbot, history, kwargs_prompt=plugin_prompt,
                                                             knowledge_base=knowledge_base)
    inputs_array, inputs_show_user_array = split_content_limit
    gpt_response_collection = yield from submit_multithreaded_tasks(inputs_array, inputs_show_user_array,
                                                                    llm_kwargs, chatbot, history,
                                                                    plugin_kwargs)
    return gpt_response_collection


# <---------------------------------------å†™å…¥æ–‡ä»¶æ–¹æ³•----------------------------------------->
def file_classification_to_dict(gpt_response_collection):
    """
    æ¥æ”¶gptå¤šçº¿ç¨‹çš„è¿”å›æ•°æ®ï¼Œå°†è¾“å…¥ç›¸åŒçš„ä½œä¸ºkey, gptè¿”å›ä»¥åˆ—è¡¨å½¢å¼æ·»åŠ åˆ°å¯¹åº”çš„keyä¸­ï¼Œä¸»è¦æ˜¯ä¸ºäº†åŒºåˆ†ä¸ç”¨æ–‡ä»¶çš„è¾“å…¥
    Args:
        gpt_response_collection: å¤šçº¿ç¨‹GPTçš„è¿”å›è€¶
    Returns: {'æ–‡ä»¶': [ç»“æœ1ï¼Œ ç»“æœ2...], 'æ–‡ä»¶2': [ç»“æœ1ï¼Œ ç»“æœ2...]}
    """
    file_classification = {}
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        file_classification[you_say] = []
    for inputs, you_say in zip(gpt_response_collection[1::2], gpt_response_collection[0::2]):
        file_classification[you_say].append(inputs)
    return file_classification


def batch_recognition_images_to_md(img_list, ipaddr):
    """
    Args: å°†å›¾ç‰‡æ‰¹é‡è¯†åˆ«ç„¶åå†™å…¥mdæ–‡ä»¶
        img_list: å›¾ç‰‡åœ°å€list
        ipaddr: ç”¨æˆ·æ‰€å±æ ‡è¯†
    Returns: [æ–‡ä»¶list]
    """
    temp_list = []
    save_path = os.path.join(init_path.private_files_path, ipaddr, 'ocr_to_md')
    for img in img_list:
        if os.path.exists(img):
            img_content, img_result, _ = reader_fns.ImgHandler(img, save_path).get_paddle_ocr()
            temp_file = os.path.join(save_path,
                                     img_content.splitlines()[0][:20] + '.md')
            with open(temp_file, mode='w', encoding='utf-8') as f:
                f.write(f"{func_box.html_view_blank(temp_file)}\n\n" + img_content)
            temp_list.append(temp_list)
        else:
            print(img, 'æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨')
    return temp_list


def name_de_add_sort(response, index=0):
    if type(index) is not int:
        return response  # å¦‚æœä¸æ˜¯æ•°å­—ä¸‹æ ‡ï¼Œé‚£ä¹ˆä¸æ’åº
    try:
        unique_tuples = set(tuple(lst) for lst in response)
        de_result = [list(tpl) for tpl in unique_tuples]
        d = {}
        for i, v in enumerate(de_result):
            if len(v) >= index:
                if v[index] not in d:
                    d[v[index]] = i
            else:
                d[v[len(v)]] = i
        de_result.sort(key=lambda x: d[x[index]])
        return de_result
    except:
        from common.toolbox import trimmed_format_exc
        tb_str = '```\n' + trimmed_format_exc() + '```'
        print(tb_str)
        return response


def parsing_json_in_text(txt_data: list, old_case, filter_list: list = 'None----', tags='æ’ä»¶è¡¥å……çš„ç”¨ä¾‹', sort_index=0):
    response = []
    desc = '\n\n---\n\n'.join(txt_data)
    for index in range(len(old_case)):
        # è·å–æ‰€æœ‰Json
        supplementary_data = reader_fns.MdProcessor(txt_data[index]).json_to_list()
        # å…¼å®¹ä¸€ä¸‹å“ˆ
        if len(txt_data) != len(old_case): index = -1
        # è¿‡æ»¤æ‰äº§å‡ºå¸¦çš„è¡¨å¤´æ•°æ®
        filtered_supplementary_data = [data for data in supplementary_data
                                       if filter_list[:5] != data[:5] or filter_list[-5:] != data[-5:]]
        # æ£€æŸ¥ filtered_supplementary_data æ˜¯å¦ä¸ºç©º
        if not filtered_supplementary_data:
            max_length = 0  # æˆ–å…¶ä»–åˆé€‚çš„é»˜è®¤å€¼
        else:
            max_length = max(len(lst) for lst in filtered_supplementary_data)
        supplement_temp_data = [lst + [''] * (max_length - len(lst)) for lst in filtered_supplementary_data]
        for new_case in supplement_temp_data:
            if new_case not in old_case[index] and new_case + [tags] not in old_case[index]:
                old_case[index].append(new_case + [tags])
        response.extend(old_case[index])
    # æŒ‰ç…§åç§°æ’åˆ—é‡ç»„
    response = name_de_add_sort(response, sort_index)
    return response, desc


def result_extract_to_test_cases(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history):
    """
    Args:
        gpt_response_collection: [è¾“å…¥æ–‡ä»¶æ ‡é¢˜ï¼Œ è¾“å‡º]
        llm_kwargs: è°ƒä¼˜å‚æ•°
        plugin_kwargs: æ’ä»¶è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å¯¹è¯å†å²
        file_key: å­˜å…¥å†å²æ–‡ä»¶
    Returns: None
    """
    template_file, sheet, sort_index = json_args_return(plugin_kwargs,
                                                        ['å†™å…¥æŒ‡å®šæ¨¡ç‰ˆ', 'å†™å…¥æŒ‡å®šSheet', 'ç”¨ä¾‹ä¸‹æ ‡æ’åº'])
    file_classification = file_classification_to_dict(gpt_response_collection)
    chat_file_list = ''
    you_say = 'å‡†å¤‡å°†æµ‹è¯•ç”¨ä¾‹å†™å…¥Excelä¸­...'
    chatbot.append([you_say, chat_file_list])
    yield from toolbox.update_ui(chatbot, history)
    files_limit = {}
    for file_name in file_classification:
        # å¤„ç†mdæ•°æ®
        test_case = reader_fns.MdProcessor(file_classification[file_name]).tabs_to_list()
        sort_test_case = name_de_add_sort(test_case, sort_index)
        # æ­£å¼å‡†å¤‡å†™å…¥æ–‡ä»¶
        save_path = os.path.join(init_path.private_files_path, llm_kwargs['ipaddr'], 'test_case')
        xlsx_handler = reader_fns.XlsxHandler(template_file, output_dir=save_path, sheet=sheet)
        xlsx_handler.split_merged_cells()  # å…ˆæŠŠåˆå¹¶çš„å•å…ƒæ ¼æ‹†åˆ†ï¼Œé¿å…å†™å…¥å¤±è´¥
        file_path = xlsx_handler.list_write_to_excel(sort_test_case, save_as_name=long_name_processing(file_name))
        chat_file_list += f'{file_name}ç”Ÿæˆç»“æœå¦‚ä¸‹:\t {func_box.html_view_blank(__href=file_path, to_tabs=True)}\n\n'
        chatbot[-1] = ([you_say, chat_file_list])
        yield from toolbox.update_ui(chatbot, history)
        files_limit.update({file_path: file_name})
    return files_limit


def result_supplementary_to_test_case(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history):
    template_file, sheet, sort_index = json_args_return(plugin_kwargs,
                                                        ['å†™å…¥æŒ‡å®šæ¨¡ç‰ˆ', 'å†™å…¥æŒ‡å®šSheet', 'ç”¨ä¾‹ä¸‹æ ‡æ’åº'])
    if not sheet:
        sheet, = json_args_return(plugin_kwargs, ['è¯»å–æŒ‡å®šSheet'])
    file_classification = file_classification_to_dict(gpt_response_collection)
    chat_file_list = ''
    you_say = 'å‡†å¤‡å°†æµ‹è¯•ç”¨ä¾‹å†™å…¥Excelä¸­...'
    chatbot.append([you_say, chat_file_list])
    yield from toolbox.update_ui(chatbot, history)
    files_limit = {}
    for file_name in file_classification:
        old_file = plugin_kwargs['ä¸Šé˜¶æ®µæ–‡ä»¶']
        old_case = plugin_kwargs[old_file]['åŸæµ‹è¯•ç”¨ä¾‹æ•°æ®']
        header = plugin_kwargs[old_file]['åŸæµ‹è¯•ç”¨ä¾‹è¡¨å¤´']
        test_case, desc = parsing_json_in_text(file_classification[file_name], old_case, filter_list=header,
                                               sort_index=sort_index)
        save_path = os.path.join(init_path.private_files_path, llm_kwargs['ipaddr'], 'test_case')
        # å†™å…¥excel
        xlsx_handler = reader_fns.XlsxHandler(template_file, output_dir=save_path, sheet=sheet)
        file_path = xlsx_handler.list_write_to_excel(test_case, save_as_name=long_name_processing(file_name))
        # å†™å…¥ markdown
        md_path = os.path.join(save_path, f"{long_name_processing(file_name)}.md")
        reader_fns.MdHandler(md_path).save_markdown(desc)
        chat_file_list += f'{file_name}ç”Ÿæˆç»“æœå¦‚ä¸‹:\t {func_box.html_view_blank(__href=file_path, to_tabs=True)}\n\n' \
                          f'{file_name}è¡¥å……æ€è·¯å¦‚ä¸‹ï¼š\t{func_box.html_view_blank(__href=md_path, to_tabs=True)}\n\n---\n\n'
        chatbot[-1] = ([you_say, chat_file_list])
        yield from toolbox.update_ui(chatbot, history)
        files_limit.update({file_path: file_name})
    return files_limit


def result_converter_to_flow_chart(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history):
    """
    Args: å°†è¾“å‡ºç»“æœå†™å…¥mdï¼Œå¹¶è½¬æ¢ä¸ºæµç¨‹å›¾
        gpt_response_collection: [è¾“å…¥ã€è¾“å‡º]
        llm_kwargs: è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²å¯¹è¯
    Returns:
        None
    """
    file_classification = file_classification_to_dict(gpt_response_collection)
    file_limit = {}
    chat_file_list = ''
    you_say = 'è¯·å°†Markdownç»“æœè½¬æ¢ä¸ºæµç¨‹å›¾~'
    chatbot.append([you_say, chat_file_list])
    for file_name in file_classification:
        inputs_count = ''
        for value in file_classification[file_name]:
            inputs_count += str(value).replace('```', '')  # å»é™¤å¤´éƒ¨å’Œå°¾éƒ¨çš„ä»£ç å—, é¿å…æµç¨‹å›¾å †åœ¨ä¸€å—
        save_path = os.path.join(init_path.private_files_path, llm_kwargs['ipaddr'])
        md_file = os.path.join(save_path, f"{long_name_processing(file_name)}.md")
        html_file = reader_fns.MdHandler(md_path=md_file, output_dir=save_path).save_mark_map()
        chat_file_list += "View: " + func_box.html_view_blank(md_file, to_tabs=True) + \
                          '\n\n--- \n\n View: ' + func_box.html_view_blank(html_file)
        chatbot.append([you_say, chat_file_list])
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æˆåŠŸå†™å…¥æ–‡ä»¶ï¼')
        file_limit.update({md_file: file_name})
    # f'tips: åŒå‡»ç©ºç™½å¤„å¯ä»¥æ”¾å¤§ï½\n\n' f'{func_box.html_iframe_code(html_file=html)}'  æ— ç”¨ï¼Œä¸å…è®¸å†…åµŒç½‘é¡µäº†
    return file_limit


def result_written_to_markdown(gpt_response_collection, llm_kwargs, plugin_kwargs, chatbot, history, stage=''):
    """
    Args: å°†è¾“å‡ºç»“æœå†™å…¥md
        gpt_response_collection: [è¾“å…¥ã€è¾“å‡º]
        llm_kwargs: è°ƒä¼˜å‚æ•°
        chatbot: å¯¹è¯ç»„ä»¶
        history: å†å²å¯¹è¯
    Returns:
        None
    """
    file_classification = file_classification_to_dict(gpt_response_collection)
    file_limit = []
    chat_file_list = ''
    you_say = 'è¯·å°†Markdownç»“æœå†™å…¥æ–‡ä»¶ä¸­...'
    chatbot.append([you_say, chat_file_list])
    for file_name in file_classification:
        inputs_all = ''
        for value in file_classification[file_name]:
            inputs_all += value
        md = Utils().write_markdown(data=inputs_all, hosts=llm_kwargs['ipaddr'],
                                    file_name=long_name_processing(file_name) + stage)
        chat_file_list = f'markdownå·²å†™å…¥æ–‡ä»¶ï¼Œä¸‹æ¬¡ä½¿ç”¨æ’ä»¶å¯ä»¥ç›´æ¥æäº¤markdownæ–‡ä»¶å•¦ {func_box.html_view_blank(md, to_tabs=True)}'
        chatbot[-1] = [you_say, chat_file_list]
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æˆåŠŸå†™å…¥æ–‡ä»¶ï¼')
        file_limit.append(md)
    return file_limit


def detach_cloud_links(link_limit, chatbot, history, llm_kwargs, valid_types):
    fp_mapping = {}
    if isinstance(chatbot, list) and isinstance(history, list):
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æ­£åœ¨è§£æäº‘æ–‡ä»¶é“¾æ¥...')
    save_path = os.path.join(init_path.private_files_path, llm_kwargs['ipaddr'])
    wps_status, qq_status, feishu_status = '', '', ''
    try:
        # wpsäº‘æ–‡æ¡£ä¸‹è½½
        wps_links = func_box.split_domain_url(link_limit, domain_name=['kdocs', 'wps'])
        wps_status, wps_mapping = reader_fns.get_kdocs_from_limit(wps_links, save_path, llm_kwargs.get('wps_cookies'))
        fp_mapping.update(wps_mapping)
    except Exception as e:
        error = toolbox.trimmed_format_exc()
        wps_status += f'# ä¸‹è½½WPSæ–‡æ¡£å‡ºé”™äº† \n ERROR: {error} \n'
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='ä¸‹è½½WPSæ–‡æ¡£å‡ºé”™äº†')
    try:
        # qqäº‘æ–‡æ¡£ä¸‹è½½
        qq_link = func_box.split_domain_url(link_limit, domain_name=['docs.qq'])
        qq_status, qq_mapping = reader_fns.get_qqdocs_from_limit(qq_link, save_path, llm_kwargs.get('qq_cookies'))
        fp_mapping.update(qq_mapping)
    except Exception as e:
        error = toolbox.trimmed_format_exc()
        wps_status += f'# ä¸‹è½½QQæ–‡æ¡£å‡ºé”™äº† \n ERROR: {error}'
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='ä¸‹è½½QQæ–‡æ¡£å‡ºé”™äº†')
    try:
        # é£ä¹¦äº‘æ–‡æ¡£ä¸‹è½½
        feishu_link = func_box.split_domain_url(link_limit, domain_name=['lg0v2tirko'])
        feishu_status, feishu_mapping = reader_fns.get_feishu_from_limit(feishu_link, save_path,
                                                                         llm_kwargs.get('feishu_header'))
        fp_mapping.update(feishu_mapping)
    except Exception as e:
        error = toolbox.trimmed_format_exc()
        wps_status += f'# ä¸‹è½½é£ä¹¦æ–‡æ¡£å‡ºé”™äº† \n ERROR: {error}'
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='ä¸‹è½½é£ä¹¦æ–‡æ¡£å‡ºé”™äº†')
    download_status = ''
    if wps_status or qq_status or feishu_status:
        download_status = "\n".join([wps_status, qq_status, feishu_status]).strip('\n')
    # ç­›é€‰æ–‡ä»¶
    for fp in fp_mapping:
        if fp.split('.')[-1] not in valid_types:
            download_status += '\n\n' + f'è¿‡æ»¤æ‰äº†`{fp_mapping[fp]}`ï¼Œå› ä¸ºä¸æ˜¯æ’ä»¶èƒ½å¤Ÿæ¥æ”¶å¤„ç†çš„æ–‡ä»¶ç±»å‹`{valid_types}`'
            fp_mapping.pop(fp)  # è¿‡æ»¤ä¸èƒ½å¤„ç†çš„æ–‡ä»¶
    return fp_mapping, download_status


def content_img_vision_analyze(content: str, chatbot, history, llm_kwargs, plugin_kwargs):
    ocr_switch, = json_args_return(plugin_kwargs, ['å¼€å¯OCR'])
    cor_cache = llm_kwargs.get('cor_cache', False)
    img_mapping = func_box.extract_link_pf(content, func_box.valid_img_extensions)
    # å¦‚æœå¼€å¯äº†OCRï¼Œå¹¶ä¸”æ–‡ä¸­å­˜åœ¨å›¾ç‰‡é“¾æ¥ï¼Œå¤„ç†å›¾ç‰‡
    if ocr_switch and img_mapping:
        vision_bro = f"æ£€æµ‹åˆ°è¯†å›¾å¼€å…³ä¸º`{ocr_switch}`ï¼Œå¹¶ä¸”æ–‡ä¸­å­˜åœ¨å›¾ç‰‡é“¾æ¥ï¼Œæ­£åœ¨è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—...è§£æè¿›åº¦å¦‚ä¸‹ï¼š"
        vision_loading_statsu = {i: "Loading..." for i in img_mapping}
        vision_start = func_box.html_folded_code(json.dumps(vision_loading_statsu, indent=4, ensure_ascii=False))
        chatbot.append([None, vision_bro + vision_start])
        yield from toolbox.update_ui(chatbot, history, 'æ­£åœ¨è°ƒç”¨`Vision`ç»„ä»¶ï¼Œå·²å¯ç”¨å¤šçº¿ç¨‹è§£æï¼Œè¯·ç¨ç­‰')
        # è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
        save_path = os.path.join(init_path.private_files_path, llm_kwargs['ipaddr'])
        vision_submission = reader_fns.submit_threads_img_handle(img_mapping, save_path, cor_cache, ocr_switch)
        chatbot[-1] = [None, vision_bro]
        for t in vision_submission:
            try:
                img_content, img_path, status = vision_submission[t].result()
                vision_loading_statsu.update({t: img_content})
                vision_end = func_box.html_folded_code(json.dumps(vision_loading_statsu, indent=4, ensure_ascii=False))
                chatbot[-1] = [None, vision_bro + vision_end]
                if not status or status != 'æœ¬æ¬¡è¯†åˆ«ç»“æœè¯»å–æ•°æ®åº“ç¼“å­˜':  # å‡ºç°å¼‚å¸¸ï¼Œä¸æ›¿æ¢æ–‡æœ¬
                    content = content.replace(img_mapping[t], f'{img_mapping[t]}\n\n{img_content}')
                yield from toolbox.update_ui(chatbot, history)
            except Exception as e:
                status = f'`{t}` `{e}` è¯†åˆ«å¤±è´¥ï¼Œè¿‡æ»¤è¿™ä¸ªå›¾ç‰‡\n\n'
                vision_loading_statsu.update({t: status})
                vision_end = func_box.html_folded_code(json.dumps(vision_loading_statsu, indent=4, ensure_ascii=False))
                chatbot[-1] = [None, vision_bro + vision_end]
                yield from toolbox.update_ui(chatbot, history)
    return content.replace(init_path.base_path, '.')  # å¢åŠ ä¿éšœï¼Œé˜²æ­¢è·¯å¾„æ³„éœ²


def content_clear_links(user_input, clear_fp_map, clear_link_map):
    """æ¸…é™¤æ–‡æœ¬ä¸­å·²å¤„ç†çš„é“¾æ¥"""
    for link in clear_link_map:
        user_input = user_input.replace(link, '')
    for pf in clear_fp_map:
        user_input = user_input.replace(clear_fp_map[pf], '')
    return user_input


def user_input_embedding_content(user_input, chatbot, history, llm_kwargs, plugin_kwargs, valid_types):
    embedding_content = []  # å¯¹è¯å†…å®¹
    chatbot.append([user_input, 'ğŸ•µğŸ»â€è¶…çº§ä¾¦æ¢ï¼Œæ­£åœ¨åŠæ¡ˆï½'])
    yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='ğŸ•µğŸ»â€è¶…çº§ä¾¦æ¢ï¼Œæ­£åœ¨åŠæ¡ˆï½')
    # äº‘æ–‡ä»¶
    fp_mapping, download_status = yield from detach_cloud_links(user_input, chatbot, history, llm_kwargs, valid_types)
    if download_status:
        chatbot[-1][1] = f'\n\nä¸‹è½½äº‘æ–‡æ¡£ä¼¼ä¹å‡ºäº†ç‚¹é—®é¢˜ï¼Ÿ\n\n```python\n{download_status}\n```\n\n'
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='ğŸ•µğŸ» â€å‡ºå¸ˆæœªæ·èº«å…ˆæ­»ğŸ´â€â˜ ï¸')
    # æœ¬åœ°æ–‡ä»¶
    fp_mapping.update(func_box.extract_link_pf(user_input, valid_types))
    content_mapping = yield from file_extraction_intype(fp_mapping, chatbot, history, llm_kwargs, plugin_kwargs)
    if content_mapping:
        mapping_data = func_box.html_folded_code(json.dumps(content_mapping, indent=4, ensure_ascii=False))
        map_bro_say = f'\n\næ•°æ®è§£æå®Œæˆï¼Œæå–`fp mapping`å¦‚ä¸‹ï¼š\n\n{mapping_data}'
        chatbot[-1][1] += map_bro_say
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æ•°æ®è§£æå®Œæˆï¼')
        for content_fp in content_mapping:  # ä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªå¯¹è¯
            file_content = content_mapping[content_fp]
            # å°†è§£æçš„æ•°æ®æäº¤åˆ°æ­£æ–‡
            input_handle = user_input.replace(fp_mapping[content_fp], file_content)
            # å°†å…¶ä»–æ–‡ä»¶é“¾æ¥æ¸…é™¤
            user_clear = content_clear_links(input_handle, fp_mapping, content_mapping)
            # è¯†åˆ«å›¾ç‰‡é“¾æ¥å†…å®¹
            complete_input = yield from content_img_vision_analyze(user_clear, chatbot, history,
                                                                   llm_kwargs, plugin_kwargs)
            embedding_content.extend([os.path.basename(content_fp), complete_input])

    elif len(user_input) > 100:  # æ²¡æœ‰æ¢æµ‹åˆ°ä»»ä½•æ–‡ä»¶ï¼Œå¹¶ä¸”æäº¤å¤§äº50ä¸ªå­—ç¬¦ï¼Œé‚£ä¹ˆè¿è¡Œå¾€ä¸‹èµ°
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æ²¡æœ‰æ¢æµ‹åˆ°æ–‡ä»¶')
        # è¯†åˆ«å›¾ç‰‡é“¾æ¥å†…å®¹
        complete_input = yield from content_img_vision_analyze(user_input, chatbot, history,
                                                               llm_kwargs, plugin_kwargs)
        embedding_content = [user_input, complete_input]
        embedding_content.extend([user_input, user_input])
    else:
        devs_document = toolbox.get_conf('devs_document')
        status = '\n\næ²¡æœ‰æ¢æµ‹åˆ°ä»»ä½•æ–‡ä»¶ï¼Œå¹¶ä¸”æäº¤å­—ç¬¦å°‘äº50ï¼Œæ— æ³•å®Œæˆåç»­ä»»åŠ¡' \
                 f'è¯·åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥éœ€è¦è§£æçš„äº‘æ–‡æ¡£é“¾æ¥æˆ–æœ¬åœ°æ–‡ä»¶åœ°å€ï¼Œå¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£åˆ™ç”¨æ¢è¡Œæˆ–ç©ºæ ¼éš”å¼€ï¼Œç„¶åå†ç‚¹å‡»å¯¹åº”çš„æ’ä»¶\n\n' \
                 f'æ’ä»¶æ”¯æŒè§£ææ–‡æ¡£ç±»å‹`{valid_types}`' \
                 f"æœ‰é—®é¢˜ï¼Ÿè¯·è”ç³»`@spike` or æŸ¥çœ‹å¼€å‘æ–‡æ¡£{devs_document}"
        if chatbot[-1][1] is None:
            chatbot[-1][1] = status
        chatbot[-1][1] += status
        yield from toolbox.update_ui(chatbot=chatbot, history=history, msg='æ²¡æœ‰æ¢æµ‹åˆ°æ•°æ®')
    # æäº¤çŸ¥è¯†åº“ ... æœªé€‚é…
    return embedding_content


def audio_extraction_text(file):
    import speech_recognition as sr
    # æ‰“å¼€éŸ³é¢‘æ–‡ä»¶
    r = sr.Recognizer()
    with sr.AudioFile(file) as source:
        # è¯»å–éŸ³é¢‘æ–‡ä»¶çš„å†…å®¹
        audio_content = r.record(source)
        # ä½¿ç”¨Googleçš„æ–‡å­—è½¬è¯æœåŠ¡å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡å­—
        text = r.recognize_google(audio_content, language='zh-CN')
    return text


def audio_comparison_of_video_converters(files, chatbot, history):
    temp_chat = ''
    chatbot.append(['å¯ä»¥å¼€å§‹äº†ä¹ˆ', temp_chat])
    temp_list = []
    for file in files:
        temp_chat += f'æ­£åœ¨å°†{func_box.html_view_blank(file)}æ–‡ä»¶è½¬æ¢ä¸ºå¯æå–çš„éŸ³é¢‘æ–‡ä»¶.\n\n'
        chatbot[-1] = ['å¯ä»¥å¼€å§‹äº†ä¹ˆ', temp_chat]
        yield from toolbox.update_ui(chatbot=chatbot, history=history)
        temp_path = os.path.join(os.path.dirname(file), f"{os.path.basename(file)}.wav")
        videoclip = AudioFileClip(file)
        videoclip.write_audiofile(temp_path)
        temp_list.extend((temp_path, audio_extraction_text(temp_path)))
    return temp_list


# <---------------------------------------ä¸€äº›Tips----------------------------------------->
previously_on_plugins = f'å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»ã€ğŸ”—ã€‘å…ˆä¸Šä¼ ï¼Œå¤šä¸ªæ–‡ä»¶è¯·ä¸Šä¼ å‹ç¼©åŒ…ï¼Œ' \
                        f'{func_box.html_tag_color("å¦‚æœæ˜¯ç½‘ç»œæ–‡ä»¶æˆ–é‡‘å±±æ–‡æ¡£é“¾æ¥ï¼Œè¯·ç²˜è´´åˆ°è¾“å…¥æ¡†")}, ç„¶åå†æ¬¡ç‚¹å‡»è¯¥æ’ä»¶' \
                        f'å¤šä¸ªæ–‡ä»¶{func_box.html_tag_color("è¯·ä½¿ç”¨æ¢è¡Œæˆ–ç©ºæ ¼åŒºåˆ†")}'

if __name__ == '__main__':
    test = [1, 2, 3, 4, [12], 33, 1]

    print(long_name_processing('ã€æ”¯ä»˜ç³»ç»Ÿã€‘æ”¯ä»˜é€šé“ä½™é¢æ–°å»ºè¡¨æ›´æ–°ä¿å­˜.docx'))
