#! .\venv\
# encoding: utf-8
# @Time   : 2023/7/29
# @Author : Spike
# @Descr   :
import os
import re
import time
import json
import requests
import urllib.parse

from bs4 import BeautifulSoup
from comm_tools import toolbox, func_box
from crazy_functions.kingsoft_fns import crazy_box
from crazy_functions import crazy_utils
from comm_tools import ocr_tools

class Kdocs:

    def __init__(self, url):
        WPS_COOKIES, = toolbox.get_conf('WPS_COOKIES',)
        self.url = url
        self.cookies = WPS_COOKIES
        self.headers = {
            'accept-language': 'en-US,en;q=0.9,ja;q=0.8',
            'content-type': 'text/plain;charset=UTF-8',
            'x-csrf-rand': '',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'}
        self.ex_headers = {
            'Host': 'www.kdocs.cn',
            'accept': 'application/json, text/plain, */*',
            'content-type': 'application/json',
            'sec-ch-ua-platform': '"macOS"',
            'origin': 'https://www.kdocs.cn',
        }
        self.dzip_header = {
            'Host': 'kdzip-download.kdocs.cn',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.82',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',

            }
        self.parm_otl_data = {"connid": "",
                              "args": {"password": "", "readonly": False, "modifyPassword": "", "sync": True,
                                       "startVersion": 0, "endVersion": 0},
                              "ex_args": {"queryInitArgs": {"enableCopyComments": False, "checkAuditRule": False}},
                              "group": "", "front_ver": ""}
        self.parm_shapes_data = {"objects": [], "expire": 86400000, "support_webp": True, "with_thumbnail": True,
                                 "support_lossless": True}
        self.parm_export_preload = {"ver": "56"}
        self.parm_bulk_download = {'file_ids': [], 'csrfmiddlewaretoken': self.cookies['csrf']}
        self.params_task = {'task_id': ''}
        self.params_continue = {"task_id": "", "download_as": [
                            {"suffix": ".otl", "as": ".pdf"},
                            {"suffix": ".ksheet", "as": ".xlsx"},
                            {"suffix": ".pof", "as": ".png"},
                            {"suffix": ".pom", "as": ".png"}]}
        self.tol_url = 'https://www.kdocs.cn/api/v3/office/file/%v/open/otl'
        self.shapes_url = 'https://www.kdocs.cn/api/v3/office/file/%v/attachment/shapes'
        self.kdocs_download_url = 'https://drive.kdocs.cn/api/v5/groups/%g/files/%f/download?isblocks=false&support_checksums=md5,sha1,sha224,sha256,sha384,sha512'
        self.drive_download_url = 'https://drive.wps.cn/api/v3/groups/%g/files/%f/download?isblocks=false'
        self.group_url = 'https://drive.wps.cn/api/v5/links/%v?review=true'
        self.export_url = 'https://www.kdocs.cn/api/v3/office/file/%f/export/%t/result'
        self.preload_url = 'https://www.kdocs.cn/api/v3/office/file/%f/export/%t/preload'
        self.bulk_download_url = 'https://www.kdocs.cn/kfc/batch/v2/files/download'
        self.bulk_continue_url = 'https://www.kdocs.cn/kfc/batch/v2/files/download/continue'
        self.task_result_url = 'https://www.kdocs.cn/kfc/batch/v2/files/download/progress'
        self.url_share_tag = ''
        self.url_dirs_tag = ''
        self.split_link_tags()
        if self.url_share_tag:
            self.file_info_parm = self.get_file_info_parm()
        self.docs_old_type = ['.docs', '.doc', '.pptx', '.ppt', '.xls', '.xlsx', '.pdf', '.csv', '.txt', '.pom', '.pof', '.xmind']
        self.to_img_type = {'.pom': '.png', '.pof': '.png'}
        self.media_type = ['.mp4', '.m4a', '.wav', '.mpga', '.mpeg', '.mp3', '.avi', '.mkv', '.flac', '.aac']
        self.smart_type = {'.otl': 'pdf', '.ksheet': 'xlsx'}

    def get_file_info_html(self):
        """
        è·å–ä¼ é€’è¿‡æ¥çš„æ–‡æ¡£HTMLä¿¡æ¯
        Returns:
            HTMLä¿¡æ¯
        """
        response = requests.get(self.url, cookies=self.cookies, headers=self.headers, verify=False)
        return response.text

    def get_file_info_parm(self):
        # è·å–åˆ†äº«æ–‡ä»¶infoä¿¡æ¯
        response = requests.get(self.group_url.replace("%v", self.url_share_tag),
                                cookies=self.cookies,
                                headers=self.headers, verify=False).json()
        try:
            file_info = response['fileinfo']
        except KeyError:
            file_info = {}
        return file_info

    def submit_batch_download_tasks(self):
        # æäº¤ç›®å½•è½¬æ¢ä»»åŠ¡
        self.parm_bulk_download.update({'file_ids': [self.url_dirs_tag]})
        dw_response = requests.post(self.bulk_download_url, cookies=self.cookies, headers=self.ex_headers,
                                 json=self.parm_bulk_download, verify=False).json()
        if dw_response.get('data', False):
            task_id = dw_response['data']['task_id']
            task_info = dw_response['data'].get('online_file'), dw_response['data'].get('online_fnum')
        else:
            print(dw_response['result'])
            task_id = None
            task_info = None
        if task_id:
            self.params_continue.update({'task_id': task_id})
            requests.post(self.bulk_continue_url, cookies=self.cookies, headers=self.ex_headers,
                          json=self.params_continue, verify=False).json()
        return task_id, task_info

    def polling_batch_download_tasks(self, task_id):
        # è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼Œæå–ä¸‹è½½é“¾æ¥
        self.params_task.update({'task_id': task_id})
        link = ''
        faillist = ''
        if task_id:
            for i in range(600):
                response = requests.get(url=self.task_result_url,
                                        params=self.params_task,
                                        cookies=self.cookies,
                                        headers=self.ex_headers, verify=False).json()
                if response['data'].get('url', False):
                    link = response['data'].get('url', '')
                    faillist = str(response['data'].get('faillist', ''))
                    break
                time.sleep(3)
        return link, faillist

    def wps_file_download(self, url):
        # éœ€è¦wpscookieæ–‡ä»¶ä¸‹è½½
        response = requests.get(url=url, cookies=self.cookies, headers=self.dzip_header, verify=False)
        return response

    def document_aggregation_download(self, file_type=''):
        #
        link_name = self.file_info_parm['fname']
        for t in self.to_img_type:
            if t in link_name:
                link_name = link_name+self.to_img_type[t]
        link = ''
        for t in self.docs_old_type:
            if t in link_name and file_type in link_name:
                link = self.get_docs_old_link()
        for t in self.media_type:
            if t in link_name and file_type in link_name:
                link = self.get_media_link()
        for t in self.smart_type:
            if file_type == self.smart_type[t]:
                file_type = t
            if t in link_name and file_type in link_name:
                link = self.get_kdocs_intelligence_link(type=self.smart_type[t])
                link_name = link_name+f".{self.smart_type[t]}"
        return link, link_name

    def get_media_link(self):
        # åª’ä½“æ–‡ä»¶ä¸‹è½½
        response = requests.get(self.drive_download_url.replace("%g", str(self.file_info_parm['groupid'])
                                                                ).replace('%f', str(self.file_info_parm['id'])),
                                cookies=self.cookies,
                                headers=self.headers, verify=False)
        link = response.json()['fileinfo']['url']
        return self.url_decode(link)

    def get_docs_old_link(self):
        # pptã€docã€pdfã€xlsä¸‹è½½
        response = requests.get(self.kdocs_download_url.replace("%g", str(self.file_info_parm['groupid'])
                                                                ).replace('%f', str(self.file_info_parm['id'])),
                                cookies=self.cookies,
                                headers=self.headers, verify=False)
        try:
            link = response.json()['fileinfo']['url']
        except:
            link = response.json()['url']
        return self.url_decode(link)

    def get_kdocs_intelligence_link(self, type='xlsx'):
        # æ™ºèƒ½æ–‡æ¡£ä¸‹è½½
        response_task = requests.post(
            self.preload_url.replace('%f', str(self.file_info_parm['id'])).replace('%t', type),
            cookies=self.cookies,
            headers=self.ex_headers,
            json=self.parm_export_preload, verify=False
        )
        self.parm_export_preload.update(response_task.json())
        for i in range(20):
            response_link = requests.post(
                self.export_url.replace('%f', str(self.file_info_parm['id'])).replace('%t', type),
                cookies=self.cookies,
                headers=self.ex_headers,
                json=self.parm_export_preload, verify=False
            )
            if response_link.json()['status'] == 'finished':
                return response_link.json()['data']['url']
        return None

    def split_link_tags(self):
        # æå–tagï¼Œç»™åç»­è¯·æ±‚è¯•ç”¨
        url_parts = re.split('[/\?&#]+', self.url)
        try:
            try:
                l_index = url_parts.index('l')
                otl_url_str = url_parts[l_index + 1]
                self.url_share_tag = otl_url_str
            except ValueError:
                l_index = url_parts.index('ent')
                otl_url_str = url_parts[-1]
                self.url_dirs_tag = otl_url_str
        except ValueError:
            print('æ—¢ä¸æ˜¯åœ¨çº¿æ–‡æ¡£ï¼Œä¹Ÿä¸æ˜¯æ–‡æ¡£ç›®å½•')
            return ''

    def get_file_content(self):
        """
        çˆ¬è™«è§£ææ–‡æ¡£å†…å®¹
        Returns:
            æ–‡æ¡£å†…å®¹
        """
        otl_url_str = self.url_share_tag
        if otl_url_str is None: return
        html_content = self.get_file_info_html()
        self.bs4_file_info(html_content)  # è°ƒç”¨ bs4_file_info() æ–¹æ³•è§£æ html_contentï¼Œè·å–æ–‡ä»¶ä¿¡æ¯# æ›´æ–°ç±»çš„parm_data å’Œ headers
        json_data = json.dumps(self.parm_otl_data)
        response = requests.post(
            str(self.tol_url).replace('%v', otl_url_str),
            cookies=self.cookies,
            headers=self.headers,
            data=json_data, verify=False)
        return response.json(), response.text

    def get_file_pic_url(self, pic_dict: dict):
        otl_url_str = self.url_share_tag
        if otl_url_str is None: return
        for pic in pic_dict:
            pic_parm = {'attachment_id': pic, "imgId": pic_dict[pic], "max_edge": 1180, "source": ""}
            self.parm_shapes_data['objects'].append(pic_parm)
        json_data = json.dumps(self.parm_shapes_data)
        response = requests.post(
            str(self.shapes_url).replace('%v', otl_url_str),
            cookies=self.cookies,
            headers=self.headers,
            data=json_data, verify=False)
        url_data = response.json()['data']
        for pic in url_data:
            try:
                pic_dict[pic] = self.url_decode(url_data[pic]['url'])
            except Exception as f:
                pass
        return pic_dict

    @staticmethod
    def url_decode(url):
        decoded_url = urllib.parse.unquote(url)
        return decoded_url


    def bs4_file_info(self, html_str):
        """
        bs4çˆ¬è™«æ–‡æ¡£ä¿¡æ¯ï¼Œæ²¡æœ‰è¿™ä¸ªå¯ä¸è¡ŒğŸ¤¨
        Args:
            html_str: HTMLä¿¡æ¯
        Returns:
            {'connid': æ–‡æ¡£id, 'group': æ–‡æ¡£çš„ç¾¤ç»„, 'front_ver': æ–‡æ¡£ç‰ˆæœ¬}
        """
        html = BeautifulSoup(html_str, "html.parser")
        # Find all script tags in the HTML
        script_tags = html.find_all("script")
        json_string = None
        # Iterate through script tags to find the one containing required data
        for tag in script_tags:
            if tag.string and "window.__WPSENV__" in tag.string:
                json_string = re.search(r"window\.__WPSENV__=(.*);", tag.string).group(1)
                break
        if json_string:
            # Load the JSON data from the found string
            json_data = json.loads(json_string)
            file_connid = json_data['conn_id']
            file_group = json_data['user_group']
            file_front_ver = json_data['file_version']
            file_id = json_data['root_file_id']
            group_id = json_data['file_info']['file']['group_id']
            self.headers['x-csrf-rand'] = json_data['csrf_token']
            self.parm_otl_data.update({'connid': file_connid, 'group': file_group, 'front_ver': file_front_ver,
                                       'file_id': file_id, 'group_id':group_id})
            return True
        else:
            return None


def if_kdocs_url_isap(url):
    kdocs = Kdocs(url)
    if 'otl' in kdocs.file_info_parm['fname']:
        return True
    return False


def get_docs_content(url, image_processing=False):
    """
    Args: çˆ¬è™«ç¨‹åºï¼Œé€šè¿‡æ‹¿åˆ°åˆ†äº«é“¾æ¥æå–æ–‡æ¡£å†…ä¿¡æ¯
        url: æ–‡æ¡£url
        image_processing: æ˜¯å¦å¼€å§‹OCR
    Returns:
    """
    kdocs = Kdocs(url)
    utils = crazy_box.Utils()
    json_data, json_dict = kdocs.get_file_content()
    text_values = utils.find_all_text_keys(json_data, filter_type='')
    _all, content, pic_dict, file_dict = utils.statistical_results(text_values, img_proce=image_processing)
    pic_dict_convert = kdocs.get_file_pic_url(pic_dict)
    empty_picture_count = sum(1 for item in _all if 'picture' in item and not item['picture']['caption'])
    return _all, content, empty_picture_count, pic_dict_convert, file_dict


def get_kdocs_dir(limit, project_folder, type, ipaddr):
    """
    Args:
        limit: æ–‡æ¡£ç›®å½•è·¯å¾„
        project_folder: å†™å…¥çš„æ–‡ä»¶
        type: æ–‡ä»¶ç±»å‹, ä¸è¿‡è¿™é‡Œæ²¡ç”¨åˆ°
        ipaddr:  æ–‡ä»¶æ‰€å±æ ‡è¯†
    Returns: [æ–‡ä»¶åˆ—è¡¨], ç›®å½•å†…æ–‡ä»¶ä¿¡æ¯, å¤±è´¥ä¿¡æ¯
    """
    kdocs = Kdocs(limit)
    task_id, task_info = kdocs.submit_batch_download_tasks()
    link, task_faillist = kdocs.polling_batch_download_tasks(task_id)
    resp = kdocs.wps_file_download(link)
    content = resp.content
    temp_file = os.path.join(project_folder, kdocs.url_dirs_tag + '.zip')
    with open(temp_file, 'wb') as f: f.write(content)
    decompress_directory = os.path.join(project_folder, 'extract', kdocs.url_dirs_tag)
    toolbox.extract_archive(temp_file, decompress_directory)
    file_list = []
    img_list = []
    for f_t in kdocs.docs_old_type:
        _, file_, _ = crazy_utils.get_files_from_everything(decompress_directory, type=f_t, ipaddr=ipaddr)
        file_list += file_
    for i_t in crazy_box.Utils().picture_format:
        _, file_, _ = crazy_utils.get_files_from_everything(decompress_directory, type=i_t, ipaddr=ipaddr)
        file_list += file_
    file_list += crazy_box.batch_recognition_images_to_md(img_list, ipaddr)
    return file_list, task_info, task_faillist


def get_kdocs_files(limit, project_folder, type, ipaddr):
    """
    Args:
        limit: é‡‘å±±æ–‡æ¡£åˆ†äº«æ–‡ä»¶åœ°å€
        project_folder: å­˜å‚¨åœ°å€
        type: æŒ‡å®šçš„æ–‡ä»¶ç±»å‹
        ipaddr: ç”¨æˆ·ä¿¡æ¯
    Returns: [æå–çš„æ–‡ä»¶list]
    """
    if type == 'otl':
        _, content, _, pic_dict, _ = get_docs_content(limit)
        name = 'temp.md'
        tag = content.splitlines()[0][:20]
        for i in pic_dict:  # å¢åŠ OCRé€‰é¡¹
            img_content, img_result, _ = ocr_tools.Paddle_ocr_select(ipaddr=ipaddr, trust_value=True
                                                                  ).img_def_content(img_path=pic_dict[i], img_tag=i)
            content = str(content).replace(f"{i}", f"{func_box.html_local_img(img_result)}\n```{img_content}```")
            name = tag + '.md'
            content = content.encode('utf-8')
    elif type or type == '':
        kdocs = Kdocs(limit)
        link, name = kdocs.document_aggregation_download(file_type=type)
        tag = kdocs.url_share_tag
        if link:
            resp = requests.get(url=link, verify=False)
            content = resp.content
        else:
            return []
    else:
        return []
    if content:
        tag_path = os.path.join(project_folder, tag)
        temp_file = os.path.join(os.path.join(project_folder, tag, name))
        os.makedirs(tag_path, exist_ok=True)
        with open(temp_file, 'wb') as f: f.write(content)
        return [temp_file]


def get_kdocs_from_everything(txt, type='', ipaddr='temp'):
    """
    Args:
        txt: kudos æ–‡ä»¶åˆ†äº«ç 
        type: type=='' æ—¶ï¼Œå°†æ”¯æŒæ‰€æœ‰æ–‡ä»¶ç±»å‹
        ipaddr: ç”¨æˆ·ä¿¡æ¯
    Returns:
    """
    link_limit = crazy_box.Utils().split_startswith_txt(link_limit=txt)
    file_manifest = []
    success = ''
    project_folder = os.path.join(func_box.users_path, ipaddr, 'kdocs')
    os.makedirs(project_folder, exist_ok=True)
    if link_limit:
        for limit in link_limit:
            if '/ent/' in limit:
                file_list, info, fail = get_kdocs_dir(limit, project_folder, type, ipaddr)
                file_manifest += file_list
                success += f"{limit}æ–‡ä»¶ä¿¡æ¯å¦‚ä¸‹ï¼š{info}\n\n ä¸‹è½½ä»»åŠ¡çŠ¶å†µï¼š{fail}\n\n"
            else:
                file_manifest += get_kdocs_files(limit, project_folder, type, ipaddr)
    return success, file_manifest, project_folder


def smart_document_extraction(url, llm_kwargs, plugin_kwargs, chatbot, history, files):
    img_ocr, = crazy_box.json_args_return(plugin_kwargs, ['å¼€å¯OCR'])
    ovs_data, content, empty_picture_count, pic_dict, kdocs_dict = get_docs_content(url, image_processing=img_ocr)
    if img_ocr:
        you_say = 'è¯·æ£€æŸ¥æ•°æ®ï¼Œå¹¶è¿›è¡Œå¤„ç†'
        if pic_dict:  # å½“æœ‰å›¾ç‰‡æ–‡ä»¶æ—¶ï¼Œå†å»æé†’
            title = crazy_box.long_name_processing(content)
            ocr_process = f'æ£€æµ‹åˆ°`{title}`æ–‡æ¡£ä¸­å­˜åœ¨{func_box.html_tag_color(empty_picture_count)}å¼ å›¾ç‰‡ï¼Œä¸ºäº†äº§å‡ºç»“æœä¸å­˜åœ¨é—æ¼ï¼Œæ­£åœ¨é€ä¸€è¿›è¡Œè¯†åˆ«\n\n' \
                          f'> çº¢æ¡†ä¸ºé‡‡ç”¨çš„æ–‡æ¡ˆ,å¯ä¿¡æŒ‡æ•°ä½äº {func_box.html_tag_color(llm_kwargs["ocr"])} å°†ä¸é‡‡ç”¨, å¯åœ¨Setting ä¸­è¿›è¡Œé…ç½®\n\n'
            chatbot.append([you_say, ocr_process])
        else:
            ocr_process = ''
        if pic_dict:
            yield from toolbox.update_ui(chatbot, history, 'æ­£åœ¨è°ƒç”¨OCRç»„ä»¶ï¼Œå·²å¯ç”¨å¤šçº¿ç¨‹è§£æï¼Œè¯·ç¨ç­‰')
            ocr_func = ocr_tools.Paddle_ocr_select(ipaddr=llm_kwargs['ipaddr'],
                                                   trust_value=llm_kwargs['ocr']).identify_cache
            thread_submission = ocr_tools.submit_threads_ocr(pic_dict, func=ocr_func,
                                                             max_threads=llm_kwargs.get('worker_num', 5))
            for t in thread_submission:
                try:
                    img_content, img_result, error = thread_submission[t].result()
                    content = str(content).replace(f"{t}",
                                                   f"{func_box.html_local_img(img_result)}\n```{img_content}```")
                    if error:
                        ocr_process += f'`tips: {error}`'
                    ocr_process += f'{t} è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«æ•ˆæœå¦‚ä¸‹{func_box.html_local_img(img_result)}\n\n'
                    chatbot[-1] = [you_say, ocr_process]
                    yield from toolbox.update_ui(chatbot, history)
                except Exception:
                    ocr_process += f'{t} è¯†åˆ«å¤±è´¥ï¼Œè¿‡æ»¤è¿™ä¸ªå›¾ç‰‡\n\n'
                    chatbot[-1] = [you_say, ocr_process]
                    yield from toolbox.update_ui(chatbot, history)

    else:
        if empty_picture_count >= 5:
            chatbot.append(['è¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹', f'\n\n éœ€æ±‚æ–‡æ¡£ä¸­æ²¡æœ‰{func_box.html_tag_color("æè¿°")}çš„å›¾ç‰‡æ•°é‡' \
                                              f'æœ‰{func_box.html_tag_color(empty_picture_count)}å¼ ï¼Œç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹å¯èƒ½å­˜åœ¨é—æ¼ç‚¹ï¼Œ'
                                              f'å¯ä»¥å‚è€ƒä»¥ä¸‹æ–¹æ³•å¯¹å›¾ç‰‡è¿›è¡Œæè¿°è¡¥å……ï¼Œæˆ–åœ¨è‡ªå®šä¹‰æ’ä»¶å‚æ•°ä¸­å¼€å§‹OCRåŠŸèƒ½\n\n' \
                                              f'{func_box.html_local_img("docs/imgs/pic_desc.png")}'])
        yield from toolbox.update_ui(chatbot, history)
    title = crazy_box.long_name_processing(content)
    temp_list = [title, content]
    temp_file = yield from crazy_box.result_written_to_markdwon(temp_list, llm_kwargs, plugin_kwargs, chatbot, history)
    files.extend(temp_file)
