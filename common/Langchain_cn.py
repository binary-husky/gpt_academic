import os.path
import threading

from common import toolbox
from crazy_functions import crazy_utils
import gradio as gr
from common import func_box, database_processor
from crazy_functions.kingsoft_fns import crazy_box, docs_kingsoft, docs_qqdocs

from crazy_functions.vector_fns.vector_database import LocalDocQA
from crazy_functions.vector_fns import vector_database


class knowledge_archive_interface():
    def __init__(self, vs_path) -> None:
        self.current_id = ""
        self.kai_path = None
        import nltk
        if vector_database.NLTK_DATA_PATH not in nltk.data.path:
            nltk.data.path = [vector_database.NLTK_DATA_PATH] + nltk.data.path
        self.qa_handle = LocalDocQA()
        self.qa_handle.init_cfg()
        self.text2vec_large_chinese = None
        self.vs_root_path = vs_path
        self.ds_docstore = ''

    def get_chinese_text2vec(self):
        if self.text2vec_large_chinese is None:
            # < -------------------é¢„çƒ­æ–‡æœ¬å‘é‡åŒ–æ¨¡ç»„--------------- >
            from common.toolbox import ProxyNetworkActivate
            print('Checking Text2vec ...')
            from langchain.embeddings import HuggingFaceEmbeddings
            with ProxyNetworkActivate('Download_LLM'):  # ä¸´æ—¶åœ°æ¿€æ´»ä»£ç†ç½‘ç»œ
                self.text2vec_large_chinese = HuggingFaceEmbeddings(model_name="GanymedeNil/text2vec-large-chinese")
        return self.text2vec_large_chinese

    def filter_quarterly_files(self, files):
        database_files = list(self.get_loaded_file(files))

    def construct_vector_store(self, vs_id, files):
        for file in files:
            assert os.path.exists(file), "è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨"
        vs_path = os.path.join(self.vs_root_path, vs_id)
        vs_path, loaded_files = self.qa_handle.init_knowledge_vector_store(filepath=files, vs_path=vs_path,
                                                                           sentence_size=100,
                                                                           text2vec=self.get_chinese_text2vec())
        return self, vs_path

    def get_current_archive_id(self):
        return self.current_id

    def get_loaded_file(self, files):
        return self.qa_handle.get_loaded_file(files)

    def get_init_file(self, vs_id):
        from langchain.vectorstores import FAISS
        vs_path = os.path.join(self.vs_root_path, vs_id)
        self.qa_handle.vector_store = FAISS.load_local(vs_path, self.get_chinese_text2vec())
        ds = self.qa_handle.vector_store.docstore
        self.ds_docstore = ds
        file_dict = {ds._dict[k].metadata['source']: {vs_id: ds._dict[k].metadata['filetype']} for k in ds._dict}
        return self, file_dict

    def answer_with_archive_by_id(self, txt, vs_id, llm_kwargs=None, VECTOR_SEARCH_SCORE_THRESHOLD=0,
                                  VECTOR_SEARCH_TOP_K=4, CHUNK_SIZE=521):
        if llm_kwargs:
            vector_config = llm_kwargs.get('vector')
            VECTOR_SEARCH_SCORE_THRESHOLD = vector_config['score']
            VECTOR_SEARCH_TOP_K = vector_config['top-k']
            CHUNK_SIZE = vector_config['size']
        self.kai_path = os.path.join(self.vs_root_path, vs_id)
        if not os.path.exists(self.kai_path):
            return '', '', False
        resp, prompt = self.qa_handle.get_knowledge_based_conent_test(
            query=txt,
            vs_path=self.kai_path,
            score_threshold=VECTOR_SEARCH_SCORE_THRESHOLD,
            vector_search_top_k=VECTOR_SEARCH_TOP_K,
            chunk_conent=True,
            chunk_size=CHUNK_SIZE,
            text2vec=self.get_chinese_text2vec(),
        )
        return resp, prompt, True


def classification_filtering_tag(cls_select, ipaddr):
    if cls_select == 'ä¸ªäººçŸ¥è¯†åº“':
        cls_select = os.path.join(cls_select, ipaddr)
    return cls_select


def knowledge_base_writing(cls_select, links: str, select, name, kai_handle, ipaddr: gr.Request):
    # < --------------------è¯»å–å‚æ•°--------------- >
    user_addr = func_box.user_client_mark(ipaddr)
    cls_select = classification_filtering_tag(cls_select, user_addr)
    if not cls_select:
        raise gr.Error('æ–°å»ºåˆ†ç±»åç§°è¯·ä¸è¦ä¸ºç©º')
    vector_path = os.path.join(func_box.knowledge_path, cls_select)
    if name and select:
        kai_id = name
        os.rename(os.path.join(vector_path, select), os.path.join(vector_path, name))
        _, load_file = func_box.get_directory_list(vector_path, user_addr)
        yield ('', f'æ›´åæˆåŠŸï½ `{select}` -> `{name}`',
               gr.update(), gr.update(choices=load_file,
                                      value=kai_id), gr.update(), kai_handle)
        if not links and not kai_handle.get('file_list'): return  # å¦‚æœæ–‡ä»¶å’Œé“¾æ¥éƒ½ä¸ºç©ºï¼Œé‚£ä¹ˆå°±æœ‰å¿…è¦å¾€ä¸‹æ‰§è¡Œäº†
    elif select:
        kai_id = select
    else:
        kai_id = func_box.created_atime()
        waring = 'æ–°å»ºçŸ¥è¯†åº“æ—¶ï¼ŒçŸ¥è¯†åº“åç§°å»ºè®®ä¸è¦ä¸ºç©ºï¼Œæœ¬æ¬¡çŸ¥è¯†åº“åç§°å–ç”¨æœåŠ¡å™¨æ—¶é—´`kai_id`ä¸ºçŸ¥è¯†åº“åç§°ï¼ï¼ï¼'
        yield '', waring, gr.Dropdown.update(), gr.update(), gr.Dropdown.update(), kai_handle
    # < --------------------é™åˆ¶ä¸Šç­æ—¶é—´æ®µæ„å»ºçŸ¥è¯†åº“--------------- >
    reject_build_switch = toolbox.get_conf('reject_build_switch')
    if reject_build_switch:
        if not func_box.check_expected_time():
            raise gr.Error('ä¸Šç­æ—¶é—´æ®µä¸å…è®¸å¯åŠ¨æ„å»ºçŸ¥è¯†åº“ä»»åŠ¡ï¼Œè‹¥æœ‰ç´§æ€¥ä»»åŠ¡è¯·è”ç³»ç®¡ç†å‘˜')
    # < --------------------è¯»å–æ–‡ä»¶æ­£å¼å¼€å§‹--------------- >
    yield 'å¼€å§‹å’¯å¼€å§‹å’¯ï½', '', gr.update(), gr.update(), gr.update(), kai_handle
    files = kai_handle['file_path']
    file_manifest = []
    spl = toolbox.get_conf('spl')
    # æœ¬åœ°æ–‡ä»¶
    error = ''
    for sp in spl:
        _, file_manifest_tmp, _ = crazy_utils.get_files_from_everything(files, type=f'.{sp}')
        file_manifest += file_manifest_tmp
    # ç½‘ç»œæ–‡ä»¶
    try:
        task_info, kdocs_manifest_tmp, _ = docs_kingsoft.get_kdocs_from_everything(links, type='', ipaddr=user_addr)
        # task_info, kdocs_manifest_tmp, _ = crzay_kingsoft.get(links, type='', ipaddr=user_addr)
        if kdocs_manifest_tmp:
            error += task_info
            yield (f"", error, gr.update(), gr.update(), gr.update(), kai_handle)
    except:
        import traceback
        error_str = traceback.format_exc()
        error += f'æå–å‡ºé”™æ–‡ä»¶é”™è¯¯å•¦\n\n```\n{error_str}\n```'
        yield (f"", error, gr.update(), gr.update(), gr.update(), kai_handle)
        kdocs_manifest_tmp = []
    file_manifest += kdocs_manifest_tmp
    # < --------------------ç¼ºé™·æ–‡ä»¶æ‹†åˆ†--------------- >
    file_manifest = func_box.handling_defect_files(file_manifest)
    # < --------------------æ­£å¼å‡†å¤‡å¯åŠ¨ï¼--------------- >
    if len(file_manifest) == 0:
        types = "\t".join(f"`{s}`" for s in spl)
        link_type = f'\n\nç›®å½•: https://www.kdocs.cn/{func_box.html_tag_color("ent")}/41000207/{func_box.html_tag_color("130730080903")}\n\n' \
                    f'åˆ†äº«æ–‡ä»¶: https://www.kdocs.cn/l/{func_box.html_tag_color("cpfcxiGjEvqK")}'
        yield (
        f'æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯è¯»å–æ–‡ä»¶ï¼Œ å½“å‰æ”¯æŒè§£æçš„æœ¬åœ°æ–‡ä»¶æ ¼å¼å¦‚ä¸‹: \n\n{types}\n\nåœ¨çº¿æ–‡æ¡£é“¾æ¥æ”¯æŒå¦‚ä¸‹: {link_type}',
        error, gr.Dropdown.update(), gr.Dropdown.update(),
        gr.Dropdown.update(), kai_handle)
        return
    # # < -------------------é¢„çƒ­æ–‡æœ¬å‘é‡åŒ–æ¨¡ç»„--------------- >
    # yield ('æ­£åœ¨åŠ è½½å‘é‡åŒ–æ¨¡å‹...', '', gr.Dropdown.update(), gr.Dropdown.update(), gr.Dropdown.update(), kai_handle)
    # with toolbox.ProxyNetworkActivate():    # ä¸´æ—¶åœ°æ¿€æ´»ä»£ç†ç½‘ç»œ
    #     HuggingFaceEmbeddings(model_name="GanymedeNil/text2vec-large-chinese")
    # < -------------------æ„å»ºçŸ¥è¯†åº“--------------- >
    tab_show = [os.path.basename(i) for i in file_manifest]
    preprocessing_files = func_box.to_markdown_tabs(head=['æ–‡ä»¶'], tabs=[tab_show])
    yield (f'æ­£åœ¨å‡†å¤‡å°†ä»¥ä¸‹æ–‡ä»¶å‘é‡åŒ–ï¼Œç”ŸæˆçŸ¥è¯†åº“æ–‡ä»¶ï¼Œè‹¥æ–‡ä»¶æ•°æ®è¾ƒå¤šï¼Œå¯èƒ½éœ€è¦ç­‰å¾…å‡ å°æ—¶ï¼š\n\n{preprocessing_files}',
           error, gr.Dropdown.update(), gr.Dropdown.update(),
           gr.update(), kai_handle)
    with toolbox.ProxyNetworkActivate():  # ä¸´æ—¶åœ°æ¿€æ´»ä»£ç†ç½‘ç»œ
        kai = knowledge_archive_interface(vs_path=vector_path)
        qa_handle, vs_path = kai.construct_vector_store(vs_id=kai_id, files=file_manifest)
    with open(os.path.join(vector_path, kai_id, user_addr), mode='w') as f:
        pass
    _, kai_files = kai.get_init_file(kai_id)
    kai_handle['file_list'] = [os.path.basename(file) for file in kai_files if os.path.exists(file)]
    kai_files = func_box.to_markdown_tabs(head=['æ–‡ä»¶'], tabs=[tab_show])
    kai_handle['know_obj'].update({kai_id: qa_handle})
    kai_handle['know_name'] = kai_id
    load_list, user_list = func_box.get_directory_list(vector_path, user_addr)
    yield (f'æ„å»ºå®Œæˆ, å½“å‰çŸ¥è¯†åº“å†…æœ‰æ•ˆçš„æ–‡ä»¶å¦‚ä¸‹, å·²è‡ªåŠ¨å¸®æ‚¨é€‰ä¸­çŸ¥è¯†åº“ï¼Œç°åœ¨ä½ å¯ä»¥ç•…å¿«çš„å¼€å§‹æé—®å•¦ï½\n\n{kai_files}',
           error, gr.Dropdown.update(value=cls_select, choices=load_list),
           gr.Dropdown.update(value='æ–°å»ºçŸ¥è¯†åº“', choices=load_list),
           gr.Dropdown.update(value=kai_id, choices=load_list), kai_handle)


def knowledge_base_query(txt, chatbot, history, llm_kwargs, plugin_kwargs):
    # < -------------------ä¸ºç©ºæ—¶ï¼Œä¸å»æŸ¥è¯¢å‘é‡æ•°æ®åº“--------------- >
    if not txt: return txt
    know_cls_kw = {llm_kwargs['know_cls']: llm_kwargs['know_id']}
    new_txt = f'{txt}'
    # < -------------------æ£€æŸ¥åº”è¯¥èµ°å“ªå¥—æµç¨‹-------------- >
    associated_knowledge_base, = crazy_box.json_args_return(plugin_kwargs, ['å…³è”çŸ¥è¯†åº“'])
    if associated_knowledge_base:
        know_cls_kw = {}
        for _kw in associated_knowledge_base:
            know_cls_kw[_kw] = associated_knowledge_base[_kw]['æŸ¥è¯¢åˆ—è¡¨']
        txt = None
    gpt_say = f'æ­£åœ¨å°†é—®é¢˜å‘é‡åŒ–ï¼Œç„¶åå¯¹`{str(know_cls_kw)}`çŸ¥è¯†åº“è¿›è¡ŒåŒ¹é….\n\n'
    if list(know_cls_kw.values())[-1]:
        if gpt_say not in str(chatbot):
            chatbot.append([txt, gpt_say])
            yield from toolbox.update_ui(chatbot=chatbot, history=history)  # åˆ·æ–°ç•Œé¢
    for know_cls in know_cls_kw:
        for id in know_cls_kw[know_cls]:
            if llm_kwargs['know_dict']['know_obj'].get(id, False):
                kai = llm_kwargs['know_dict']['know_obj'][id]
            else:
                know_cls = classification_filtering_tag(know_cls, llm_kwargs['ipaddr'])
                vs_path = os.path.join(func_box.knowledge_path, know_cls)
                kai = knowledge_archive_interface(vs_path=vs_path)
                llm_kwargs['know_dict']['know_obj'][id] = kai
            # < -------------------æŸ¥è¯¢å‘é‡æ•°æ®åº“--------------- >
            prompt_cls = 'çŸ¥è¯†åº“æç¤ºè¯'
            resp, prompt, _ok = kai.answer_with_archive_by_id(new_txt, id, llm_kwargs)
            referenced_documents = "\n".join(
                [f"{k}: " + doc.page_content for k, doc in enumerate(resp['source_documents'])])
            source_documents = "\n".join({func_box.html_view_blank(doc.metadata.get('source', '')): '' for k, doc in
                                          enumerate(resp['source_documents'])})
            if not referenced_documents:
                gpt_say += f"`{id}`çŸ¥è¯†åº“ä¸­æ²¡æœ‰ä¸é—®é¢˜åŒ¹é…çš„æ–‡æœ¬ï¼Œæ‰€ä»¥ä¸ä¼šæä¾›ä»»ä½•å‚è€ƒæ–‡æœ¬ï¼Œä½ å¯ä»¥åœ¨Settings-æ›´æ”¹`çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³åº¦`ä¸­è¿›è¡Œè°ƒä¼˜ã€‚\n"
                chatbot[-1] = [txt, gpt_say]
            else:
                if associated_knowledge_base:
                    prompt_name = associated_knowledge_base[know_cls].get(prompt_cls)
                    tips = f'åŒ¹é…ä¸­äº†`{id}`çŸ¥è¯†åº“ï¼Œä½¿ç”¨çš„Promptæ˜¯`{prompt_cls}`åˆ†ç±»ä¸‹çš„`{prompt_name}`, æ’ä»¶è‡ªå®šä¹‰å‚æ•°å…è®¸æŒ‡å®šå…¶ä»–Promptå“¦ï½'
                    if tips not in str(chatbot):
                        gpt_say += tips
                    prompt_con = database_processor.SqliteHandle(table=f'prompt_{prompt_cls}_sys').find_prompt_result(
                        prompt_name)
                else:
                    prompt_name = 'å¼•ç”¨çŸ¥è¯†åº“å›ç­”'
                    tips = f'`{id}`çŸ¥è¯†åº“é—®ç­”ä½¿ç”¨çš„Promptæ˜¯`{prompt_cls}`åˆ†ç±»ä¸‹çš„' \
                           f'`{prompt_name}`, ä½ å¯ä»¥ä¿å­˜ä¸€ä¸ªåŒåçš„Promptåˆ°ä¸ªäººåˆ†ç±»ä¸‹ï¼ŒçŸ¥è¯†åº“é—®ç­”ä¼šä¼˜å…ˆä½¿ç”¨ä¸ªäººåˆ†ç±»ä¸‹çš„æç¤ºè¯ã€‚'
                    if tips not in str(chatbot):
                        gpt_say += tips
                    prompt_con = database_processor.SqliteHandle(table=f'prompt_{prompt_cls}_sys').find_prompt_result(
                        prompt_name, individual_priority=llm_kwargs['ipaddr'])
                gpt_say += f"\n\nå¼•ç”¨æ–‡æ¡£:\n\n> {source_documents}"
                chatbot[-1] = [txt, gpt_say]
                prompt_content = func_box.replace_expected_text(prompt=prompt_con, content=referenced_documents,
                                                                expect='{{{v}}}')
                new_txt = func_box.replace_expected_text(prompt=prompt_content, content=new_txt, expect='{{{q}}}')
            yield from toolbox.update_ui(chatbot=chatbot, history=history)  # åˆ·æ–°ç•Œé¢
    return new_txt


def obtain_classification_knowledge_base(cls_name, ipaddr: gr.Request):
    user = func_box.user_client_mark(ipaddr)
    if cls_name == 'ä¸ªäººçŸ¥è¯†åº“':
        load_path = os.path.join(func_box.knowledge_path, 'ä¸ªäººçŸ¥è¯†åº“', user)
    else:
        load_path = os.path.join(func_box.knowledge_path, cls_name)
    load_list, user_list = func_box.get_directory_list(load_path, user)
    know_user_build = toolbox.get_conf('know_user_build')
    if know_user_build:
        mesg = 'æ„å»ºé‡æ„æ²¡æœ‰ä»»ä½•é™åˆ¶ï¼Œä½ å¯ä»¥æ›´æ”¹configä¸­çš„`know_user_build`ï¼Œé™åˆ¶åªèƒ½é‡æ„æ„å»ºä¸ªäººçš„çŸ¥è¯†åº“'
    else:
        mesg = 'ä½ åªèƒ½é‡æ„è‡ªå·±ä¸Šä¼ çš„çŸ¥è¯†åº“å“¦ğŸ˜'
    status = f"{mesg}" \
             f"\n\n{func_box.to_markdown_tabs(head=['å¯ç¼–è¾‘çŸ¥è¯†åº“', 'å¯ç”¨çŸ¥è¯†åº“'], tabs=[user_list, load_list], column=False)}\n\n"
    return gr.Dropdown.update(choices=user_list), gr.Dropdown.update(choices=load_list, label=f'{cls_name}'), status


def want_to_rename_it(cls_name, select, ipaddr: gr.Request):
    user = func_box.user_client_mark(ipaddr)
    if cls_name == 'ä¸ªäººçŸ¥è¯†åº“':
        load_path = os.path.join(func_box.knowledge_path, 'ä¸ªäººçŸ¥è¯†åº“', user)
    else:
        load_path = os.path.join(func_box.knowledge_path, cls_name)
    load_list, user_list = func_box.get_directory_list(load_path, user)
    if select in load_list:
        return gr.Button.update(visible=True)
    else:
        return gr.update(visible=False)


def obtaining_knowledge_base_files(cls_select, vs_id, chatbot, kai_handle, model, ipaddr: gr.Request):
    if vs_id and 'é¢„åŠ è½½çŸ¥è¯†åº“' in model:
        cls_select = classification_filtering_tag(cls_select, func_box.user_client_mark(ipaddr))
        vs_path = os.path.join(func_box.knowledge_path, cls_select)
        you_say = f'è¯·æ£€æŸ¥çŸ¥è¯†åº“å†…æ–‡ä»¶{"  ".join([func_box.html_tag_color(i) for i in vs_id])}'
        chatbot.append([you_say, None])
        yield chatbot, 'ğŸƒğŸ»â€ æ­£åœ¨åŠªåŠ›è½®è¯¢ä¸­....è¯·ç¨ç­‰ï¼Œ tipsï¼šçŸ¥è¯†åº“å¯ä»¥å¤šé€‰ï¼Œä½†ä¸è¦è´ªæ¯å“¦ï½ï¸', kai_handle
        kai_files = {}
        for id in vs_id:
            if kai_handle['know_obj'].get(id, None):
                kai = kai_handle['know_obj'][id]
            else:
                kai = knowledge_archive_interface(vs_path=vs_path)
            qa_handle, _dict = kai.get_init_file(vs_id=id)
            kai_files.update(_dict)
            kai_handle['know_obj'].update({id: qa_handle})
        tabs = [[_id, func_box.html_view_blank(file), kai_files[file][_id]] for file in kai_files for _id in
                kai_files[file]]
        kai_handle['file_list'] = [os.path.basename(file) for file in kai_files if os.path.exists(file)]
        chatbot[-1] = [you_say, f'æ£€æŸ¥å®Œæˆï¼Œå½“å‰é€‰æ‹©çš„çŸ¥è¯†åº“å†…å¯ç”¨æ–‡ä»¶å¦‚ä¸‹ï¼š'
                                f'\n\n {func_box.to_markdown_tabs(head=["æ‰€å±çŸ¥è¯†åº“", "æ–‡ä»¶", "æ–‡ä»¶ç±»å‹"], tabs=tabs, column=True)}\n\n'
                                f'ğŸ¤© å¿«æ¥å‘æˆ‘æé—®å§ï½']
        yield chatbot, 'âœ… æ£€æŸ¥å®Œæˆ', kai_handle
    else:
        yield chatbot, 'Done', kai_handle


def single_step_thread_building_knowledge(cls_name, know_id, file_manifest, llm_kwargs):
    cls_select = classification_filtering_tag(cls_name, llm_kwargs['ipaddr'])
    vector_path = os.path.join(func_box.knowledge_path, cls_select)
    os.makedirs(vector_path, exist_ok=True)

    def thread_task():
        kai = knowledge_archive_interface(vs_path=vector_path)
        qa_handle, vs_path = kai.construct_vector_store(vs_id=know_id, files=file_manifest)
        llm_kwargs['know_dict']['know_obj'][know_id] = qa_handle

    threading.Thread(target=thread_task, ).start()
